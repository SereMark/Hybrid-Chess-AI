%!TeX program = xelatex
\documentclass[12pt,a4paper]{report}

\usepackage{iftex}
\usepackage{mathtools}

\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp}
  \usepackage{newtxtext,newtxmath}
\else
  \usepackage{fontspec}
  \setmainfont{Times New Roman}
  \setsansfont{Arial}
  \IfFontExistsTF{Consolas}
    {\setmonofont{Consolas}}
    {\setmonofont{Courier New}}
  \usepackage{unicode-math}
  \setmathfont{Cambria Math}
\fi

\usepackage[magyar]{babel}
\usepackage[sort,nocompress]{cite}

\usepackage[autostyle]{csquotes}

\usepackage[a4paper,left=25mm,right=25mm,top=25mm,bottom=25mm,bindingoffset=10mm]{geometry}
\usepackage{setspace}
\onehalfspacing
\raggedbottom
\usepackage{microtype}
\ifPDFTeX
  \DisableLigatures{encoding=T1,family=ntxtlf}
\fi
\frenchspacing
\clubpenalty=10000
\widowpenalty=10000
\displaywidowpenalty=10000
\usepackage{siunitx}
\sisetup{output-decimal-marker = {,},group-separator = {\,}}

\usepackage{graphicx}
\graphicspath{{figures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg}
\usepackage{float}
\usepackage{booktabs}

\usepackage{listings}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Bemenet:}}
\renewcommand{\algorithmicensure}{\textbf{Kimenet:}}
\floatname{algorithm}{Algoritmus}
\renewcommand{\listalgorithmname}{Algoritmusok jegyzéke}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\ifPDFTeX
  \lstset{inputencoding=utf8}
\fi

\lstset{style=mystyle,
  columns=fullflexible,
  literate={á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
           {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
           {ö}{{\"o}}1 {ü}{{\"u}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
           {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
           {×}{{\texttimes}}1
}

\renewcommand{\lstlistingname}{Kódrészlet}
\renewcommand{\lstlistlistingname}{Kódrészletek jegyzéke}

\usepackage{chngcntr}
\counterwithin{figure}{chapter}
\counterwithin{table}{chapter}
\counterwithin{algorithm}{chapter}
\makeatletter
\AtBeginDocument{%
  \@ifundefined{c@lstlisting}{}{%
    \counterwithin{lstlisting}{chapter}%
  }%
}
\makeatother
\counterwithin{equation}{chapter}

\usepackage[unicode,hypertexnames=false]{hyperref}
\usepackage{xurl}
\usepackage{bookmark}
\usepackage[nameinlink,noabbrev]{cleveref}
\crefname{chapter}{fejezet}{fejezetek}
\Crefname{chapter}{Fejezet}{Fejezetek}
\crefname{section}{szakasz}{szakaszok}
\Crefname{section}{Szakasz}{Szakaszok}
\crefname{figure}{ábra}{ábrák}
\Crefname{figure}{Ábra}{Ábrák}
\crefname{table}{táblázat}{táblázatok}
\Crefname{table}{Táblázat}{Táblázatok}
\crefname{equation}{képlet}{képletek}
\Crefname{equation}{Képlet}{Képletek}
\crefname{algorithm}{algoritmus}{algoritmusok}
\Crefname{algorithm}{Algoritmus}{Algoritmusok}
\crefname{lstlisting}{kódrészlet}{kódrészletek}
\Crefname{lstlisting}{Kódrészlet}{Kódrészletek}
\urlstyle{same}

\newcommand{\ThesisTitle}{Neurális hálózatokat és Monte Carlo fakeresést kombináló hibrid sakkrobot fejlesztése nyitókönyv integrációval}
\newcommand{\Fig}[1]{\ref{#1}.~ábra}
\newcommand{\Tab}[1]{\ref{#1}.~táblázat}
\hypersetup{
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  urlcolor=blue,
  pdfauthor={Sere Gergő Márk},
  pdftitle={\ThesisTitle},
  pdfsubject={Szakdolgozat},
  pdfcreator={LaTeX}
}

\newcommand{\doi}[1]{\href{https://doi.org/#1}{doi:\nolinkurl{#1}}}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\nouppercase\leftmark}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter.~fejezet: #1}{}}
\setlength{\headheight}{14.5pt}
\fancypagestyle{plain}{%
  \fancyhf{}%
  \rfoot{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

\usepackage{titlesec}
\titleformat{\chapter}[hang]{\bfseries\fontsize{14pt}{16pt}\selectfont}{\thechapter.\quad}{0pt}{}
\titleformat{\section}[hang]{\bfseries\fontsize{12pt}{14pt}\selectfont}{\thesection\quad}{0pt}{}
\titleformat{\subsection}[hang]{\bfseries\fontsize{12pt}{14pt}\selectfont}{\thesubsection\quad}{0pt}{}
\titlespacing*{\chapter}{0pt}{\baselineskip}{\baselineskip}
\titlespacing*{\section}{0pt}{\baselineskip}{\baselineskip}
\titlespacing*{\subsection}{0pt}{\baselineskip}{\baselineskip}

\usepackage{caption}
\captionsetup{font=small,labelfont=bf}
\usepackage{enumitem}
\setlist[itemize]{noitemsep,topsep=2pt}
\setlist[enumerate]{noitemsep,topsep=2pt}

\pdfstringdefDisableCommands{%
  \def\H#1{#1}%
}

\newcommand{\AuthorName}{Sere Gergő Márk}
\newcommand{\StudentProgram}{programtervező informatikus BSc}
\newcommand{\SupervisorName}{Dr.~Békési József, főiskolai tanár}
\newcommand{\FacultyNameDisplay}{Szegedi Tudományegyetem\\Természettudományi és Informatikai Kar\\Informatikai Intézet}
\newcommand{\DepartmentNameDisplay}{Számítástudomány Alapjai Tanszék}
\newcommand{\FacultyNameText}{Szegedi Tudományegyetem Természettudományi és Informatikai Kar Informatikai Intézet}
\newcommand{\DepartmentNameText}{Számítástudomány Alapjai Tanszék}
\newcommand{\SubmissionDate}{2025. december 11.}
\newcommand{\SubmissionYear}{2025}

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}

\begin{document}
\renewcommand{\abstractname}{Tartalmi összefoglaló}

\setlength{\emergencystretch}{12em}

\hypersetup{pageanchor=false}
\begin{titlepage}
  \thispagestyle{empty}
  \begin{center}
    \vspace*{0.5cm}
    \includegraphics[width=3.2cm]{SZTE_cimer}\par
    \vspace{0.5cm}
    {\Large \textbf{\FacultyNameDisplay}\par}
    \vspace{0.5cm}
    {\large \textbf{\DepartmentNameDisplay}\par}
    \vspace{2.0cm}
    {\Large \textbf{Szakdolgozat}\par}
    \vspace{12pt}
    {\LARGE \textbf{\ThesisTitle}\par}
    \vspace{4cm}
    \begin{minipage}[t]{0.46\textwidth}
      \raggedright
      {\bfseries Készítette}\\[-1em]
      \rule{0.35\textwidth}{0.4pt}\par
      {\large \AuthorName}\par
      \StudentProgram
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.46\textwidth}
      \raggedleft
      {\bfseries Témavezető}\\[-1em]
      \rule{0.35\textwidth}{0.4pt}\par
      {\large \SupervisorName}\par
      \DepartmentNameDisplay
    \end{minipage}
    \vspace{2.0cm}\par
    {\large Szeged, \SubmissionYear\par}
  \end{center}
  \vfill
\end{titlepage}

\clearpage
\pagenumbering{arabic}
\setcounter{page}{2}
\hypersetup{pageanchor=true}

\chapter*{Feladatkiírás}
\thispagestyle{plain}
\phantomsection
\addcontentsline{toc}{chapter}{Feladatkiírás}
\noindent\textbf{Téma megnevezése:} Neurális hálózatokat és Monte Carlo fakeresést kombináló hibrid sakkrobot fejlesztése nyitókönyv integrációval

\noindent\textbf{Feladat rövid leírása:} Olyan AlphaZero-típusú~\cite{silver2017chess} sakkprogram fejlesztése, amelyben a lépésgenerálás és a PUCT-alapú Monte Carlo fakeresés (MCTS) C++ nyelven valósul meg. A lépésirány- és értékbecslést reziduális konvolúciós neurális háló (CNN) végzi. A rendszer támogassa az önjátszásos tanítást, a kötegelt kiértékelést, a telemetriai méréseket, valamint az arénamérkőzéseken alapuló modellértékelést és monitorozást. A nyitókönyv biztosítsa a kezdőállások minél nagyobb sokféleségét.

\noindent\textbf{Részfeladatok:}
\begin{enumerate}
  \item Bitboard-alapú sakkmag és szabályosságellenőrzés implementálása korszerű C++-ban (C++23).
  \item PUCT-alapú MCTS megvalósítása adaptív paraméterezéssel, Dirichlet-zajjal és kötegelt kiértékelő interfésszel.
  \item Reziduális CNN tervezése és betanítása önjátszásból származó adatra; aszinkron, automatikus kevert pontosságú (AMP) inferencia az önjátszás és az arénamérkőzések során.
  \item Visszajátszási puffer, eredménymegállapítás (adjudikáció), feladás és arénamérkőzések implementálása, valamint telemetria.
  \item Teljesítménymérés és összehasonlítás kiinduló megoldásokkal; reprodukálhatóság megvalósítása
\end{enumerate}

\clearpage
\thispagestyle{plain}
\chapter*{\abstractname}
\phantomsection
\addcontentsline{toc}{chapter}{\abstractname}
Dolgozatomban egy neurális hálózatokat és Monte Carlo fakeresést kombináló, AlphaZero-típusú hibrid sakkprogram fejlesztését mutatom be korlátozott erőforrású környezetben.

Az AlphaZero-típusú, megerősítéses tanuláson (\textit{reinforcement learning}, RL) alapuló sakkprogramok gyakorlati reprodukálása jellemzően ipari léptékű számítási kapacitást (például TPU-klasztereket) igényel. Ez a forrásigény megnehezíti a módszertan elérhetőségét egyetemi kutatók és hobbifejlesztők számára. Azt vizsgálom, hogy egyetlen fogyasztói GPU-n (RTX~3070 Laptop) és körülbelül 15~órás tanítási időkeretben milyen mérnöki döntésekkel valósítható meg egy kísérleti célokra alkalmas, AlphaZero-típusú sakkprogram.

Az általam bemutatott rendszer C++23-alapú sakkmotort és PUCT-alapú Monte Carlo fakeresést (MCTS) kombinál egy PyTorch-ban implementált, reziduális konvolúciós neurális hálózattal (CNN). A megoldás kötegelt kiértékelést, önjátszást és arénamérkőzéseket alkalmaz. A korlátozott erőforrások és az adatdiverzitás közötti egyensúly megteremtése érdekében a rendszer tömör bemeneti reprezentációt és nyitókönyvet~\cite{lichess} használ.

Munkám főbb hozzájárulásai a következők: korlátos erőforrásokra optimalizált,
C++/Python hibrid AlphaZero-típusú sakkmotor megvalósítása; a „Mindig világos”
kanonikus nézet (\textit{Canonical Always White}) bevezetése és indoklása a rendszerben; öt
különböző konfiguráció szisztematikus kísérleti összehasonlítása; valamint egy
teljesen automatizált benchmark- és reprodukciós feldolgozási lánc kialakítása.

Azonos hardverkörnyezetben öt különböző konfiguráció került tesztelésre. A \textbf{Nagy Entrópia} konfiguráció rövid futási idő alatt is 1249-es, saját skálán mért Elo-pontszámot (Elo-értéket, a sakkban használt relatív erősségmutatót) ért el, és felülmúlta a mohó heurisztikát, miközben a mélyebb, de kevésbé változatos keresést alkalmazó beállítások gyengébben teljesítettek. Méréseim azt jelzik, hogy rövid futási idő esetén a generált játszmák sokszínűségét biztosító stratégia fontosabb, mint a puszta keresési mélység.

Dolgozatomban öt kutatási kérdésre (RQ1-től RQ5-ig) keresem a választ, amelyek a rendszer mérnöki teljesítményét, skálázhatóságát, tanulási dinamikáját, az adatdiverzitást és a kísérletek reprodukálhatóságát vizsgálják.

\textbf{Kulcsszavak:} AlphaZero, Monte Carlo fakeresés (MCTS), megerősítéses tanulás (RL), hibrid architektúra, GPU-gyorsítás, kísérleti validáció
\vfill
\clearpage
\tableofcontents
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Táblázatok jegyzéke}
\listoftables
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Ábrák jegyzéke}
\listoffigures
\cleardoublepage

\chapter*{Rövidítések jegyzéke}
\thispagestyle{plain}
\phantomsection
\addcontentsline{toc}{chapter}{Rövidítések jegyzéke}
\begin{itemize}
  \item \textbf{AMP}: automatikus kevert pontosság (\textit{Automatic Mixed Precision}, PyTorch-környezetben alkalmazott technika)
  \item \textbf{CNN}: konvolúciós neurális hálózat (\textit{Convolutional Neural Network})
  \item \textbf{CPU}: központi feldolgozóegység (\textit{Central Processing Unit})
  \item \textbf{EMA}: exponenciális mozgóátlag (\textit{Exponential Moving Average})
  \item \textbf{GIL}: globális interpreter zár (\textit{Global Interpreter Lock})
  \item \textbf{GPU}: grafikus feldolgozóegység (\textit{Graphics Processing Unit})
  \item \textbf{JSON}: strukturált adatcsere-formátum (\textit{JavaScript Object Notation})
  \item \textbf{LRU}: legrégebben használt (\textit{Least Recently Used})
  \item \textbf{MCTS}: Monte Carlo fakeresés (\textit{Monte Carlo Tree Search})
  \item \textbf{NPS}: csomópont másodpercenként (\textit{Nodes Per Second})
  \item \textbf{PUCT}: UCT-variáns döntési prior súlyozással (\textit{Policy UCT / Prioritized UCT})
  \item \textbf{RL}: megerősítéses tanulás (\textit{Reinforcement Learning})
  \item \textbf{RQ}: kutatási kérdés (\textit{Research Question})
  \item \textbf{SGD}: sztochasztikus gradiensmódszer (\textit{Stochastic Gradient Descent})
  \item \textbf{TDP}: tervezett hőteljesítmény (\textit{Thermal Design Power})
  \item \textbf{TPU}: tenzorprocesszor-egység (\textit{Tensor Processing Unit})
  \item \textbf{UCT}: fákra alkalmazott felső konfidenciahatár (\textit{Upper Confidence bounds applied to Trees})
\end{itemize}

\cleardoublepage
\chapter{Bevezetés}

Az AlphaZero~\cite{silver2017chess} 2017-ben látványosan bemutatta, hogy a megerősítéses tanulás (RL) sikerrel alkalmazható komplex, teljes információjú játékokban is. Ugyanakkor a módszer gyakorlati reprodukálása jellemzően ipari léptékű erőforrásokat (például TPU-klasztereket) feltételez. Ez a magas belépési küszöb megnehezíti a technológia oktatását és kutatását, különösen a hazai felsőoktatási intézményekben. Dolgozatomban a „sakkrobot” kifejezés olyan szoftveres sakkprogramot jelöl, amely egy keresőmotort és egy neurális hálózatot kombinál.

Fő motivációm a technológia elérhetővé tétele hallgatók és kutatócsoportok számára. A cél egy olyan, korlátos erőforrásokra optimalizált mérnöki megközelítés kidolgozása, amely lehetővé teszi, hogy egyetlen fogyasztói GPU-n is érdemi kísérleteket lehessen végezni AlphaZero-típusú sakkágensekkel.

Munkámban azt vizsgálom, hogy egy NVIDIA RTX 3070 Laptop GPU-val, korlátozott, körülbelül 15~órás tanítási időkeretben milyen architekturális és hiperparaméter-beli kompromisszumokkal hozható létre működőképes, önjátszással tanuló sakkprogram. Az értékelés során öt eltérő konfiguráció kerül összehasonlításra a keresési mélység, az entrópia-alapú exploráció és a frissítési gyakoriság szempontjából.

Dolgozatom főbb eredményeit és hozzájárulásait az alábbiakban foglalom össze:
\begin{itemize}
  \item \textbf{Architekturális egyszerűsítés:} A „Mindig világos” kanonikus nézet (\textit{Canonical Always White}) bevezetésével a bemeneti állapottér komplexitása érdemben csökken, ami mérsékli az erőforrás-igényes adataugmentáció szükségességét.
  \item \textbf{Hibrid architektúra:} A rendszer a számításigényes feladatokat (lépésgenerálás, MCTS) nagy teljesítményű C++23 kóddal, míg a tanítást és a kötegelt kiértékelést rugalmas, PyTorch-alapú környezetben valósítja meg. A két réteg között egy vékony, minimális adatmásolást igénylő illesztőfelület biztosítja a hatékony adatcserét.
  \item \textbf{Eredmények röviden:} A \textbf{Nagy Entrópia} konfiguráció érte el a legjobb, saját skálán mért Elo-pontszámot, és a mérések alapján kis, de kimutatható előnyt mutatott a többi beállítással szemben, ami a diverzitás fontosságára utal rövid tanítási horizonton.
  \item \textbf{Reprodukálhatóság:} Teljesen automatizált benchmark- és artefaktumgeneráló szkriptek, valamint verziózott YAML-konfigurációk rögzítik a hardver- és hiperparaméter-beállításokat, így a kísérleti futtatások azonos beállítások mellett más környezetben is megismételhetők.
\end{itemize}

\medskip
A szakdolgozat készítése során két generatív, mesterséges intelligencián alapuló nyelvi modellcsaládot használtam kizárólag kiegészítő segédeszközként. Az OpenAI ChatGPT GPT-4o és GPT-5 modelljeit, valamint a Google Gemini 2.5-öt alkalmaztam a tanulási folyamat támogatására (szakirodalom gyors áttekintése, nyelvi-stiláris ellenőrzés), nem pedig a dolgozat érdemi tartalmának előállítására. Konkrétan vázlatos összefoglalók készítése, alapfogalmak tisztázása, valamint nyelvhelyességi javaslatok kérése során vettem igénybe ezeket az eszközöket. Nem használtam őket bekezdések vagy fejezetek automatikus létrehozására, és nem vettem igénybe az MI-t az algoritmusok, a forráskód vagy a szakmai következtetések megalkotásához; ezek a saját, önálló munkám eredményei.

Az MI által javasolt tartalmakat minden esetben kritikusan, segédeszközként kezeltem. A kapott információkat független, hivatkozott források alapján ellenőriztem, a nyelvi javaslatokat pedig dolgozatom érvrendszeréhez és saját megfogalmazásomhoz igazítva építettem be. A dolgozat tartalmáért és esetleges hibáiért teljes egészében én vállalom a felelősséget. Az MI használata az „A mesterséges intelligencia elfogadható és felelős használata a Szegedi Tudományegyetemen Hallgatói segédlet a mesterséges intelligencia tanulásban történő tudatos, hatékony és etikus alkalmazásához” című útmutatóban rögzített alapelvekkel összhangban, a hallgatói önálló munka elsődlegességét tiszteletben tartva történt.

A dolgozat felépítése a következő: az \textbf{Irodalmi áttekintés} fejezet a kapcsolódó szakirodalmat foglalja össze. Ezt követi a \textbf{Célkitűzések és módszertan} fejezet, amely meghatározza az RQ1-től RQ5-ig terjedő kutatási kérdéseket és a kísérleti elrendezést. A \textbf{Rendszer tervezése és implementáció} fejezet részletesen bemutatja a hibrid architektúrát és a teljesítménykritikus komponensek tervezési döntéseit. Az \ref{ch:experiments}.\ fejezet az öt vizsgált forgatókönyv eredményeit ismerteti, végül pedig a \textbf{Megbeszélés és korlátok} fejezet a skálázhatósági tapasztalatokat és a kutatási tanulságokat összegzi.


\chapter{Irodalmi áttekintés}

\section{Klasszikus sakkprogramozás}

A modern sakkmotorok gondolkodásmódját már Shannon korai elemzései keretezték \cite{shannon1950chess}. Ő vezette be azt a ma is használt megkülönböztetést, amely szerint az \emph{A-típusú} keresés kimerítő, állandó mélységig vizsgálja a legtöbb lehetséges folytatást, míg a \emph{B-típusú} megközelítés csak néhány, előzetesen ígéretesnek ítélt jelöltet visz mélyre. A történeti fejlődés végül az A-típusú, kvázi „nyers erő” (\textit{brute-force}) irányt részesítette előnyben. A megnövekedett számítási kapacitás és a kiforrott vágási heurisztikák jobban skáláztak, mint a korai, erősen kézi szabályokra támaszkodó szelektív keresések.

A klasszikus sakkprogramok a Minimax-elvre épülő, korlátozott mélységű keresést alkalmaznak. A sakkjátékfa Shannon által becsült komplexitása nagyságrendileg $10^{120}$ lehetséges játék~\cite{shannon1950chess}, miközben a pozíciók száma is óriási: Shannon eredeti becslése $\approx 10^{43}$ nagyságrendű, későbbi munkák pedig $10^{45}$ és $10^{50}$ közé teszik, ezért a teljes játékfa bejárása gyakorlatilag lehetetlen. A fa levelein (például a nyugalmi keresés végén) heurisztikus kiértékelő függvény (\textit{evaluation function}) ad pontszámot az állásnak. Ezek a függvények tipikusan több, egymástól elkülöníthető komponensre bontják a sakkpozíciót (anyag, gyalogstruktúra, tisztek aktivitása, királybiztonság stb.), majd \emph{lineáris, kézzel hangolt súlyokkal} kombinálják ezeket. A nagymesterek és programozók évtizedeken át finomhangolták ezeket a súlyokat; ezzel szemben a neurális hálókon alapuló kiértékelésnél (amelyet dolgozatomban is alkalmazok) a nemlineáris, adatból tanult reprezentációk váltják fel a kézi súlytáblákat.

A keresési tér hatékony bejárását az alfa-béta vágás (Alpha-Beta pruning) \cite{knuth1975alphabeta} teszi lehetővé, amely matematikailag garantáltan ugyanazt az eredményt adja, mint a teljes Minimax. Az irreleváns ágak levágásával (ahol a lépés már biztosan rosszabb, mint egy korábban talált alternatíva) a módszer ideális rendezés esetén nagyjából a négyzetgyökére csökkenti a keresési fa effektív elágazási tényezőjét. A modern motorok ezt tovább javítják olyan heurisztikákkal, mint a null-move vágás \cite{cpwNullMove} (ahol a lépés jogának átadása mellett keresés történik, feltételezve, hogy ha így is előny áll fenn, az eredeti állás még jobb) és a késői lépésredukció (Late Move Reductions, LMR)~\cite{cpwLMR}.

A Deep Blue 1997-es győzelme Kaszparov felett \cite{campbell2002deepblue} rámutatott a számítógépes sakk nagy potenciáljára, de ez a győzelem specifikus hardverre és hatalmas kézi tudásbázisra épült. A mai legjobbak közé tartozó klasszikus motor, a Stockfish \cite{stockfish} is ezt a vonalat képviseli. Keresési algoritmusa továbbra is a klasszikus alfa-béta vágáson alapul, ugyanakkor a korábbi, kézzel hangolt kiértékelő függvényt ma már NNUE (Efficiently Updatable Neural Network)~\cite{nasu2018nnue} technológiával megvalósított neurális értékelés váltotta fel. Az NNUE-alapú rendszerekben a kiértékelő súlyok nagyméretű játszmadatbázisokon tanult paraméterek, ugyanakkor az input-reprezentáció és a keresési heurisztikák továbbra is erősen domén-specifikusak, ami korlátozza az általánosíthatóságot.

\section{Monte Carlo Tree Search és AlphaZero}

\subsection{MCTS alapok és fejlesztések}

A Monte Carlo fakeresés módszereit Browne és mtsai~\cite{browne2012} részletesen áttekintik; az alábbiakban dolgozatom szempontjából releváns elemek kerülnek röviden összefoglalásra. A Monte Carlo fakeresés (\textit{Monte Carlo Tree Search}, MCTS) \cite{coulom2006cg,kocsis2006uct} egy aszimmetrikus, legjobb-első (\textit{best-first}) jellegű keresőalgoritmus véletlen szimulációkkal, amely a Go területén ért el először jelentős eredményt \cite{gelly2006mogo}. Az algoritmus elvileg nem igényel doménspecifikus heurisztikát (elegendőek a játékszabályok és a végállapotok kiértékelése), a gyakorlatban azonban a sikeres rendszerek a hatékonyság növelése érdekében számos heurisztikával egészítik ki az alapalgoritmust.

Az MCTS működése négy, egymásra épülő lépés ciklikus ismétlésén alapul:
\begin{enumerate}
    \item \textbf{Szelekció:} A gyökértől indulva a fa mentén olyan útvonalat választunk, amely egy kiválasztási stratégia (pl.~UCT vagy PUCT) szerint ígéretesnek tűnik. A bejárás addig folytatódik, amíg olyan csomóponthoz nem jutunk, ahol még van legalább egy, eddig nem vizsgált folytatás. A döntés célja minden lépésben az exploráció (új lehetőségek kipróbálása) és az exploitáció (jól bevált folytatások további vizsgálata) közötti kompromisszum.
    \item \textbf{Expanzió:} A kiválasztott csomóponthoz \emph{csak ekkor} adunk hozzá egy vagy több új gyermeket, amelyek a következő lépéseket reprezentálják. Így a fa kizárólag a ténylegesen vizsgált folytatások mentén nő tovább, nem pedig teljes szélességben.
    \item \textbf{Szimuláció:} Az eredeti, „klasszikus” MCTS-variánsokban innen indul egy gyors, sokszor véletlenszerű (vagy egyszerű heurisztikákkal vezetett) játszma a végállapotig (rollout), és a végkimenetelt használják az állás értékének becslésére. A modern, AlphaZero-típusú rendszerek, így a jelen dolgozatban ismertetett megoldás is, a költséges rolloutokat jellemzően neurális értékbecsléssel váltják ki: a levélcsomópontnál a hálózat becsli meg a pozíció várható kimenetelét.
    \item \textbf{Visszaterjesztés:} A kapott értéket visszavezetjük a fán felfelé a gyökérig, minden érintett csomópontban frissítve a látogatási számot ($N$) és az átlagos értéket ($Q$). Így a következő iterációk egyre megbízhatóbb statisztikákra támaszkodhatnak, és a keresés fokozatosan a kedvezőbb ágak felé koncentrálódik.
\end{enumerate}

Az UCT algoritmus \cite{kocsis2006uct} a többkaros bandit problémákra kidolgozott UCB1 stratégiát \cite{auer2002ucb} alkalmazza a fában. Ennek képlete:
\[ UCT = \frac{w_i}{n_i} + C \sqrt{\frac{\ln N_i}{n_i}} \]
ahol $w_i$ a nyerések száma, $n_i$ a csomópont látogatottsága, $N_i$ a szülő látogatottsága, $C$ pedig az explorációs konstans. A RAVE-heurisztika~\cite{gelly2007rave} további gyors konvergenciát tesz lehetővé a szimulációk eredményének megosztásával a testvércsomópontok között.

Az AlphaGo \cite{silver2016alphago} 2016-ban új alapokra helyezte ezt a megközelítést azzal, hogy a költséges és nagy varianciájú véletlen játszmabefejezéseket (rolloutokat) neurális hálózatokkal egészítette ki, és jelentősen kiegészítette, részben kiváltotta. Az AlphaGo Zero \cite{silver2017go} tovább egyszerűsítette a rendszert. A korábbi különálló döntéshozó (policy) és értékbecslő (value) hálózatokat egyetlen, kétfejű reziduális hálózatba integrálta. Továbbá bevezette a PUCT kiválasztási formulát, amely a hálózat által jósolt a priori valószínűségeket ($P(s,a)$) használja az exploráció irányítására.

A PUCT (UCT variáns döntési prior (\textit{policy prior}) súlyozással) alapformáját az AlphaGo Zero cikk vezette be~\cite{silver2017go}, ahol $c_{\text{puct}}$ egy, az exploráció szintjét meghatározó konstans. Az AlphaZero-~\cite{silver2017chess,silver2018science} és MuZero-tanulmányok~\cite{schrittwieser2020muzero} kiegészítő anyagaiban ezzel szemben egy, a szülőcsomópont látogatottságától függő dinamikus $c_{\text{puct}}$-skálázás jelenik meg. A közösségi fejlesztésű, AlphaZero-típusú implementációkban (pl.~KataGo~\cite{wu2019katago}, LC0~\cite{lczero}, CrazyAra~\cite{crazyara}) jellemzően ezt a dinamikus formulát veszik át és hangolják tovább; dolgozatomban is a \eqref{eq:cpuct} szerinti alakot alkalmazom:
\begin{equation}
c_{\text{puct}} = \log\left(\frac{N(s) + c_{\text{base}} + 1}{c_{\text{base}}}\right) + c_{\text{init}}
\label{eq:cpuct}
\end{equation}
ahol tipikusan $c_{\text{base}} = \num{19652}$, és $c_{\text{init}}$ tipikusan 1 és 3 közötti érték (míg jelen rendszerben konkrétan $c_{\text{init}}=\num{1.55}$). Ez biztosítja, hogy ritkábban látogatott állapotokban nagyobb exploráció történjen, tipikusan $c_{\text{puct}} \in [1, 5]$ tartományban (lásd \eqref{eq:cpuct}).

A PUCT akció-kiválasztási formula a következő pontszámot maximalizálja:
\[
\text{Score}(s,a) = Q(s,a) + U(s,a),
\]
ahol az explorációs tag definíciója:
\[
U(s,a) = c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)}.
\]
Itt $N(s)$ a csomópont (állapot) látogatási száma, $N(s,a)$ az adott él látogatottsága, $Q(s,a)$ az átlagos érték, $P(s,a)$ pedig a hálózat által adott prior valószínűség. A rendszerben a konfigurációs $c_{\text{puct,scale}}$ érték (pl.~\num{1.35} vagy \num{2.5}) a \eqref{eq:cpuct} szerinti dinamikus faktort skálázza, ahogyan azt az \Tab{tab:scenarios} mutatja.
Dirichlet-zaj biztosítja az explorációt a gyökérben. Az AlphaZero \cite{silver2017chess} ezt általánosította sakkra és shogira, emberfeletti szintet érve el néhány óra tanítással (sakkra ~9~óra).

\subsection{AlphaZero tanítási eljárás részletei}

Az AlphaZero tanítási folyamata négy, egymást követő fázis iterálásán alapul~\cite{silver2017chess}. 

Az \textbf{önjátszás} (self-play) szakaszban az aktuális modell játszik önmaga ellen változatos kezdőpozíciókból. Az eredeti AlphaZero-rendszerben minden lépéshez körülbelül 800 MCTS-szimuláció tartozik, a neurális háló érték- és policy-kimenetei pedig a levelekben irányítják a keresést~\cite{silver2017chess}. A lépés kiválasztása a csomópontok látogatási számaiból képzett eloszlásból történik, amelyet a hőmérsékletparaméter torzít. Korai játékban magasabb $\tau$ érték mellett, később $\tau \to 0$ közelében, hogy a rendszer fokozatosan a legvalószínűbb lépések felé koncentráljon. A játék végén a kimenetel ($z \in \{-1, 0, +1\}$) minden, a játszma során előfordult pozícióhoz hozzárendelt értékcéllá válik; a saját implementációmban ugyanezt az elvet követem, de a szimulációk számát a rendelkezésre álló erőforrásokhoz igazítom (lásd \Tab{tab:scenarios}).

Az \textbf{adattárolás} fázisban a pozíció-lépésirány-kimenetel hármasokat visszajátszási pufferben gyűjtik. Az AlphaGo Zero~\cite{silver2017go} kiegészítő anyaga explicit módon a legutóbbi $\sim$\num{500000} játékot tartalmazó visszajátszási puffert ír elő. Az AlphaZero-tanulmányok~\cite{silver2017chess,silver2018science} ugyanennek a tanítási protokollnak az adaptációi, de a konkrét pufferméretet nem minden esetben részletezik. A közösségi reimplementációk ezért jellemzően 500k és 1M közötti puffert alkalmaznak. A minták $(\boldsymbol{s}_t, \boldsymbol{\pi}_t, z_t)$ formában tárolódnak, ahol $\boldsymbol{s}_t$ a bemeneti reprezentáció (8 történeti lépés), $\boldsymbol{\pi}_t$ az MCTS látogatási eloszlás (policy target), és $z_t$ a végső kimenetel a $t$-edik pozíció nézőpontjából.

A \textbf{tanítási fázis} mini-kötegeken optimalizálja a hálót. A veszteségfüggvény \cite{silver2017chess} három tagból áll:
\begin{equation*}
\mathcal{L} =
  \underbrace{-\boldsymbol{\pi}^T \log \boldsymbol{p}}_{\text{policy-veszteség}}
  + \underbrace{\lambda_v (z - v)^2}_{\text{értékveszteség}}
  + \underbrace{\lambda_c \lVert\boldsymbol{\theta}\rVert^2}_{\text{L2 regularizációs tag}}
\end{equation*}
ahol $\boldsymbol{p}$ és $v$ a háló kimenetei, $\lambda_v$ tipikusan 1, $\lambda_c$ a súlycsökkenés koefficiense ($10^{-4}$). A gyakorlatban $\lambda_v = 1$, ezért dolgozatomban a $\mathcal{L}_{\text{val}}$ definíciójában elhagyom. Az AlphaZero az eredeti cikkben \cite{silver2017chess} momentumos SGD-t és lépcsőzetes (\textit{stepwise}) ütemezést alkalmaz a tanulási rátára.

A modern AlphaZero-típusú reimplementációk gyakran bemelegítési szakasszal (\textit{warmup}) és koszinuszos ütemezésű csökkenéssel kombinált stratégiát (SGDR-szerű \cite{loshchilov2017sgdr} ütemezés újraindítás nélkül) használnak. Dolgozatomban is ezt a megközelítést követem.

Az \textbf{aréna-értékelés} (\textit{arena evaluation}) fogalma a szakirodalomban két, egymástól elkülönülő szerepben jelenik meg. Az AlphaGo Zero~\cite{silver2017go} tanítási protokolljában az úgynevezett aréna-kapuzás (\textit{arena gating}) döntötte el, hogy egy jelölt háló átveheti-e az önjátszásban használt „best player” szerepét. Minden új háló 400 játszmát játszott az aktuális legjobb ellen, és csak akkor lépett elő, ha a győzelmi aránya meghaladta az 55\%-ot. Ez a konzervatív küszöb csökkentette a zajból eredő fluktuációt, és segített elkerülni a tanulás instabil „visszacsúszását”, ugyanakkor a tanulást nem tette monoton növekvővé, sok iteráció is eltelhetett új „best player” nélkül. Az AlphaZero-tanulmányok~\cite{silver2017chess,silver2018science} ezzel szemben kifejezetten elhagyják ezt a kapuzási lépést. Az önjátszásos adatok generálására mindig az éppen legutóbb tanított hálót használják, és az aréna jellegű tornákat csak utólagos erősségmérésre (pl.\ Elo-becslés Stockfish vagy AlphaGo-változatok ellen) alkalmazzák, nem pedig a további önjátszásban használt modell kiválasztására. Az MCTS-ben használt átlagolt $Q$-értékek részben „simítják” a neurális háló rövid távú hibáit, így a rendszer képes rosszabb köztes súlykonfigurációkból is „kimozogni” anélkül, hogy formális kapuzási döntésre lenne szükség.

\subsection{Ismert tipikus hibamódok és kihívások}

Az AlphaZero-típusú rendszerek számos tipikus hibamóddal rendelkeznek, amelyeket a szakirodalom~\cite{silver2017chess,lczero,wu2019katago} és a közösségi reimplementációk tapasztalatai részletesen dokumentálnak.

\textbf{Értékbecslő ág kollapszusa}: A neurális háló értékbecslő ága konstans értéket tanulhat meg (pl.~minden pozícióra $v \approx 0$), különösen túl kis adatmennyiség vagy túl nagy hálókapacitás esetén. Ez azt eredményezi, hogy az MCTS nem kap hasznos értékbecsléseket, és véletlenszerű lépéseket választ. A lehetséges megoldások: kisebb háló, több adat, vagy az értékbecslő ág tanulási rátájának csökkentése.

\textbf{Lépésirány-túlillesztés}: A policy ág memorizálhatja a tanítóadatok specifikus pozícióit anélkül, hogy általános sakkstratégiát tanulna. Ez magas tanítási pontosságot és alacsony validációs teljesítményt eredményez. Ez különösen kis nyitókönyv esetén gyakori, amikor a modell ugyanazokat a pozíciókat látja újra és újra. A megoldás nagyobb nyitókönyv, adataugmentáció (tükrözés, forgatás), dropout.

\textbf{Exploráció-kizsákmányolás egyensúly}: Túl konzervatív exploráció (alacsony Dirichlet zaj, magas $c_{\text{puct}}$) korai konvergenciát okoz rossz lokális optimumba. Túl agresszív exploráció véletlenszerű játékot eredményez, lassítva a tanulást. Az optimális paraméterek játékonként változnak. Sakkban $\alpha = \num{0.3}$, shogiban $\alpha = \num{0.15}$, Go-ban $\alpha = \num{0.03}$, amelyeket az AlphaZero cikk \cite{silver2017chess} és az AlphaGo Zero cikk \cite{silver2017go} részletesen dokumentál (a kiegészítő anyagokban), és amelyeket a közösségi AlphaZero-típusú implementációk gyakorlatban is megerősítettek.

\textbf{Aréna-értékelés zajérzékenysége}: Kis meccsszám ($< 100$ játék) nagy varianciát eredményez a becslésben. Két azonos erejű modell mérkőzése esetén a kimenetel közel azonos, nagyjából 50\%-os arány mindkét fél számára, így 100 játék esetén a 95\%-os konfidenciaintervallum $\pm 10\%$. Az AlphaGo Zero által alkalmazott konzervatív küszöb (55\%+ győzelmi arány) és nagy meccsszám (400+ arénaparti) csökkenti a hamis pozitív előléptetéseket; az AlphaZero esetében ehelyett folyamatos, kapuzás nélküli önjátszás zajlik, és az arénajellegű tornák csak külső erősségmérésre szolgálnak.

\textbf{Számítási költség skálázódása}: Az AlphaZero eredeti sakkfuttatása mintegy 9~órás volt, 5000 első generációs TPU-t használva az önjátszásos adatok generálására és 16 második generációs TPU-t a háló tanítására~\cite{silver2018science}. Az eredeti arXiv-változat és több közösségi összefoglaló 64 második generációs TPU-t említ~\cite{silver2017chess}, ugyanakkor a végleges \textit{Science}-cikk 16 TPU-t ad meg; itt ez utóbbi adatot veszem alapul. Közösségi becslések szerint ez milliós–tízmilliós nagyságrendű önjátszásos játszmát eredményezhetett, ami egyetlen fogyasztói GPU-n nagyságrendileg éveket venne igénybe. Kompromisszumok szükségesek, amik kevesebb iterációt, kisebb hálót, kevesebb MCTS-szimulációt, és kisebb puffert jelentenek.

\subsection{Leela Chess Zero tanulságok}

A Leela Chess Zero \cite{lczero} nyílt forráskódú közösségi projekt, amely az AlphaZero-módszertant valósítja meg elosztott önkéntes GPU-kon. Több ezer önkéntes GPU-idejét aggregálva a projekt 2018 óta több milliárd önjátszásos játszmát generált.

\textbf{Hálóarchitektúra evolúció}: Az LC0 kezdetben kisebb reziduális hálókat használt (pl.~6 reziduális blokk 64 és 128 csatornával), majd fokozatosan 15, 20 és később $\sim$40 blokkos hálókra nőtt több tízmillió paraméterrel. A 2020-as évek közepére a legmodernebb LC0/LZ hálók $\sim$40 vagy több blokkot és nagyobb csatornaszámokat használnak, gyakran tízmilliós paraméterszámmal, Squeeze-and-Excitation (SE, csatornaválasztó modul)~\cite{hu2018senet} rétegekkel kiegészítve. Ez arra utal, hogy a sakkhoz szükséges reprezentációs kapacitás jelentős, és a kisebb hálók (5 és 10 blokk között) csak korlátozott játékerőt érnek el.

\textbf{Tanítási trükkök}: A KataGo \cite{wu2019katago} és az LC0 projekt számos fejlesztést vezetett be az AlphaZero eredeti receptjéhez képest. A KataGo összetettebb veszteségfüggvényt és adatpipeline-t alkalmaz (például többféle értékcélt, playout cap randomizációt és a policy-érték jel közötti arány finomhangolását), amelyek segítenek mérsékelni az értékfej túlillesztését és esetleges kollapszusát. Az LC0 projekt emellett \emph{FPU reductiont} (First-Play Urgency csökkentés) és \emph{smart pruningot} (intelligens levágás) vezetett be a gyakorlatban. Az FPU reduction a nem látogatott csomópontoknak konzervatívabb kezdeti értéket ad, elkerülve a túl agresszív, zajos explorációt, míg a smart pruning alacsony priorú lépéseket vág le korán, a keresés hatékonyságát javítva egy elfogadott pontosság-sebesség kompromisszum mellett.

\textbf{Adatmennyiség vs. hálókapacitás}: Az LC0 tapasztalata, hogy a hálóméret növelése csak akkor hasznos, ha az adatmennyiség arányosan nő. Korai fázisban (< 10M játék) a kis hálók (6 és 10 blokk között) gyorsabban tanulnak. Nagyobb adatbázissal (100M+ játék) a nagy hálók (20 és 40 blokk között) előnybe kerülnek, de konvergenciájuk lassabb. Ez alátámasztja a tanterv alapú tanulás (\textit{curriculum learning})~\cite{bengio2009curriculum} megközelítést: kezdjük kis hálóval, majd növeljük a kapacitást konvergencia után.

\textbf{Elosztott infrastruktúra}: Az LC0 szerver-kliens architektúrát használ. Központi szerver osztja ki a tanítási súlyokat és gyűjti az önjátszás adatokat, kliensek (önkéntes GPU-k) generálják a játékokat és küldik vissza. Ez lehetővé teszi a skálázódást, de késleltetést és koordinációs költséget vezet be. Az általam vizsgált egyetlen GPU-s megközelítés elkerüli ezt a bonyolultságot, de korlátozott adatgenerálási sebességgel rendelkezik.

\section{Közösségi implementációk és hardveroptimalizáció}

A Leela Chess Zero (LC0, \cite{lczero}) kezdetben az AlphaZero-nál kompaktabb, kisebb dimenziójú policy kimenetet alkalmazó lépéskódolási sémákat is vizsgált, szemben az AlphaZero \num{4672}-es ($73\times8\times8$) kódolásával. Az újabb LC0 hálók azonban már jellemzően AlphaZero-stílusú, $73\times8\times8$ policy fejet alkalmaznak. A KataGo \cite{wu2019katago} és MuZero \cite{schrittwieser2020muzero} kifinomult tanítási technikákat vezetett be. Kulcsfontosságú mérnöki technikák: kötegelt feldolgozás, \texttt{channels\_last} (csatorna-utolsó) memória, vegyes precizió \cite{micikevicius2018amp}, aszinkron feldolgozási láncok, gyorsítótár. Hibrid C++/Python architektúrák \cite{pybind11} egyensúlyban tartják a sebességet és fejlesztési hatékonyságot.

\section{Kutatási rés}

Az AlphaZero-irányzat ipari léptékű hardverigénye \cite{silver2018science} jelentős korlátot jelent a módszertan széles körű elterjedése szempontjából. A szakirodalomban számos, különböző minőségű és célú reimplementáció található (például tisztán Python-alapú, oktatási jellegű keretrendszerek vagy nagy, elosztott infrastruktúrára tervezett rendszerek), ugyanakkor nem találtam olyan, széles körben hivatkozott referenciaimplementációt, amely \emph{egyidejűleg} teljesítené az alábbi követelményeket:
\begin{itemize}
  \item magas szintű, gyakorlatban is értelmezhető sakktudást ér el;
  \item C++-alapú, teljesítményre optimalizált keresőmotort használ;
  \item egyetlen fogyasztói GPU-ra van optimalizálva, rövid (egynapos) futási idővel;
  \item oktatási és kutatási célokra jól dokumentált, könnyen újrafuttatható kísérleti pipeline-t biztosít.
\end{itemize}
Ez a hiány nehezíti az AlphaZero-módszertan oktatási alkalmazását és a reprodukálható kutatást. Korlátozott erőforrással rendelkező egyetemi kutatócsoportok és egyéni fejlesztők gyakorlatilag nem tudnak olyan, jól definiált, fogyasztói hardverre szabott referenciarendszerre támaszkodni, amely mind a játékerő, mind a mérnöki szempontok tekintetében iránymutató volna.

Dolgozatom ezért elsődlegesen a módszertan megvalósíthatóságára és mérhető, korlátos erőforrások mellett elérhető teljesítményére fókuszál; a pontos célkitűzéseket a következő fejezet részletezi.

\chapter{Célkitűzések és módszertan}

Dolgozatom központi kérdése az, hogy az AlphaZero-típusú sakkprogram megvalósítható-e fogyasztói hardveren, korlátozott időkeret mellett, és ehhez milyen mérnöki döntések szükségesek. Emellett azt is vizsgálom, hogy ezekkel a korlátokkal milyen teljesítmény érhető el.

Ebben a fejezetben foglalom össze a kutatási célkitűzéseket, a hozzájuk tartozó kutatási kérdéseket és a kísérleti elrendezés főbb elemeit; a részletes rendszerleírást a következő fejezet tartalmazza.

A munka öt fő célkitűzése:
\begin{enumerate}
  \item \textbf{Hibrid C++/Python architektúra megvalósítása} tiszta, jól dokumentált API-val.
  \item \textbf{Komponens-szintű gyorsulások mérése} (sakkmag, MCTS, GPU-következtetés).
  \item \textbf{Teljes tanítási ciklus demonstrálása} egynapos költségvetéssel.
  \item \textbf{Arénamérkőzések metrikáinak rögzítése} a tanulási stabilitás követéséhez.
  \item \textbf{A kísérletek reprodukálhatóságának biztosítása} benchmarkokkal és konfigurációkkal.
\end{enumerate}

\section{Kutatási kérdések (RQ1-től RQ5-ig)}
\label{sec:kutatasi-kerdesek}

A célkitűzésekből öt kutatási kérdés (RQ1-től RQ5-ig) adódik:
\begin{description}
  \item[RQ1, Teljesítmény:] Milyen komponens-szintű gyorsulások érhetők el a hibrid architektúrával?
  \item[RQ2, Skálázhatóság:] Hogyan skálázódik a rendszer a párhuzamosítással, és melyek a szűk keresztmetszetek?
  \item[RQ3, Tanulási dinamika:] Mutat-e a rendszer belső tanulási jeleket (a policy- és value-veszteség tartós csökkenése, valamint az arénamérkőzések győzelmi arányainak javulása) rövid (egynapos) tanítási horizonton?
  \item[RQ4, Adatdiverzitás:] Milyen mértékben biztosítja a nyitókönyvvel~\cite{lichess} indított önjátszás és az explorációs beállítások együttese a pozíciók kellő sokszínűségét?
  \item[RQ5, Reprodukálhatóság:] Mennyire támogatja a rendszer felépítése és az automatizált eszközlánc a futások reprodukálhatóságát (azonos beállítások mellett)?
\end{description}

Dolgozatom célja \textbf{nem} egy versenyképes sakkprogram létrehozása, hanem az AlphaZero-módszertan \textbf{megvalósíthatóságának} demonstrálása korlátozott erőforrásokkal, valamint a rendszer \textbf{mérnöki teljesítményének} jellemzése.

A megvalósítás során a kritikus útvonalhoz tartozó komponenseket (bitboard-alapú sakkmag, PUCT-MCTS) C++23-ban, a tanulási folyamatokat pedig Pythonban, PyTorch~\cite{pytorch}-ra épülő gépi tanulási feldolgozási lánccal valósítottam meg. A tervezés köteg-orientált, kötegelt GPU-feldolgozással. A rendszer többszintű LRU-gyorsítótárat (érték-, kimenet- és kódolási gyorsítótár) és ritka lépésirány-tárolást alkalmaz.

A sikerkritériumok három kategóriába sorolhatók:
\begin{itemize}
  \item \textbf{Teljesítmény:} a pozíció/mp-ben és áteresztőképességben mért mutatók jelentős javítása a Python-alapú kiinduló megoldáshoz képest (python-chess sakkmag, tisztán Python MCTS, nem optimalizált neurális következtetés).
  \item \textbf{Tanulás:} a belső metrikák jelentős csökkenése.
  \item \textbf{Mérnöki:} automatikus metrika-naplózás (CSV/JSONL), YAML-konfigurációk és unit tesztek megléte.
\end{itemize}

A fenti célkitűzéseket a későbbi fejezetekben bemutatott kísérleti eredmények alapján értékelem, különösen az RQ1 (teljesítmény) és RQ3 (tanulási dinamika) kapcsán. A játékerő hosszú távú növekedésének és Elo-alapú külső validációnak a vizsgálata dolgozatom hatókörén kívül esik; a fókusz a technikai megvalósíthatóságon és a komponens-teljesítményen van.

A kísérleti elrendezés három pillérre épül:
\begin{enumerate}
    \item Számos izolált teljesítménymérés készült, amelyek lefedték a sakkmag, az MCTS-keresés, a neurális következtetés, az augmentáció és a visszajátszási puffer sebességét.
    \item Ezt követően teljes rendszerfuttatások történtek egységes, 6 blokkos, 96 csatornás hálóval, eltérő szimulációs mélység és kötegméret beállításokkal.
    \item Végül ablációs vizsgálatok készültek nyitókönyvvel és anélkül, erősen lecsökkentett gyorsítótár-kapacitással (gyakorlatilag gyorsítótár nélküli konfigurációkban), exponenciális mozgóátlag (\textit{Exponential Moving Average}, EMA) és közvetlen súlyok összehasonlításával, valamint a játszma-kiértékelés progresszív és fix változataival.
\end{enumerate}

A következő fejezet a fenti célok és módszertan megvalósítását ismerteti. Bemutatja a hibrid architektúra részleteit, a teljesítménykritikus komponensek tervezési döntéseit, valamint a széles körben alkalmazott optimalizációs technikákat. A \ref{sec:kutatasi-kerdesek}. szakaszban megfogalmazott, RQ1-től RQ5-ig terjedő kérdéseket az \ref{ch:experiments}. fejezet értékeli ki, a 7.\ fejezet pedig ezeket egyenként összegzi és kontextusba helyezi.

\chapter{Rendszer tervezése és implementáció}

\section{Architektúra}

A rendszer hibrid felépítésű. A számításigényes komponensek (bitboard sakkmag, virtuális veszteségre (\textit{virtual loss}) épülő konkurencia-kezelésű MCTS-motor) C++23-ban íródtak, míg a neurális háló tanítása és következtetése Pythonra épülő PyTorch környezetben történik. A C++ és a Python közötti interfészt a \texttt{pybind11} könyvtár biztosítja, amely a NumPy buffer-protokolljára építve megfelelően megtervezett, lapos tömbök esetén gyakorlatilag másolásmentes adatcserét is lehetővé tesz. A numerikus adatok hatékony kezelését a \texttt{NumPy}~\cite{harris2020numpy} csomag biztosítja, míg a kísérleti eredmények vizualizációját a \texttt{matplotlib}~\cite{hunter2007matplotlib} és \texttt{seaborn}~\cite{waskom2021seaborn} könyvtárak támogatják.

A rendszer alapvető tervezési elve a \textbf{kanonikus reprezentáció}, azaz a „Mindig világos” nézet. A hálózat kizárólag a világos szemszögéből látja a táblát. Sötét lépése esetén a táblát és a lépéseket a bemeneti kódolás során a rendszer tükrözi (vertikális tükrözés + színcsere), majd a hálózat kimenetét (policy) visszatükrözi. Hasonló „always white” nézetet alkalmaznak a modern neurális sakkmotorok is (pl.~Stockfish NNUE~\cite{stockfish,nasu2018nnue}, LC0~\cite{lczero}); dolgozatomban ezt a már bevált elvet igazítom a korlátozott erőforrású, egy-GPU-s AlphaZero-jellegű rendszerhez. Ez a megoldás felére csökkenti az állapottér komplexitását és feleslegessé teszi a heurisztikus adat-augmentációt. Az önjátszás során a kezdőállások egy részét véletlenszerűen a sötét játékos szemszögéből (FEN-tükrözéssel) veszem, de a kanonikus kódolás miatt ez a hálózat számára ekvivalens bemenetet eredményez. Dolgozatom keretei között ezt a kanonikus nézetet nem hasonlítottam közvetlen, nem-kanonikus baseline-hoz; a választást szakirodalmi és mérnöki szempontok (állapottér-csökkenés, egyszerűbb pipeline) indokolják.

A tanulási ciklus folyamatos, AlphaZero-stílusú tanulást valósít meg. Iteratív lépésekben ismétlődnek az önjátszás-tanítás-értékelés fázisai, a hálózat súlyai pedig a futás teljes időtartama alatt frissülnek, és az önjátszás mindig az éppen legfrissebb (EMA-val súlyozott) modellt használja. A klasszikus AlphaGo Zero-féle aréna-kapuzással szemben itt nincs olyan küszöbérték, amely az arénamérkőzések eredménye alapján engedélyezné vagy elutasítaná a jelölt modellt. Az arénamérkőzések a gyakorlatban kizárólag értékelő metrikaként szolgálnak, rendszeres időközönként az aktuális modell egy korábbi pillanatfelvétel ellen játszik, az eredményekből pedig győzelmi arányt, döntetlenarányt és futásidőt számolok, és legfeljebb az összehasonlításhoz használt „legjobb” referencia-modellt frissítem; az önjátszásban használt modellválasztásba ez nem avatkozik be:
\begin{enumerate}
  \item \textbf{Önjátszás:} Aszinkron munkaszálak generálják a játszmákat a legfrissebb hálózattal.
  \item \textbf{Adatgyűjtés:} A generált játszmák egy körkörös visszajátszási pufferbe kerülnek (\num{50000}-es kapacitás, nagy áteresztőképességű konfiguráció esetén \num{80000}).
  \item \textbf{Tanítás:} Minden iterációban a GPU mini-kötegekben mintavételez a pufferből, és momentumos SGD optimalizálóval frissíti a hálózat súlyait.
  \item \textbf{Szinkronizáció:} Az önjátszó szálak rendszeres időközönként átveszik az új súlyokat.
\end{enumerate}

\section{Módszerek}

\subsection{Sakkmag és bitboard reprezentáció}
A rendszer alapját egy nagy teljesítményű C++23 sakkmag képezi. A sakktábla reprezentációjának egyik legelterjedtebb és hatékony módja a \textbf{bitboard} technika, amely a klasszikus motorok (pl.~Stockfish~\cite{stockfish}) és a szakirodalom de facto szabványává vált; itt saját implementációban alkalmazom ugyanezt az elvet. Egy 64 bites egész szám (pl.~\texttt{uint64\_t} C++-ban) pontosan megfeleltethető a sakktábla 64 mezejének. Minden báb típushoz (gyalog, huszár, futó, bástya, vezér, király) és színhez (világos, sötét) külön bittáblát tart a rendszer nyilván, ahol az 1-es bit jelzi a báb jelenlétét az adott mezőn.

A bitboardok előnye, hogy a lépésgenerálás és a táblaműveletek (pl.~támadott mezők számítása) bitműveletekkel (AND, OR, XOR, bitshift) párhuzamosan végezhetők el az egész táblán. Például egy huszár összes lehetséges lépése egy adott mezőről előre kiszámítható és egy keresőtáblában (\textit{lookup table}) tárolható. A lépésgenerálás során a
\[ \texttt{targets} = \texttt{knight\_attacks}[\texttt{sq}] \ \& \ \sim\texttt{own\_pieces} \]
művelet néhány CPU ciklus alatt megadja az összes legális célmezőt, automatikusan kizárva a saját bábukat tartalmazó mezőket.

A csúszó bábuk (bástya, futó, vezér) esetében a rendszer a \textbf{PEXT-alapú bitboard-technikát} (Parallel Bit Extract) alkalmazza BMI2 utasításkészlettel (régebbi CPU-kon ennek szoftveres PEXT/bit-kompressziós megvalósításával). Ez egy hardveresen gyorsított, hash-alapú módszerrel kezeli a blokkoló bábukat, és gyakorlatilag konstans idejű ($O(1)$) lekérdezést biztosít a sugárirányú támadásokhoz.

A sakkmag felelős továbbá a szabályosság ellenőrzéséért (pl.~király nem léphet sakkba), a háromszori lépésismétlés és az 50 lépéses szabály detektálásáért, valamint a Zobrist-hash \cite{zobrist1970} számításáért, amelyet a neurális kiértékelések gyorsítótárazásához (transzpozíciós gyorsítótár) használ a rendszer.

\subsection{MCTS motor (C++)}
A keresőmotor a Monte Carlo Tree Search (MCTS) algoritmust valósítja meg PUCT kiválasztási stratégiával. A C++ implementáció több optimalizációt is tartalmaz a Python-alapú megoldásokhoz képest:

\begin{itemize}
\item \textbf{Konkurencia-kezelés (virtuális veszteség):} A \texttt{VIRTUAL\_LOSS = 1.0} paraméterrel a kiválasztott csomópontok értékösszegét ideiglenesen negatív irányba tolja el a rendszer, így az éppen kiértékelés alatt álló útvonalak átmenetileg kevésbé vonzóak a további szimulációk számára. Ez a klasszikus MCTS \emph{virtual loss} technika hatékonyan kezeli a függőben lévő, kötegelt neurális kiértékeléseket, és csökkenti annak esélyét, hogy a párhuzamos szimulációk ugyanarra az ágra koncentrálódjanak.
\item \textbf{Kötegelt levélkiértékelés:} A levélcsomópontok kötegelve kerülnek kiértékelésre; a C++ oldal int32 tömbökbe gyűjti a pozíciókat, majd egyetlen tenzorként adja át a Python rétegnek.
\item \textbf{Robusztus memóriakezelés:} A NodePool indexalapú, vektoralapú allokációja biztosítja, hogy az átméretezés a pointerek érvényességének megtartása mellett is skálázható legyen.
\end{itemize}

\subsection{Bemeneti reprezentáció és hálózat}
A sakkállásokat az AlphaZero-ban is alkalmazott, síkokra bontott tenzorral kódolja a rendszer. Az aktuális és az azt megelőző 7 lépésállás mindegyikét 14 síkon kódolja a rendszer (összesen $8 \cdot 14 = 112$ sík), valamint további 7 meta-síkot ad hozzá, így a teljes bemenet $119 \times 8 \times 8$ méretű:
\begin{itemize}
    \item \textbf{Előzmény (History):} Az aktuális és az azt megelőző 7 lépésállás (History=8).
    \item \textbf{Pozíció kódolás (14 sík):}
    \begin{itemize}
        \item 12 sík: Saját/Ellenfél gyalog, huszár, futó, bástya, vezér, király (P, N, B, R, Q, K).
        \item 1 sík: En Passant célmező (one-hot jelöléssel).
        \item 1 sík: Ismétlődési számláló $\ge 2$ (Döntetlen veszély jelzése).
    \end{itemize}
    \item \textbf{Meta-síkok (7 db):} szín, lépésszám, sáncjogok és fél-lépés számláló, ezek több bináris síkon kódolva (pl. külön sík a világos/sötét jogoknak).
\end{itemize}

A neurális hálózat egy \textbf{ResNet v1 architektúrát} követi~\cite{he2016resnet}, amely a mély hálózatok tanítását teszi lehetővé az eltűnő gradiens probléma (\textit{vanishing gradient}) enyhítésével. A hálózat alapépítőköve a reziduális blokk (Residual Block).

Egy reziduális blokk a következő rétegekből áll:
\begin{itemize}
    \item Konvolúciós réteg ($3 \times 3$ kernel, padding=1)
    \item Batch Normalization
    \item ReLU aktiváció (Rectified Linear Unit)
    \item Konvolúciós réteg ($3 \times 3$ kernel, padding=1)
    \item Batch Normalization
    \item \textbf{Áthidaló kapcsolat (Skip Connection):} A blokk bemenetét hozzáadjuk a második konvolúció kimenetéhez.
    \item ReLU aktiváció
\end{itemize}
Az áthidaló kapcsolat (\textit{skip connection}, \textit{identity shortcut}) lehetővé teszi, hogy a gradiens akadálytalanul áramoljon vissza a hálózat elejére a backpropagation során, így sokkal mélyebb hálózatok is hatékonyan taníthatók.

A jelenlegi hálóarchitektúra főbb jellemzői a következők:
\begin{itemize}
    \item \textbf{Törzs:} 6 reziduális blokk, 96 csatorna, $3 \times 3$ konvolúciók. Ez egyensúlyt teremt a kiértékelési sebesség és a játékerő között a korlátozott hardver környezetben.
    \item \textbf{Policy ág:} $1 \times 1$ konvolúció (2 csatorna) $\to$ Batch Norm $\to$ ReLU $\to$ Flatten $\to$ Fully Connected $\to$ 73×64 kimenet (4672 logit). A policy ág 73×64 logitet ad, amelyet a tanítási/inferencia réteg Softmax-szal valószínűségi eloszlássá alakít.
    \item \textbf{Értékbecslő ág (\textit{Value Head}):} $1 \times 1$ konvolúció (12 csatorna) $\to$ Batch Norm $\to$ ReLU $\to$ Flatten $\to$ 256 rejtett neuron (ReLU) $\to$ teljesen összekötött réteg (\textit{fully connected}) $\to$ 1 kimenet $\to$ Tanh. A kimenet $[-1, 1]$ intervallumban becsli a pozíció értékét (1: győzelem, 0: döntetlen, -1: vereség).
    \item \textbf{Inicializálás:} Kaiming Normal (\textit{He}) inicializáció kerül alkalmazásra, amely a ReLU aktivációkhoz optimalizált szórású véletlen súlyokkal indítja a hálózatot.
\end{itemize}

\subsection{Tanítási folyamat}
A rendszer a „folyamatos tanulás” paradigmát követi. A self-play és a tanítás egy időben zajlik, a hálózat súlyai minden iterációban frissülnek, és az önjátszást mindig a legutóbbi modell végzi.

A veszteségfüggvény:
\begin{equation*}
\mathcal{L} = \mathcal{L}_{\text{pol}} + \mathcal{L}_{\text{val}} - \lambda_{\text{entropy}} \cdot \mathcal{H}(p) + \lambda_c \cdot \lVert\theta\rVert^2
\end{equation*}
ahol $\mathcal{L}_{\text{pol}}$ a keresztentrópia, $\mathcal{L}_{\text{val}}$ a négyzetes hiba (MSE), $\lambda_{\text{entropy}} \approx 2\cdot 10^{-4}$ az entrópia-regularizációs tag súlya, amely a negatív előjel miatt a hálózat kimeneti döntési stratégiájának entrópiájának növelésére ösztönöz, gátolva a túl korai konvergenciát. A veszteségfüggvény az AlphaZero-ban használt formulát követi, kiegészítve ezzel az entrópia-taggal. A korábbi AlphaZero-jellegű rendszerekben használt táblaszimmetriákra épülő tükrözéses adataugmentáció helyett a kanonikus nézet biztosítja a szimmetria-invarianciát.

A következő fejezet a fenti architektúra alapján lefuttatott kísérleteket és az azokból származó eredményeket mutatja be.

\chapter{Kísérletek és eredmények}
\label{ch:experiments}

A kiértékelés során öt kísérleti forgatókönyv került vizsgálatra, hogy a különböző hiperparaméter-beállítások hatása elkülöníthető legyen a tanulási teljesítményre. Minden mérés azonos hardverkonfiguráción (RTX 3070 Laptop GPU, Ryzen 5800H), kontrollált környezetben futott; a teljes tanítási idők 9 és 18{,}4~óra között változtak.

Öt célzott konfigurációt definiáltam a \texttt{configs/} könyvtárban található beállítások alapján.

\begin{table}[htbp]
\centering
\caption{Az 5 kísérleti forgatókönyv paraméterei.}
\label{tab:scenarios}
\small
\begin{tabular}{l|l|ccc}
\toprule
\textbf{Név / Cél} & \textbf{Szimulációk száma} & \textbf{Köteg} & \textbf{PUCT} & \textbf{Zaj ($\alpha$)} \\
\midrule
\textbf{Referencia} (kontroll) & 128 & 256 & 1.35 & 0.3 \\
\textbf{Mély Keresés} (pontosság) & \textbf{256} & 256 & 1.35 & 0.3 \\
\textbf{Nagy Áteresztőképesség} (sekély keresés) & \textbf{64} & 256 & 1.35 & 0.3 \\
\textbf{Nagy Entrópia} (exploráció) & 128 & 256 & \textbf{2.5} & \textbf{0.5} \\
\textbf{Hatékonyság} (gyakori frissítés) & 128 & \textbf{128} & 1.35 & 0.3 \\
\bottomrule
\end{tabular}
\end{table}

\section{Eredmények}

\subsection{RQ1: Rendszer teljesítmény}

A C++23-ra történő áttérés és a virtuális veszteségre épülő konkurenciakezelés bevezetése számottevő sebességnövekedést eredményezett (lásd \Tab{tab:chess_cpp_bench}).

\begin{table}[htbp]
\centering
\caption{Sakkmag teljesítményének összehasonlítása.}
\label{tab:chess_cpp_bench}
\begin{tabular}{lcc}
\toprule
\textbf{Implementáció} & \textbf{Pozíció/mp} & \textbf{Relatív gyorsulás} \\
\midrule
Python (python-chess)~\cite{pythonchess} & \num{10699} & 1-szeres \\
C++ (Hybrid Core) & \textbf{\num{56887}} & \textbf{\num{5.3}-szoros} \\
\bottomrule
\end{tabular}
\end{table}

A neurális következtetés áteresztőképességét az \Tab{tab:inference_bench} szemlélteti.

\begin{table}[htbp]
\centering
\caption{Neurális következtetés teljesítménye (RTX 3070 Laptop GPU).}
\label{tab:inference_bench}
\begin{tabular}{llcr}
\toprule
\textbf{Eszköz} & \textbf{Precízió} & \textbf{Kötegméret} & \textbf{Pozíció/mp} \\
\midrule
CPU & Float32 & 1 & 570 \\
CPU & Float32 & 64 & \num{2850} \\
GPU (CUDA) & Float16 & 1 & 201 \\
GPU (CUDA) & Float16 & 64 & \num{12715} \\
GPU (CUDA) & Float16 & 512 & \textbf{\num{81519}} \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Sakkmotor (MoveGen):} A C++ implementáció \num{56887} pozíció/mp sebességet ért el, szemben a Python (python-chess) \num{10699} pozíció/mp teljesítményével. Ez \textbf{\num{5.3}-szoros gyorsulást} jelent.
    \item \textbf{Neurális következtetés:} Az FP16 precízió és a \texttt{channels\_last} memóriaelrendezés révén a rendszer 512-es kötegméret mellett \textbf{\num{81519} pozíció/mp} áteresztőképességet ért el.
    \item \textbf{MCTS-skálázódás:} A keresési sebesség (NPS) a kötegméret növelésével mérsékelten nő 64-es méretig, ahol eléri a \textbf{\num{25000} NPS} körüli csúcsot, majd csökken; a skálázódás nem lineáris.
\end{itemize}

\textbf{Erőforrás-hatékonyság:} A rendszer energiahatékonysága a „Hatékonyság” forgatókönyv alapján került vizsgálatra. A 300~iterációs teljes tanítási ciklus futási ideje \textbf{\num{9.1}~óra} volt az RTX 3070 Laptop GPU-n. A becsült energiafogyasztás (\SI{220}{\watt} TDP mellett) nagyságrendileg \textbf{\SI{2.0}{\kilo\watt\hour}}, ami azt mutatja, hogy a módszertan otthoni környezetben is alkalmazható, mérsékelt energiaigénnyel; a hosszabb futások arányosan több energiát igényeltek.

A mérések alapján az \textbf{RQ1}-re válaszolva kijelenthető, hogy a hibrid C++/Python architektúra a sakkmagban \num{5.3}-szoros gyorsulást, a neurális következtetésben pedig nagykötegű futtatás mellett nagyságrendileg tízszeresnél nagyobb sebességnövekedést eredményezett a Python-alapú kiinduló megoldáshoz képest. Ez a teljesítmény elegendő az egynapos önjátszásos tanítási ciklus megvalósításához.

\subsection{RQ2: Skálázhatóság}

Az önjátszás párhuzamosítása során jól megfigyelhető az Amdahl-törvény hatása~\cite{amdahl1967} (\Fig{fig:nps_scaling}). Az ábra bal oldali grafikonja az önjátszás teljesítményének skálázódását mutatja a szálak számának függvényében. A játszma/mp érték a szálak számával nő (1 szálon $\approx$0.5, 6 szálon $\approx$1.8 játszma/mp), azonban a mért gyorsulás jelentősen elmarad az elméleti lineáris maximumtól (szaggatott vonal). 6 szálnál a gyorsulás nagyjából 3.5$\times$, a várható 6$\times$ helyett. Ez a Python GIL, a szálak közti szinkronizáció és az adatmozgatás többletköltségei miatt fellépő párhuzamosítási veszteségekre utal.

A jobb oldali grafikon a keresési sebességet (NPS) ábrázolja a kötegméret függvényében. A görbe egyértelmű maximumot mutat 64-es kötegméretnél ($\approx$25\,000 NPS), amely az adott hardveren az optimális munkapontot jelenti. Kisebb kötegeknél (1 és 4 közötti kötegméret esetén) a keretrendszer fix költségei dominálnak, ezért csak $\sim$20\,000 NPS érhető el. 8 és 32 között meredek emelkedés figyelhető meg, ahogy a kötegelt feldolgozás jobban kihasználja az erőforrásokat, míg 128 és 256 esetén a várakozási idők, a memóriasávszélesség és a cache-hatások miatt ismét csökken a hatékonyság. A 4 körüli lokális törés, illetve az átmeneti visszaesés arra utal, hogy a kötegelt végrehajtás inicializációs költségei a kis-közepes tartományban még jelentősen terhelik a rendszert.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{nps_scaling}
  \caption{A keresési sebesség (NPS) és az önjátszás skálázódása.}
  \label{fig:nps_scaling}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Önjátszás skálázódása munkaszálak függvényében.}
\label{tab:scaling}
\begin{tabular}{cc}
\toprule
Munkaszálak & Játszma/mp \\
\midrule
1 & 0{,}53 \\
2 & 0{,}98 \\
4 & 1{,}53 \\
6 & \textbf{1{,}89} \\
\bottomrule
\end{tabular}
\end{table}

A \Tab{tab:scaling} alapján a 6 szálas konfiguráció \textbf{\num{3.5}-szörös gyorsulást} nyújt az egyszálúhoz képest, ami alátámasztja a párhuzamos architektúra hatékonyságát, még ha a skálázódás nem is tökéletesen lineáris.

A skálázódást vizsgáló \textbf{RQ2} eredményei azt mutatják, hogy az önjátszás teljesítménye 6 munkaszálon kb.\ \num{3.5}-szörösére nőtt az egyszálú futtatáshoz képest, a keresési sebesség (NPS) pedig 64-es kötegméretig skálázódik jól, ahol eléri a $\sim$\num{25000} NPS körüli csúcsot; a további skálázást elsősorban a Python GIL, illetve a host és a device közötti adatmozgatás korlátozza.

\subsection{RQ3: Tanulási dinamika és bajnokság}

A tanulási dinamikát az \Fig{fig:training_dynamics} foglalja össze. A bal oldali ábrák a stratégiai (policy) veszteséget, a jobb oldaliak az értékbecslési (value) MSE-t mutatják, felül az iterációk, alul az eltelt futási idő függvényében. A stratégiai veszteség szempontjából a \textbf{Nagy Áteresztőképesség} (zöld) konfiguráció adja a legjobb eredményt, a keresztentrópia kb.\ 2{,}7 körüli értéken stabilizálódik. Ezt követi a \textbf{Mély Keresés} (sárga) és a \textbf{Nagy Entrópia} (narancs) 3{,}3 és 3{,}5 közötti végértékkel. A \textbf{Hatékonyság} (lila) és különösen a \textbf{Referencia} (kék) konfigurációk ugyan az első $\sim$50 iterációban gyorsan csökkennek, utána azonban 4{,}2 és 4{,}6 közötti platóra állnak be, ami arra utal, hogy a gyakori frissítések miatt a döntési stratégia viszonylag korán „megfagy”. A görbék lokális fluktuációi természetesek a kis adatmennyiség és a rövid tanítási horizont miatt; az iterációról iterációra történő ingadozás jelentős része tanítási zajnak tekinthető, ezért a konfigurációk közötti különbségek értelmezésekor elsősorban a tartós trendekre (tartósan alacsonyabb, stabil szintekre) támaszkodom.

Az értékbecslési veszteség eltérő mintázatot mutat. A \textbf{Nagy Áteresztőképesség} MSE-je nagyon alacsony, $\sim$0{,}05 és 0{,}07 közötti tartományban marad. A \textbf{Hatékonyság} konfiguráció lassabban, de hasonlóan alacsony szintre (kb.\ 0{,}08 és 0{,}1 közötti tartományba) konvergál. A \textbf{Referencia} és a \textbf{Mély Keresés} görbéje valamivel magasabban, nagyjából 0{,}1 és 0{,}15 közötti tartományban stabilizálódik, míg a \textbf{Nagy Entrópia} esetén a veszteség a 100. iteráció után ismét növekedni kezd, és a tanítás végére $\sim$0{,}25 és 0{,}3 közötti értékre áll be. Ez arra utal, hogy a tartósan magas exploráció ugyan javítja a döntési stratégia sokféleségét, de a value-fej tanulását a zajos, időben változó célok megnehezítik.

Az időalapú összehasonlítás (alsó sor) a fenti trendeket futásidővel súlyozza. A \textbf{Hatékonyság} konfiguráció fejezi be leggyorsabban a tanulást (kb.\ 9~óra), de közben magas stratégiai veszteségen marad, és a későbbi Elo-mérések alapján ez a konfiguráció teljesít a leggyengébben. A többi konfiguráció 14 és 18 óra közötti futási időt igényel. A \textbf{Nagy Áteresztőképesség} és a \textbf{Mély Keresés} a leghosszabb, de ezek adják a legalacsonyabb végső stratégiai veszteséget. A \textbf{Nagy Áteresztőképesség} így a futásidő és a végső teljesítmény között kedvező kompromisszumot jelent, míg a \textbf{Referencia} konfiguráció hosszabb idő alatt is lényegesen rosszabb döntési stratégián marad.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{training_dynamics}
  \caption{Tanulási dinamika: policy- és value-veszteségek az iterációk függvényében.}
  \label{fig:training_dynamics}
\end{figure}

Az arénamérkőzések eredményeit az \Fig{fig:evaluation_win_rate} szemlélteti. Az átlagos győzelmi arány az első 20 és 30 közötti iterációban gyorsan konvergál az 50\%-os, kiegyenlített szint közelébe, és ezt követően mind az öt konfiguráció nagyon hasonló tartományban marad. A \textbf{Referencia} (kék) kb.\ 60\% körüli kezdő értékről fokozatosan esik vissza $\sim$50\% alá. A \textbf{Mély Keresés} (sárga) és a \textbf{Nagy Áteresztőképesség} (zöld) végig enyhén 50\% fölött, míg a \textbf{Nagy Entrópia} (narancs) és a \textbf{Hatékonyság} (lila) közvetlenül az 50\%-os vonal körül stabilizálódik. A tanítás végére a görbék mindössze néhány százalékpontos különbséggel, nagyjából 48 és 52\% között helyezkednek el, ezért a konfigurációk közötti eltérések a mérési bizonytalanságon belül maradnak.

Az árnyékolt sávok az iterációnkénti 24 arénapartiból számolt 95\%-os konfidenciaintervallumokat jelölik. A sávok nagy szélessége (tipikusan kb.\ $\pm$20 és 25 százalékpont közötti érték) azt mutatja, hogy az egyes iterációk közti ingadozások jelentős része statisztikai zaj, nem pedig tartós teljesítménykülönbség. Ez alátámasztja a folyamatos, AlphaZero-stílusú tanulás választását az AlphaGo Zero-féle szigorú aréna-kapuzással szemben. Ilyen zajos becslések mellett az előreléptetési döntések megbízhatatlanok lettek volna, ezért a jelen kísérletekben az arénamérkőzéseket kizárólag mérőszámként használtam, és nem engedtem, hogy a self-play modell kiválasztásáról döntsenek.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{evaluation_win_rate}
  \caption{Győzelmi arányok alakulása a tanítás során (95\% konfidenciaintervallummal).}
  \label{fig:evaluation_win_rate}
\end{figure}

A modellek relatív játékerejét körmérkőzéses (round-robin) bajnokságban mértem, két kézzel definiált referenciaügynökkel kiegészítve:
\begin{itemize}
    \item \textbf{Véletlen (\textit{Random}):} tisztán véletlenszerű lépések.
    \item \textbf{Mohó (\textit{Greedy}):} egyszerű, anyagelőnyt maximalizáló heurisztika.
\end{itemize}

A bajnokság végeredményét az \Fig{fig:tournament_elo} mutatja Elo-pontszámok formájában.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{tournament_elo}
  \caption{A bajnokság végeredménye (Elo-pontszámok).}
  \label{fig:tournament_elo}
\end{figure}

A körmérkőzéses bajnokság eredményei szerint a \textbf{Nagy Entrópia} konfiguráció (1249 Elo) teljesített a legjobban, egyedüliként megelőzve a \textbf{Mohó} (\textit{Greedy}) ágenst (1228 Elo). A többi tanult konfiguráció (\textbf{Referencia} 1192, \textbf{Mély Keresés} 1195, \textbf{Nagy Áteresztőképesség} 1198 Elo) szűk sávban, közvetlenül a Mohó alatt csoportosul; ez jelzi, hogy bár mindegyik modell elsajátított egy használható sakkstratégiát, a rendelkezésre álló időben egyik sem tudott a kézzel tervezett heurisztikánál érdemben erősebb játékot kialakítani.

A \textbf{Hatékonyság} modell 1164 Elo-val a mezőny végén végzett, és még a \textbf{Véletlen} ágensnél is gyengébbnek bizonyult (1169 Elo; a becsült Elo-pontszám alapján, a mérési zajt figyelembe véve). Ez arra utal, hogy a túl agresszív frissítési stratégia a hálózat instabil, irányt tévesztett tanulásához vezethet (katasztrofális felejtés vagy rossz lokális optimum), ezért az ilyen beállítások gyakorlatban kerülendők.

Fontos ugyanakkor kiemelni, hogy a bajnokság korlátozott játszmaszáma miatt az Elo-becslések bizonytalansága várhatóan több tíz pont nagyságrendű. A kisebb különbségek (például 1164 versus 1169 Elo) ezért inkább jelzésértékűek, mintsem statisztikailag egyértelműen szignifikáns eltérések; a konfigurációk relatív sorrendje megbízhatóbb információt ad, mint az abszolút értékek.

A tanulási dinamikára vonatkozó \textbf{RQ3} kapcsán az látszik, hogy a policy- és value-veszteséggörbék mind az öt konfiguráció esetén jól látható csökkenést mutatnak, különösen a \textbf{Nagy Áteresztőképesség} és \textbf{Mély Keresés} beállításoknál, ami azt jelzi, hogy rövid, 9 és 18 óra közötti tanítási horizonton kimutatható, stabil tanulás zajlik. Az \Fig{fig:training_dynamics} görbéi alapján a policy-veszteség a tanítás elején mért értékéhez képest a legtöbb konfigurációnál nagyságrendileg 40\%-os, míg az értékveszteség 50\%-os relatív javulást mutat; ez vizuálisan összhangban van a 3.\ fejezetben megfogalmazott sikerkritériumok nagyságrendjével.

\subsection{RQ4: Adatdiverzitás}

Az \Fig{fig:game_length_dist} a lejátszott játszmák hosszának eloszlását mutatja fél-lépésekben. A \textbf{Mély Keresés} (sárga) konfiguráció sűrűségfüggvénye adja a legmagasabb csúcsot. A maximum kb.\ 0{,}036 körül alakul, nagyjából 45 és 50 fél-lépés között. A \textbf{Hatékonyság} (lila) eloszlása a legkeskenyebb és legkoncentráltabb, csúcsa kb.\ 40 fél-lépés körül jelenik meg, ami arra utal, hogy a gyakori gradiens-frissítések (\texttt{samples\_per\_new\_game} = 8) és a kisebb kötegméret gyors, de viszonylag egyoldalú konvergenciához vezettek, így a játszmák többsége hasonló hosszúságú, kevés változatos végjátékkal.

A legmarkánsabban eltérő viselkedést a \textbf{Nagy Entrópia} (narancs) konfiguráció mutatja, amelynek eloszlása egyértelműen \emph{bimodális}. Egy első csúcs látható kb.\ 50 fél-lépésnél, míg egy második, alacsonyabb csúcs kb.\ 110 fél-lépés körül jelenik meg. Az első csúcs a középjátékban lezáruló partikat, a második a hosszú, végjátékig elnyúló játszmákat reprezentálja. A magas exploráció ($c_{\text{puct}} = 2.5$, Dirichlet $\alpha = 0.5$) egyrészt lehetővé teszi a döntő taktikai ütközéseket, másrészt gyakran megakadályozza a korai lépésismétléses döntetleneket, így széles skálán, sokféle pozíciót lefedve tud tanulni a modell. Ez a diverzitás összhangban van a konfiguráció kiemelkedő Elo-eredményével.

A \textbf{Mély Keresés} (sárga) és a \textbf{Nagy Áteresztőképesség} (zöld) eloszlásai hasonló, egymóduszú görbéket adnak. A tömegük zöme 40 és 60 fél-lépés között helyezkedik el, a \textbf{Nagy Áteresztőképesség} csúcsa azonban kissé jobbra tolódik, ami arra utal, hogy a sekélyebb keresés (64 szimuláció) ritkábban dönt el partikat már a középjátékban. A \textbf{Referencia} (kék) konfiguráció eloszlása a két szélsőség között helyezkedik el, mind csúcsmagasságban, mind pozícióban, ami jól illeszkedik a referencia jellegéhez.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{game_length_dist}
  \caption{A lejátszott játszmák hosszának eloszlása (fél-lépésekben).}
  \label{fig:game_length_dist}
\end{figure}

Az adatdiverzitást firtató \textbf{RQ4}-re a válasz, hogy a magas explorációs paraméterekkel futó \textbf{Nagy Entrópia} konfiguráció sokkal változatosabb, bimodális játszmahossz-eloszlást produkál, ami szélesebb pozíciókészletet eredményez. A Nagy Entrópia konfiguráció bimodális játszmahossz-eloszlása jól rezonál az RQ3 alatt mért Elo-előnnyel. A sokszínűbb tanítópozíciók rövid tanítási horizonton fontosabbnak bizonyultak, mint a puszta keresési mélység növelése.

\subsection{RQ5: Reprodukálhatóság}

A reprodukálhatóságot három szinten támogattam: (1) verziózott konfigurációs állományokkal, (2) expliciten kezelt véletlenszám-generátorokkal és determinisztikus beállításokkal, valamint (3) teljesen automatizált benchmark- és ábrageneráló eszközlánccal.

\medskip
\noindent\textbf{Konfigurációk és futtatási környezet.}
Minden kísérleti futás \texttt{YAML}-alapú konfigurációs fájlokból indul, amelyeket a \texttt{main.py} belépési pont \texttt{--config} és \texttt{--override} kapcsolókkal rétegez egymásra.
A konfigurációkezelő komponens minden futás elején egy \emph{végleges}, már felülbírált konfigurációt állít elő, amelyet a rendszer a \texttt{runs/\textless{}azonosító\textgreater{}/config/merged.\{yaml,json\}} állományokba ment.
Így a dolgozatban szereplő öt forgatókönyv (Referencia, Mély Keresés, Nagy Áteresztőképesség, Nagy Entrópia, Hatékonyság) minden hiperparaméter-beállítása (MCTS-szimulációk száma, kötegméret, explorációs paraméterek stb.) visszakereshető és más gépen is pontosan újrahasználható.
A Python-környezet főbb csomagjai a \texttt{requirements.txt}-ben rögzítettek; a kritikus függőség, a PyTorch, fix verzióval (\texttt{torch==2.8.0}) szerepel.

\medskip
\noindent\textbf{Véletlenszám-generátorok és determinisztika.}
A rendszer globális \texttt{SEED} paramétere (\texttt{config.SEED}) határozza meg, hogy a futás determinisztikus módban történjen-e.
Amennyiben a \texttt{SEED} értéke nem nulla, a \texttt{main.py} a tanítás indításakor inicializálja a Python beépített \texttt{random} modulját, a \texttt{NumPy} véletlenszám-generátorát, valamint a \texttt{torch} CPU- és GPU-RNG-it a megadott maggal.
Ezzel párhuzamosan beállítja a \texttt{torch.use\_deterministic\_algorithms(True)} opciót, a \texttt{CUBLAS\_WORKSPACE\_CONFIG} környezeti változót, letiltja a \texttt{cuDNN} adaptív \texttt{benchmark} módját, illetve a logikai processzorszámhoz igazítja az intra- és inter-op szálak számát.
Ez a kombináció gyakorlatban közel determinisztikus viselkedést eredményez ugyanazon hardveren, viszont egyes, GPU-n futó műveletek és a párhuzamos önjátszás miatt bitpontosan azonos futások nem minden esetben garantálhatók.
A teljesítménymérések és Elo-becslések emiatt várhatóan néhány tíz pont nagyságrendű szórást mutatnak.

Az elemző és benchmark szkriptek külön dedikált magokat használnak.
A \texttt{tools/benchmarks.py} modulban az inferencia-benchmark rögzített \texttt{2025}-ös Torch-maggal fut, az önjátszás áteresztőképességét mérő komponens a \texttt{SelfPlayEngine} belső RNG-jét \texttt{123}-as maggal inicializálja, míg a C++ sakkmag- és MCTS-benchmarkok véletlen FEN-állásait szintén paraméterezhető, alapértelmezésben \texttt{2025}-ös mag vezérli.
Az arénamérkőzésekre épülő bajnokság \texttt{NumPy} \texttt{default\_rng(2025)} generátorral készül, így a párosítások sorrendje és a lépésválasztások sztochasztikus komponense is kontrollált.

\medskip
\noindent\textbf{Artefaktum- és ábragenerálás.}
Minden, a dolgozatban szereplő tanítási metrika, eloszlás és Elo-összehasonlítás automatikusan generált.
A tanítás során a rendszer iterációnként CSV- és JSONL-formátumban naplózza a legfontosabb mutatókat (veszteségek, önjátszás- és tanítási idők, puffer- és resign-statisztikák), az arénamérkőzések PGN-fájljai pedig iterációnként elkülönített könyvtárakba kerülnek.
A \texttt{tools/generate\_artifacts.py} szkript ezeket az állományokat olvassa be, a konfigurációs metaadatokkal együtt, majd egységes betűtípus- és stílusbeállítások mellett automatikusan készíti el az ábrákat (tanulási dinamika, skálázódási görbék, játszmahossz-eloszlás, bajnoki Elo-eredmények).

A fenti mechanizmusok eredményeként a kísérletek reprodukálása a gyakorlatban a következő, jól definiált lépésekre bontható:
\begin{enumerate}
  \item A kód és a \texttt{requirements.txt} alapján a futtatási környezet felépítése, azonos vagy hasonló GPU-val.
  \item A kívánt konfiguráció kiválasztása (pl.\ \path{configs/run4_high_exploration.yaml}) és a teljes tanítás lefuttatása a \texttt{main.py} szkripttel, opcionálisan rögzített \texttt{SEED} értékkel.
  \item A benchmark-szkriptek (\texttt{tools/benchmarks.py}) futtatása opcionális teljesítménymérésekhez.
  \item Az ábrák teljes automatikus újragenerálása a \texttt{tools/generate\_artifacts.py} eszközzel, amely a \texttt{runs/} könyvtárból olvassa ki a metrikákat.
\end{enumerate}

A rendszer felépítése, a verziózott konfigurációk, az explicit RNG-kezelés és az automatizált artefaktum-pipeline együttesen biztosítja, hogy az itt bemutatott kísérleti eredmények azonos vagy hasonló hardverkörnyezetben számottevően kis szórással, gyakorlatban reprodukálhatók legyenek.

\section{Összefoglalás}
A mérések szerint a rendszer technikai teljesítménye (\num{56887} pozíció/mp, \num{25000} NPS) a kitűzött kutatási célok szempontjából elegendőnek bizonyult. A kísérleti eredmények alapján korlátozott erőforrások mellett az \textbf{exploráció maximalizálása} a kulcs a játékerő növeléséhez, nem pedig a puszta számítási mélység vagy a frissítési sebesség.

\chapter{Megbeszélés és korlátok}

Ebben a fejezetben az RQ1-től RQ5-ig terjedő kérdésekre kapott eredményeket értelmezem, majd kitérek a módszertan korlátaira.

\section{Eredmények értelmezése}

\subsection{Az exploráció dominanciája}

Az exploráció dominanciája elsősorban az RQ3 (tanulási dinamika) és RQ4 (adatdiverzitás) szempontjából releváns. Korlátozott erőforrás mellett a változatos adatgenerálás fontosabbnak bizonyult, mint a puszta keresési mélység. A magas explorációs beállítások ($\alpha=0.5$, $c_{\text{puct}}=2.5$) megakadályozták, hogy a rendszer túl korán passzív stratégiákhoz tapadjon. Míg a Hatékonyság forgatókönyv gyorsan konvergált egy szűk, de gyenge stratégiára (és ezzel elveszítette rugalmasságát), a Nagy Entrópia ágens szélesebb körű keresést végzett. Ez stabilabb játékot eredményezett, miközben a Hatékonyság konfiguráció a Random referencia szintjén vagy az alatt végzett, érdemi előny nélkül.

\subsection{A rendszer skálázhatósága}

A rendszer skálázhatósága (RQ1, RQ2) tekintetében a kötegelt levélértékelés és a virtuális veszteségre épülő konkurenciakezelés együtt \textbf{\num{3.5}-szörös} önjátszás-gyorsulást adott 6 szálon, miközben megőrizte a fa diverzitását. A NodePool indexalapú, vektoralapú allokációja biztosítja, hogy az átméretezés a pointerek érvényessége mellett is skálázható legyen. A fő szűk keresztmetszetek a Python GIL és a host és a device közötti adatmozgatás jelentik, ami jelzi, hogy több GPU vagy teljesen natív önjátszás további gyorsulást hozhat.

\section{Korlátok}

\subsection{Abszolút játékerő}

Bár a rendszer a saját, zárt skálán mért 1249 Elo-pontszámmal legyőzte a mohó (Greedy) ágenst, ez a szint nagyjából egy erősebb amatőr játékos szintjének felel meg, és nem hasonlítható közvetlenül sem a FIDE-, sem az online szerverek Elo-értékeihez. Az 1249-es Elo-érték így kizárólag a dolgozatomban definiált, zárt bajnokságon belül értelmezhető. A modell képes alapvető pozíciós elvek követésére (például a gyalogstruktúra védelmére), de hiányzik belőle a mély taktikai számolás képessége, amelyhez nagyságrendileg több (milliós nagyságrendű) önjátszásos játszmára lenne szükség. A szakirodalomban tipikusan több tízezer, sőt akár milliós nagyságrendű játszmára épülő, standardizált ellenfelek elleni méréseket használnak FIDE-közeli skálákhoz; dolgozatomban ilyen típusú, külső, Stockfish-alapú Elo-kalibrációt tudatosan nem végeztem.

Ez összhangban áll a Célkitűzések fejezetben megfogalmazott korlátozott hatókörrel. A cél a módszertan megvalósíthatóságának demonstrálása volt, nem pedig versenyképes motor fejlesztése.

\subsection{Számítási horizont}

A 300~iteráció és a 9 és 18 óra közötti futási idő a „megvalósíthatósági bizonyítás” kategóriájába esik. Az AlphaZero eredeti tanulmánya több ezer TPU-t használt, nagyságrendekkel nagyobb önjátszásos adatmennyiség mellett. Bár kísérleti validációval sikerült igazolni a módszertant, a versenyképes szint elérése ezen a hardveren hónapokat venne igénybe.

\chapter{Összefoglalás}

Dolgozatomban egy fogyasztói hardverre optimalizált, AlphaZero-típusú sakkprogramot mutattam be. A rendszer C++23-alapú keresőt és PyTorch-alapú tanítási modult egyesít, kifejezetten egy RTX 3070 Laptop GPU környezetére tervezve.

A fejlesztés három fő pillére az architekturális egyszerűsítés (a „Mindig világos” kanonikus nézet), a virtuális veszteségre épülő konkurenciakezelés és a kötegelt levélértékelés volt. A kanonikus nézet a bemeneti állapottér felére csökkentésével egyszerű, mégis hatékony mérnöki kompromisszumot jelent, amely jól illeszkedik a rendelkezésre álló hardverkorlátokhoz. Munkámban a hatását elsősorban kvalitatív módon vizsgálom, ugyanakkor nem törekszem a többi architekturális döntéstől való szigorú, kvantitatív elválasztására. A három pillér együttesen teszi lehetővé, hogy a rendszer korlátozott erőforrások mellett is elegendő keresési sebességet és áteresztőképességet érjen el. Az öt konfigurációt lefedő kísérleti validáció alapján a magas entrópiájú keresés bizonyult a leghatékonyabbnak a rendelkezésre álló adat- és időkeret mellett.

Az RQ1-től RQ5-ig terjedő kutatási kérdésekre adott válaszokat az alábbiakban foglalom össze:
\begin{enumerate}
\item \textbf{RQ1, Teljesítmény:} a C++23-alapú sakkmag \num{56887} pozíció/mp sebességet ért el, míg a GPU-alapú neurális következtetés \num{81519} pozíció/mp áteresztőképességet biztosított nagy kötegméreteknél. Ez legalább \num{5.3}-szoros gyorsulást jelent a sakkmag esetén a Python-alapú megoldáshoz képest, és nagyságrendileg tízszeresnél nagyobb gyorsulást a neurális következtetésben, ami elegendő az önjátszásos tanításhoz egynapos időkeretben.
\item \textbf{RQ2, Skálázhatóság:} az önjátszás teljesítménye 6 munkaszálon kb.\ \num{3.5}-szörösére nőtt az egyszálú futtatáshoz képest, a keresési sebesség (NPS) pedig 64-es kötegméretig skálázódik jól, ahol eléri a $\sim$\num{25000} NPS körüli csúcsot; a további skálázást a Python GIL és a host és a device közötti adatmozgatás korlátozza.
\item \textbf{RQ3, Tanulási dinamika:} a policy- és value-veszteség mind az öt vizsgált konfiguráció esetén jelentősen csökkent, különösen a \textbf{Nagy Áteresztőképesség} és \textbf{Mély Keresés} beállításoknál, ami azt mutatja, hogy rövid tanítási horizonton is érdemi, mérhető tanulás zajlik.
\item \textbf{RQ4, Adatdiverzitás:} a magas explorációs beállításokkal futó \textbf{Nagy Entrópia} konfiguráció bimodális játszmahossz-eloszlást és sokkal változatosabb tanítóadatot eredményezett, és a mérések alapján kis előnnyel felülmúlta a mohó heurisztikát a saját Elo-skálán, ami alátámasztja az adatdiverzitás szerepét korlátozott erőforrások mellett. Az itt közölt Elo-értékek a korlátozott játszmaszám miatt becslések, várhatóan néhány tíz pontos bizonytalansággal.
\item \textbf{RQ5, Reprodukálhatóság:} a teljesen automatizált benchmark- és artefaktumgeneráló szkriptek, valamint a YAML-konfigurációk rögzítik a hardver- és hiperparaméter-beállításokat, így a kísérleti futtatások más környezetben is megismételhetők, legfeljebb kisebb, a sztochasztikusságból adódó Elo-szórással.
\end{enumerate}

Dolgozatom elsődleges hozzájárulása egy keretrendszer, amely referenciapontot nyújt az AlphaZero-módszertan korlátozott erőforrású megvalósításához. A jövőbeli fejlesztési irányok közé tartozik a jelenlegi 6 blokkos háló fokozatos skálázása 10 és 20 blokk közötti méretig, és annak vizsgálata, hogy az itt bemutatott paraméterezés mellett hogyan változik a tanulási dinamika. További lehetőség a tanult környezetmodellek (pl.\ MuZero-típusú megközelítések) bevezetése, ahol a környezetmodell is tanulható lenne. Emellett fontos jövőbeli lépés lehet a rendszer adaptálása elosztott (kliens-szerver) környezetre. Ezek a fejlesztések közelebb vihetnek egy olyan AlphaZero-típusú sakkmotorhoz, amely már nemcsak kísérleti, hanem gyakorlati szempontból is versenyképes játékerőt képvisel.

\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Irodalomjegyzék}
\begingroup
\singlespacing
\footnotesize
\begin{thebibliography}{99}\setlength{\itemsep}{\baselineskip}

\bibitem{amdahl1967}
Amdahl, G. M.
Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities.
In: \emph{Proceedings of the April 18--20, 1967, Spring Joint Computer Conference}, 483--485 (1967).\\
\doi{10.1145/1465482.1465560}

\bibitem{auer2002ucb}
Auer, P., Cesa-Bianchi, N., Fischer, P.
Finite-time Analysis of the Multiarmed Bandit Problem.
\emph{Machine Learning} 47(2--3):235--256 (2002).\\
\doi{10.1023/A:1013689704352}

\bibitem{bengio2009curriculum}
Bengio, Y., Louradour, J., Collobert, R., Weston, J.
Curriculum Learning.
In: \emph{Proceedings of the 26th International Conference on Machine Learning (ICML 2009)}, 41--48 (2009).\\
\doi{10.1145/1553374.1553380}

\bibitem{browne2012}
Browne, C. B., Powley, E., Whitehouse, D., \emph{et al.}
A Survey of Monte Carlo Tree Search Methods.
\emph{IEEE Transactions on Computational Intelligence and AI in Games} 4(1):1--43 (2012).\\
\doi{10.1109/TCIAIG.2012.2186810}

\bibitem{campbell2002deepblue}
Campbell, M., Hoane, A. J., Hsu, F.-H.
Deep Blue.
\emph{Artificial Intelligence} 134(1--2):57--83 (2002).\\
\doi{10.1016/S0004-3702(01)00129-1}

\bibitem{cpwLMR}
Chess Programming Wiki.
Late Move Reductions.\\
\url{https://www.chessprogramming.org/Late_Move_Reductions}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{cpwNullMove}
Chess Programming Wiki.
Null Move Pruning.\\
\url{https://www.chessprogramming.org/Null_Move_Pruning}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{coulom2006cg}
Coulom, R.
Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search.
In: \emph{Proceedings of the 5th International Conference on Computers and Games (CG 2006)}, LNCS 4630, 72--83 (2007).\\
\doi{10.1007/978-3-540-75538-8_7}

\bibitem{crazyara}
Czech, J., Willig, M., Beyer, A., Kersting, K., Fürnkranz, J.
Learning to Play the Chess Variant Crazyhouse Above World Champion Level with Deep Neural Networks and Human Data.
\emph{Frontiers in Artificial Intelligence} 3:24 (2020).\\
\doi{10.3389/frai.2020.00024}

\bibitem{gelly2006mogo}
Gelly, S., Wang, Y.
Exploration Exploitation in Go: UCT for Monte-Carlo Go.
In: \emph{NIPS 2006 Workshop on Online Trading of Exploration and Exploitation}, Whistler, Canada (2006).

\bibitem{gelly2007rave}
Gelly, S., Silver, D.
Combining Online and Offline Knowledge in UCT.
In: \emph{Proceedings of the 24th International Conference on Machine Learning (ICML 2007)}, 273--280 (2007).\\
\doi{10.1145/1273496.1273531}

\bibitem{he2016resnet}
He, K., Zhang, X., Ren, S., Sun, J.
Deep Residual Learning for Image Recognition.
In: \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)}, 770--778 (2016).\\
\doi{10.1109/CVPR.2016.90}

\bibitem{hu2018senet}
Hu, J., Shen, L., Sun, G.
Squeeze-and-Excitation Networks.
In: \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)}, 7132--7141 (2018).\\
\doi{10.1109/CVPR.2018.00745}

\bibitem{pybind11}
Jakob, W., Rhinelander, J., Moldovan, D.
pybind11: Seamless operability between C++11 and Python.\\
\url{https://github.com/pybind/pybind11}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{knuth1975alphabeta}
Knuth, D. E., Moore, R. W.
An Analysis of Alpha-Beta Pruning.
\emph{Artificial Intelligence} 6(4):293--326 (1975).\\
\doi{10.1016/0004-3702(75)90019-3}

\bibitem{kocsis2006uct}
Kocsis, L., Szepesvári, C.
Bandit Based Monte-Carlo Planning.
In: \emph{Machine Learning: ECML 2006}, LNCS 4212, 282--293 (2006).\\
\doi{10.1007/11871842_29}

\bibitem{lczero}
Leela Chess Zero (Lc0). Open-source neural network chess engine.\\
Project: \url{https://lczero.org/}.\\
GitHub: \url{https://github.com/LeelaChessZero}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{lichess}
Lichess.
\emph{chess-openings dataset}.\\
\url{https://github.com/lichess-org/chess-openings}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{loshchilov2017sgdr}
Loshchilov, I., Hutter, F.
SGDR: Stochastic Gradient Descent with Warm Restarts.
In: \emph{Proceedings of the 5th International Conference on Learning Representations (ICLR 2017)}.\\
\url{https://openreview.net/forum?id=Skq89Scxx}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{micikevicius2018amp}
Micikevicius, P., Narang, S., Alben, J., \emph{et al.}
Mixed Precision Training.
In: \emph{Proceedings of the 6th International Conference on Learning Representations (ICLR 2018)}.\\
\url{https://openreview.net/forum?id=r1gs9JgRZ}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{nasu2018nnue}
Nasu, Y.
Efficiently Updatable Neural-Network-based Evaluation Function for computer Shogi.
Technical report, 28th World Computer Shogi Championship, 2018.\\
\url{https://github.com/asdfjkl/nnue}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{pytorch}
Paszke, A., Gross, S., Massa, F., \emph{et al.}
PyTorch: An Imperative Style, High-Performance Deep Learning Library.
In: \emph{Advances in Neural Information Processing Systems 32} (NeurIPS 2019), 8024--8035.\\
\url{https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{pythonchess}
Fiekas, N.
python-chess: a chess library for Python.\\
\url{https://python-chess.readthedocs.io}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{schrittwieser2020muzero}
Schrittwieser, J., Antonoglou, I., Hubert, T., \emph{et al.}
Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.
\emph{Nature} 588(7839):604--609 (2020).\\
\doi{10.1038/s41586-020-03051-4}

\bibitem{shannon1950chess}
Shannon, C. E.
Programming a Computer for Playing Chess.
\emph{The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science} 41(314):256--275 (1950).\\
\doi{10.1080/14786445008521796}

\bibitem{silver2016alphago}
Silver, D., Huang, A., Maddison, C. J., \emph{et al.}
Mastering the Game of Go with Deep Neural Networks and Tree Search.
\emph{Nature} 529(7587):484--489 (2016).\\
\doi{10.1038/nature16961}

\bibitem{silver2017chess}
Silver, D., Schrittwieser, J., Simonyan, K., \emph{et al.}
Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.
\emph{arXiv preprint} arXiv:1712.01815 (2017).\\
\url{https://arxiv.org/abs/1712.01815}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{silver2017go}
Silver, D., Schrittwieser, J., Simonyan, K., \emph{et al.}
Mastering the Game of Go without Human Knowledge.
\emph{Nature} 550(7676):354--359 (2017).\\
\doi{10.1038/nature24270}

\bibitem{silver2018science}
Silver, D., Hubert, T., Schrittwieser, J., \emph{et al.}
A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go Through Self-Play.
\emph{Science} 362(6419):1140--1144 (2018).\\
\doi{10.1126/science.aar6404}

\bibitem{stockfish}
Stockfish developers.
Stockfish chess engine.\\
\url{https://stockfishchess.org}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{wu2019katago}
Wu, D. J.
Accelerating Self-Play Learning in Go.
\emph{arXiv preprint} arXiv:1902.10565 (2019).\\
\url{https://arxiv.org/abs/1902.10565}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{zobrist1970}
Zobrist, A. L.
A New Hashing Method with Application for Game Playing.
\emph{Technical Report 88}, Computer Sciences Department, University of Wisconsin, Madison, WI (1970).\\
\url{https://research.cs.wisc.edu/techreports/1970/TR88.pdf}.\\
Utolsó megtekintés: 2025. 12. 11.

\bibitem{harris2020numpy}
Harris, C. R., \emph{et al.}
Array Programming with NumPy.
\emph{Nature} 585(7825):357--362 (2020).\\
\doi{10.1038/s41586-020-2649-2}

\bibitem{hunter2007matplotlib}
Hunter, J. D.
Matplotlib: A 2D Graphics Environment.
\emph{Computing in Science \& Engineering} 9(3):90--95 (2007).\\
\doi{10.1109/MCSE.2007.55}

\bibitem{waskom2021seaborn}
Waskom, M. L.
Seaborn: Statistical Data Visualization.
\emph{Journal of Open Source Software} 6(60):3021 (2021).\\
\doi{10.21105/joss.03021}

\end{thebibliography}
\endgroup

\cleardoublepage
\chapter*{Nyilatkozat}
\thispagestyle{plain}
\phantomsection
\addcontentsline{toc}{chapter}{Nyilatkozat}

\noindent Alulírott \AuthorName{} \StudentProgram{} szakos hallgató, kijelentem, hogy a dolgozatomat a Szegedi Tudományegyetem, Informatikai Intézet \DepartmentNameText{}én készítettem, \StudentProgram{} diploma megszerzése érdekében.

\medskip

\noindent Kijelentem, hogy a dolgozatot más szakon korábban nem védtem meg, saját munkám eredménye, és csak a hivatkozott forrásokat (szakirodalom, eszközök, stb.) használtam fel. Tudomásul veszem, hogy szakdolgozatomat a Szegedi Tudományegyetem Diplomamunka Repozitóriumában tárolja.

\vspace{18mm}
\begin{flushright}
Szeged, \SubmissionDate
\end{flushright}

\vspace{14mm}
\begin{flushright}
\makebox[6cm]{\hrulefill}\\
aláírás
\end{flushright}

\cleardoublepage
\chapter*{Köszönetnyilvánítás}
\thispagestyle{plain}
\phantomsection
\addcontentsline{toc}{chapter}{Köszönetnyilvánítás}

Köszönettel tartozom témavezetőmnek a szakmai iránymutatásért és az értékes visszajelzésekért. Hálás vagyok családomnak és barátaimnak a támogatásért és bátorításért. Köszönet illeti a fejlesztői közösséget is a nyílt forráskódú eszközök és könyvtárak elérhetővé tételéért, amelyek e munka szakmai alapját adták.

\end{document}