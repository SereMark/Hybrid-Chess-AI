%!TeX program = xelatex
\documentclass[12pt,a4paper]{report}

\usepackage{iftex}
\usepackage{mathtools}

\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp}
  \usepackage{newtxtext,newtxmath}
\else
  \usepackage{fontspec}
  \setmainfont{Times New Roman}
  \setsansfont{Arial}
  \IfFontExistsTF{Consolas}
    {\setmonofont{Consolas}}
    {\setmonofont{Courier New}}
  \usepackage{unicode-math}
  \setmathfont{Cambria Math}
\fi

\usepackage[magyar]{babel}
\usepackage[sort,nocompress]{cite}

\usepackage[autostyle]{csquotes}

\usepackage[a4paper,left=25mm,right=25mm,top=25mm,bottom=25mm,bindingoffset=10mm]{geometry}
\usepackage{setspace}
\onehalfspacing
\raggedbottom
\usepackage{microtype}
\ifPDFTeX
  \DisableLigatures{encoding=T1,family=ntxtlf}
\fi
\frenchspacing
\clubpenalty=10000
\widowpenalty=10000
\displaywidowpenalty=10000
\usepackage{siunitx}
\sisetup{output-decimal-marker = {,},group-separator = {\,}}

\usepackage{graphicx}
\graphicspath{{figures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg}
\usepackage{float}
\usepackage{booktabs}

\usepackage{listings}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Bemenet:}}
\renewcommand{\algorithmicensure}{\textbf{Kimenet:}}
\floatname{algorithm}{Algoritmus}
\renewcommand{\listalgorithmname}{Algoritmusok jegyzéke}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\ifPDFTeX
  \lstset{inputencoding=utf8}
\fi

\lstset{style=mystyle,
  columns=fullflexible,
  literate={á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
           {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
           {ö}{{\"o}}1 {ü}{{\"u}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
           {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
           {×}{{\texttimes}}1
}

\renewcommand{\lstlistingname}{Kódrészlet}
\renewcommand{\lstlistlistingname}{Kódrészletek jegyzéke}

\usepackage{chngcntr}
\counterwithin{figure}{chapter}
\counterwithin{table}{chapter}
\counterwithin{algorithm}{chapter}
\makeatletter
\AtBeginDocument{%
  \@ifundefined{c@lstlisting}{}{%
    \counterwithin{lstlisting}{chapter}%
  }%
}
\makeatother
\counterwithin{equation}{chapter}

\usepackage[unicode,hypertexnames=false]{hyperref}
\usepackage{xurl}
\usepackage{bookmark}
\usepackage[nameinlink,noabbrev]{cleveref}
\crefname{chapter}{fejezet}{fejezetek}
\Crefname{chapter}{Fejezet}{Fejezetek}
\crefname{section}{szakasz}{szakaszok}
\Crefname{section}{Szakasz}{Szakaszok}
\crefname{figure}{ábra}{ábrák}
\Crefname{figure}{Ábra}{Ábrák}
\crefname{table}{táblázat}{táblázatok}
\Crefname{table}{Táblázat}{Táblázatok}
\crefname{equation}{képlet}{képletek}
\Crefname{equation}{Képlet}{Képletek}
\crefname{algorithm}{algoritmus}{algoritmusok}
\Crefname{algorithm}{Algoritmus}{Algoritmusok}
\crefname{lstlisting}{kódrészlet}{kódrészletek}
\Crefname{lstlisting}{Kódrészlet}{Kódrészletek}
\urlstyle{same}

\newcommand{\ThesisTitle}{Neurális hálózatokat és Monte Carlo fakeresést kombináló hibrid sakkrobot fejlesztése nyitókönyv integrációval}
\newcommand{\Fig}[1]{\ref{#1}.~ábra}
\newcommand{\Tab}[1]{\ref{#1}.~táblázat}
\hypersetup{
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  urlcolor=blue,
  pdfauthor={Sere Gergő Márk},
  pdftitle={\ThesisTitle},
  pdfsubject={Szakdolgozat},
  pdfcreator={LaTeX}
}

\newcommand{\doi}[1]{\href{https://doi.org/#1}{doi:\nolinkurl{#1}}}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\nouppercase\leftmark}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter. fejezet #1}{}}
\setlength{\headheight}{14.5pt}
\fancypagestyle{plain}{%
  \fancyhf{}%
  \rfoot{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

\usepackage{titlesec}
\titleformat{\chapter}[hang]{\bfseries\fontsize{14pt}{16pt}\selectfont}{\thechapter.\quad}{0pt}{}
\titleformat{\section}[hang]{\bfseries\fontsize{12pt}{14pt}\selectfont}{\thesection\quad}{0pt}{}
\titleformat{\subsection}[hang]{\bfseries\fontsize{12pt}{14pt}\selectfont}{\thesubsection\quad}{0pt}{}
\titlespacing*{\chapter}{0pt}{\baselineskip}{\baselineskip}
\titlespacing*{\section}{0pt}{\baselineskip}{\baselineskip}
\titlespacing*{\subsection}{0pt}{\baselineskip}{\baselineskip}

\usepackage{caption}
\captionsetup{font=small,labelfont=bf}
\usepackage{enumitem}
\setlist[itemize]{noitemsep,topsep=2pt}
\setlist[enumerate]{noitemsep,topsep=2pt}

\pdfstringdefDisableCommands{%
  \def\H#1{#1}%
}

\newcommand{\AuthorName}{Sere Gergő Márk}
\newcommand{\StudentProgram}{programtervező informatikus BSc}
\newcommand{\SupervisorName}{Dr.~Békési József, főiskolai tanár}
\newcommand{\FacultyNameDisplay}{Szegedi Tudományegyetem\\Természettudományi és Informatikai Kar\\Informatikai Intézet}
\newcommand{\DepartmentNameDisplay}{Számítástudomány Alapjai Tanszék}
\newcommand{\FacultyNameText}{Szegedi Tudományegyetem Természettudományi és Informatikai Kar Informatikai Intézet}
\newcommand{\DepartmentNameText}{Számítástudomány Alapjai Tanszék}
\newcommand{\SubmissionDate}{2025. december 7.}

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}

\begin{document}
\renewcommand{\abstractname}{Tartalmi összefoglaló}

\setlength{\emergencystretch}{12em}

\hypersetup{pageanchor=false}
\begin{titlepage}
  \thispagestyle{empty}
  \begin{center}
    \vspace*{0.5cm}
    \includegraphics[width=3.2cm]{SZTE_cimer}\par
    \vspace{0.5cm}
    {\Large \textbf{\FacultyNameDisplay}\par}
    \vspace{0.5cm}
    {\large \textbf{\DepartmentNameDisplay}\par}
    \vspace{2.0cm}
    {\Large \textbf{Szakdolgozat}\par}
    \vspace{12pt}
    {\LARGE \textbf{\ThesisTitle}\par}
    \vspace{4cm}
    \begin{minipage}[t]{0.46\textwidth}
      \raggedright
      {\bfseries Készítette}\par
      \rule{0.35\textwidth}{0.4pt}\par
      {\large \AuthorName}\par
      \StudentProgram
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.46\textwidth}
      \raggedleft
      {\bfseries Témavezető}\par
      \rule{0.35\textwidth}{0.4pt}\par
      {\large \SupervisorName}\par
      \DepartmentNameDisplay
    \end{minipage}
    \vspace{1.0cm}\par
    {\large Szeged, \SubmissionDate\par}
  \end{center}
  \vfill
\end{titlepage}

\clearpage
\pagenumbering{arabic}
\setcounter{page}{2}
\hypersetup{pageanchor=true}

\chapter*{Feladatkiírás}
\thispagestyle{plain}
\phantomsection
\addcontentsline{toc}{chapter}{Feladatkiírás}
\noindent\textbf{Téma megnevezése:} Neurális hálózatokat és Monte Carlo fakeresést kombináló hibrid sakkrobot fejlesztése nyitókönyv integrációval

\noindent\textbf{Feladat rövid leírása:} Olyan AlphaZero-típusú~\cite{silver2017chess} sakkprogram megvalósítása, amelyben a lépésgenerálás és a PUCT-alapú Monte Carlo fakeresés (MCTS) C++ nyelven valósul meg. A lépésirány- és értékbecslést reziduális konvolúciós neurális háló (CNN) végzi. A rendszer támogassa az önjátszásos tanítást, a kötegelt kiértékelést, a telemetriai méréseket, valamint az arénamérkőzéseken alapuló modellértékelést és monitorozást. A nyitókönyv biztosítsa a kezdőállások minél nagyobb sokféleségét.

\noindent\textbf{Részfeladatok:}
\begin{enumerate}
  \item Bitboard-alapú sakkmag és szabályosságellenőrzés implementálása korszerű C++-ban (C++23).
  \item PUCT-alapú MCTS megvalósítása adaptív paraméterezéssel, Dirichlet-zajjal és kötegelt kiértékelő interfésszel.
  \item Reziduális CNN tervezése és betanítása önjátszásból származó adatra; aszinkron, automatikus kevert pontosságú (AMP) inferencia.
  \item Visszajátszási puffer, eredménymegállapítás (adjudikáció), feladás és arénamérkőzések implementálása, valamint telemetria.
  \item Teljesítménymérés és összehasonlítás kiinduló megoldásokkal; reprodukálhatóság megvalósítása
\end{enumerate}

\clearpage
\thispagestyle{plain}
\chapter*{\abstractname}
\phantomsection
\addcontentsline{toc}{chapter}{\abstractname}
A dolgozat egy neurális hálózatokat és Monte Carlo fakeresést kombináló, AlphaZero-típusú hibrid sakkprogram fejlesztését mutatja be korlátozott erőforrású környezetben.

Az AlphaZero-típusú, megerősítéses tanuláson (\textit{reinforcement learning}, RL) alapuló sakkprogramok gyakorlati reprodukálása jellemzően ipari léptékű számítási kapacitást (például TPU-klasztereket) igényel. Ez a forrásigény megnehezíti a módszertan elérhetőségét egyetemi kutatók és hobbifejlesztők számára. A dolgozat azt vizsgálja, hogy egyetlen fogyasztói GPU-n (RTX~3070 Laptop) és körülbelül 15~órás tanítási időkeretben milyen mérnöki döntésekkel valósítható meg egy kísérleti célokra alkalmas, AlphaZero-típusú sakkprogram.

A bemutatott rendszer C++23-alapú sakkmotort és PUCT-alapú Monte Carlo fakeresést (MCTS) kombinál egy PyTorch-ban implementált, reziduális konvolúciós neurális hálózattal (CNN). A megoldás kötegelt kiértékelést, önjátszást és arénamérkőzéseket alkalmaz. A korlátozott erőforrások és az adatdiverzitás közötti egyensúly megteremtése érdekében a rendszer tömör bemeneti reprezentációt és nyitókönyvet~\cite{lichess} használ.

A dolgozat főbb hozzájárulásai a következők: korlátos erőforrásokra optimalizált,
C++/Python hibrid AlphaZero-típusú sakkmotor megvalósítása; a „Mindig világos”
kanonikus nézet (\textit{Canonical Always White}) bemutatása és elemzése; öt
különböző konfiguráció szisztematikus kísérleti összehasonlítása; valamint egy
teljesen automatizált benchmark- és reprodukciós feldolgozási lánc kialakítása.

Azonos hardverkörnyezetben öt különböző konfiguráció került tesztelésre. A \textbf{Nagy Entrópia} konfiguráció rövid futási idő alatt is 1249-es, saját skálán mért Elo-pontszámot ért el, és felülmúlta a mohó heurisztikát, miközben a mélyebb, de kevésbé változatos keresést alkalmazó beállítások gyengébben teljesítettek. Méréseim igazolták, hogy rövid futási idő esetén a generált játszmák sokszínűségét biztosító stratégia fontosabb, mint a puszta keresési mélység.

A dolgozat öt kutatási kérdésre (RQ1-től RQ5-ig) keresi a választ, amelyek a rendszer mérnöki teljesítményét, skálázhatóságát, tanulási dinamikáját, az adatdiverzitást és a kísérletek reprodukálhatóságát vizsgálják.

\textbf{Kulcsszavak:} AlphaZero, Monte Carlo fakeresés (MCTS), megerősítéses tanulás (RL), hibrid architektúra, GPU-gyorsítás, kísérleti validáció
\vfill
\clearpage
\tableofcontents
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Táblázatok jegyzéke}
\listoftables
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Ábrák jegyzéke}
\listoffigures
\cleardoublepage

\chapter*{Rövidítések jegyzéke}
\thispagestyle{plain}
\phantomsection
\addcontentsline{toc}{chapter}{Rövidítések jegyzéke}
\begin{itemize}
  \item \textbf{AMP}: automatikus kevert pontosság (\textit{Automatic Mixed Precision}, PyTorch-környezetben alkalmazott technika)
  \item \textbf{CNN}: konvolúciós neurális hálózat (\textit{Convolutional Neural Network})
  \item \textbf{CPU}: központi feldolgozóegység (\textit{Central Processing Unit})
  \item \textbf{EMA}: exponenciális mozgóátlag (\textit{Exponential Moving Average})
  \item \textbf{GIL}: globális interpreter zár (\textit{Global Interpreter Lock})
  \item \textbf{GPU}: grafikus feldolgozóegység (\textit{Graphics Processing Unit})
  \item \textbf{JSON}: strukturált adatcsere-formátum (\textit{JavaScript Object Notation})
  \item \textbf{LRU}: legrégebben használt (\textit{Least Recently Used})
  \item \textbf{MCTS}: Monte Carlo fakeresés (\textit{Monte Carlo Tree Search})
  \item \textbf{NPS}: csomópont másodpercenként (\textit{Nodes Per Second})
  \item \textbf{PUCT}: UCT-variáns döntési prior súlyozással (\textit{Policy UCT / Prioritized UCT})
  \item \textbf{RL}: megerősítéses tanulás (\textit{Reinforcement Learning})
  \item \textbf{RQ}: kutatási kérdés (\textit{Research Question})
  \item \textbf{SGD}: sztochasztikus gradiensmódszer (\textit{Stochastic Gradient Descent})
  \item \textbf{TDP}: tervezett hőteljesítmény (\textit{Thermal Design Power})
  \item \textbf{TPU}: tenzorprocesszor-egység (\textit{Tensor Processing Unit})
  \item \textbf{UCT}: fákra alkalmazott felső konfidenciahatár (\textit{Upper Confidence bounds applied to Trees})
\end{itemize}

\cleardoublepage
\chapter{Bevezetés}

Az AlphaZero~\cite{silver2017chess} 2017-ben látványosan bemutatta, hogy a megerősítéses tanulás (RL) sikerrel alkalmazható komplex, teljes információjú játékokban is. Ugyanakkor a módszer gyakorlati reprodukálása jellemzően ipari léptékű erőforrásokat (például TPU-klasztereket) feltételez. Ez a magas belépési küszöb megnehezíti a technológia oktatását és kutatását, különösen a hazai felsőoktatási intézményekben. A dolgozatban a „sakkrobot” kifejezés olyan szoftveres sakkprogramot jelöl, amely egy keresőmotort és egy neurális hálózatot kombinál.

A dolgozat fő motivációja a technológia elérhetővé tétele hallgatók és kutatócsoportok számára. A cél egy olyan, korlátos erőforrásokra optimalizált mérnöki megközelítés kidolgozása, amely lehetővé teszi, hogy egyetlen fogyasztói GPU-n is érdemi kísérleteket lehessen végezni AlphaZero-típusú sakkágensekkel.

A dolgozat azt vizsgálja, hogy egy NVIDIA RTX 3070 Laptop GPU-val, korlátozott, körülbelül 15~órás tanítási időkeretben milyen architekturális és hiperparaméter-beli kompromisszumokkal hozható létre működőképes, önjátszással tanuló sakkprogram. Az értékelés során öt eltérő konfiguráció kerül összehasonlításra a keresési mélység, az entrópia-alapú exploráció és a frissítési gyakoriság szempontjából.

A dolgozat főbb eredményei és hozzájárulásai az alábbiakban foglalhatók össze:
\begin{itemize}
  \item \textbf{Architekturális egyszerűsítés:} A „Mindig világos” kanonikus nézet (\textit{Canonical Always White}) bevezetésével a bemeneti állapottér komplexitása érdemben csökken, ami mérsékli az erőforrás-igényes adataugmentáció szükségességét.
  \item \textbf{Hibrid architektúra:} A rendszer a számításigényes feladatokat (lépésgenerálás, MCTS) nagy teljesítményű C++23 kóddal, míg a tanítást és a kötegelt kiértékelést rugalmas, PyTorch-alapú környezetben valósítja meg. A két réteg között egy vékony, minimális adatmásolást igénylő illesztőfelület biztosítja a hatékony adatcserét.
  \item \textbf{Eredmények röviden:} A \textbf{Nagy Entrópia} konfiguráció érte el a legjobb, saját skálán mért Elo-pontszámot, és a mérések alapján kis, de kimutatható előnyt mutatott a többi beállítással szemben, ami a diverzitás fontosságára utal rövid tanítási horizonton.
  \item \textbf{Reprodukálhatóság:} Teljesen automatizált benchmark- és artefaktumgeneráló szkriptek, valamint verziózott YAML-konfigurációk rögzítik a hardver- és hiperparaméter-beállításokat, így a kísérleti futtatások azonos beállítások mellett más környezetben is megismételhetők.
\end{itemize}

\medskip
A szakdolgozat készítése során két generatív mesterségesintelligencia-alapú nyelvi modell családot használtam kizárólag kiegészítő segédeszközként: az OpenAI ChatGPT-t (GPT-4o és GPT-5 modell) és a Google Gemini 2.5-öt. Alkalmazásuk a tanulási folyamat támogatására korlátozódott (szakirodalom gyors áttekintése, nyelvi-stiláris ellenőrzés), nem a dolgozat érdemi tartalmának előállítására. Konkrétan vázlatos összefoglalók készítése, alapfogalmak tisztázása, valamint nyelvhelyességi javaslatok kérése során vettem igénybe ezeket az eszközöket. Nem használtam őket bekezdések vagy fejezetek automatikus létrehozására, és nem vettem igénybe az MI-t az algoritmusok, a forráskód vagy a szakmai következtetések megalkotásához; ezek a saját, önálló munkám eredményei.

Az MI által javasolt tartalmakat minden esetben kritikusan, segédeszközként kezeltem: a kapott információkat független, hivatkozott források alapján ellenőriztem, a nyelvi javaslatokat pedig a dolgozat érvrendszeréhez és saját megfogalmazásomhoz igazítva építettem be. A dolgozat tartalmáért és esetleges hibáiért teljes egészében én vállalom a felelősséget. Az MI használata az „A mesterséges intelligencia elfogadható és felelős használata a Szegedi Tudományegyetemen Hallgatói segédlet a mesterséges intelligencia tanulásban történő tudatos, hatékony és etikus alkalmazásához” című útmutatóban rögzített alapelvekkel összhangban, a hallgatói önálló munka elsődlegességét tiszteletben tartva történt.

A dolgozat felépítése a következő: az \textbf{Irodalmi áttekintés} fejezet a kapcsolódó szakirodalmat foglalja össze. Ezt követi a \textbf{Célkitűzések és módszertan} fejezet, amely meghatározza az RQ1-től RQ5-ig terjedő kutatási kérdéseket és a kísérleti elrendezést. A \textbf{Rendszer tervezése és implementáció} fejezet részletesen bemutatja a hibrid architektúrát és a teljesítménykritikus komponensek tervezési döntéseit. Az \ref{ch:experiments}.\ fejezet az öt vizsgált forgatókönyv eredményeit ismerteti, végül pedig a \textbf{Megbeszélés és korlátok} fejezet a skálázhatósági tapasztalatokat és a kutatási tanulságokat összegzi.


\chapter{Irodalmi áttekintés}

\section{Klasszikus sakkprogramozás}

A számítógépes sakk Shannon úttörő munkájával indult \cite{shannon1950chess}, aki két fő megközelítést különböztetett meg: az „A-típusú” stratégiát, amely minden lehetséges lépést megvizsgál egy adott mélységig, és a „B-típusú” stratégiát, amely szelektíven csak az ígéretesnek ítélt lépésekre fókuszál. A sakkmotorok fejlődése évtizedekig az A-típusú (nyers erő, \textit{brute-force}) megközelítés finomhangolását jelentette.

A klasszikus sakkprogramok működési elve a Minimax-elven alapul. Mivel a sakkjátékfa Shannon által becsült komplexitása nagyságrendileg $10^{120}$ lehetséges játék~\cite{shannon1950chess}, a teljes játékfa bejárása gyakorlatilag lehetetlen (miközben a lehetséges sakktáblapozíciók száma is óriási, $\approx 10^{43}$ és $10^{47}$ közötti nagyságrend), ezért a programok korlátozott mélységű keresést végeznek. A levelekben heurisztikus kiértékelő függvényt (\textit{evaluation function}) alkalmaznak. Ez a függvény jellemzően az anyagelőny, a pozicionális tényezők (pl.~gyalogszerkezet, tisztek aktivitása) és a biztonság (pl.~királyszárny) súlyozott összegeként áll elő, amelyet nagymesterek és programozók évtizedeken át finomhangoltak.

A keresési tér hatékony bejárását az alfa-béta vágás (Alpha-Beta pruning) \cite{knuth1975alphabeta} teszi lehetővé, amely matematikailag garantáltan ugyanazt az eredményt adja, mint a teljes Minimax. Az irreleváns ágak levágásával (ahol a lépés már biztosan rosszabb, mint egy korábban talált alternatíva) a módszer ideális rendezés esetén nagyjából a négyzetgyökére csökkenti a keresési fa effektív elágazási tényezőjét. A modern motorok ezt tovább javítják olyan heurisztikákkal, mint a null-move vágás \cite{cpwNullMove} (ahol a lépés jogának átadása mellett keresés történik, feltételezve, hogy ha így is előny áll fenn, az eredeti állás még jobb) és a késői lépésredukció (Late Move Reductions, LMR)~\cite{cpwLMR}.

A Deep Blue 1997-es győzelme Kaszparov felett \cite{campbell2002deepblue} rámutatott a számítógépes sakk nagy potenciáljára, de ez a győzelem specifikus hardverre és hatalmas kézi tudásbázisra épült. A mai legjobbak közé tartozó klasszikus motor, a Stockfish \cite{stockfish} is ezt a vonalat képviseli, bár ma már NNUE (Efficiently Updatable Neural Network)~\cite{nasu2018nnue} technológiával ötvözi a heurisztikát. Ezen rendszerek közös korlátja a manuálisan hangolt tudás szükségessége és a domén-specifikusság.

\section{Monte Carlo Tree Search és AlphaZero}

\subsection{MCTS alapok és fejlesztések}

A Monte Carlo fakeresés módszereit Browne és mtsai~\cite{browne2012} részletesen áttekintik; az alábbiakban a dolgozat szempontjából releváns elemek kerülnek röviden összefoglalásra. A Monte Carlo fakeresés (\textit{Monte Carlo Tree Search}, MCTS) \cite{coulom2006cg,kocsis2006uct} egy aszimmetrikus, legjobb-első (\textit{best-first}) jellegű keresőalgoritmus véletlen szimulációkkal, amely a Go területén ért el először jelentős eredményt \cite{gelly2006mogo}. Az algoritmus elvileg nem igényel doménspecifikus heurisztikát (elegendőek a játékszabályok és a végállapotok kiértékelése), a gyakorlatban azonban a sikeres rendszerek a hatékonyság növelése érdekében számos heurisztikával egészítik ki az alapalgoritmust.

Az MCTS működése négy fázis iteratív ismétlésén alapul:
\begin{enumerate}
    \item \textbf{Szelekció:} A gyökértől indulva a fa mélye felé haladunk egy kiválasztási stratégia (pl.~UCT) alapján. A folyamat addig tart, amíg egy olyan csomóponthoz nem érünk, amelynek vannak még felderítetlen gyermekei. A cél az exploráció (új utak felfedezése) és az exploitáció (ismert utak mélyítése) közötti egyensúly fenntartása.
    \item \textbf{Expanzió:} A kiválasztott levélcsomóponthoz hozzáadunk egy vagy több új gyermekcsomópontot, amelyek a lehetséges következő lépéseket reprezentálják. Ez a lépés növeli a keresési fa méretét.
    \item \textbf{Szimuláció:} A klasszikus MCTS véletlenszerű (vagy könnyű heurisztikával vezérelt) lépésekkel játszik végig egy játszmát a végállapotig (rollout). A végkimenetel (győzelem, vereség, döntetlen) szolgál a pozíció értékbecsléseként.
    \item \textbf{Visszaterjesztés:} A szimuláció eredményét visszavezetjük a fán felfelé a gyökérig, frissítve minden érintett csomópont látogatási számát ($N$) és átlagos értékét ($Q$). Ez biztosítja, hogy a következő iterációkban pontosabb statisztikák alapján dönthessünk.
\end{enumerate}

Az UCT algoritmus \cite{kocsis2006uct} a többkaros bandit problémákra kidolgozott UCB1 stratégiát \cite{auer2002ucb} alkalmazza a fában. Ennek képlete:
\[ UCT = \frac{w_i}{n_i} + C \sqrt{\frac{\ln N_i}{n_i}} \]
ahol $w_i$ a nyerések száma, $n_i$ a csomópont látogatottsága, $N_i$ a szülő látogatottsága, $C$ pedig az explorációs konstans. A RAVE-heurisztika~\cite{gelly2007rave} további gyors konvergenciát tesz lehetővé a szimulációk eredményének megosztásával a testvércsomópontok között.

Az AlphaGo \cite{silver2016alphago} 2016-ban új alapokra helyezte ezt a megközelítést azzal, hogy a költséges és nagy varianciájú véletlen játszmabefejezéseket (rolloutokat) neurális hálózatokkal egészítette ki, és nagyrészt kiváltotta. Az AlphaGo Zero \cite{silver2017go} tovább egyszerűsítette a rendszert: a korábbi különálló döntéshozó (policy) és értékbecslő (value) hálózatokat egyetlen, kétfejű reziduális hálózatba integrálta. Továbbá bevezette a PUCT kiválasztási formulát, amely a hálózat által jósolt a priori valószínűségeket ($P(s,a)$) használja az exploráció irányítására.

A PUCT (UCT variáns döntési prior (\textit{policy prior}) súlyozással) alapformulája \cite{silver2017go,silver2017chess} konstans kiegyensúlyozási paramétert használ az exploráció szabályozására. A közösségi fejlesztésű, AlphaZero-típusú implementációkban (pl.~KataGo~\cite{wu2019katago}, LC0~\cite{lczero}, CrazyAra~\cite{crazyara}) a $c_{\text{puct}}$ együttható gyakran a \eqref{eq:cpuct} szerinti dinamikus formulát követi:
\begin{equation}
c_{\text{puct}} = \log\left(\frac{N(s) + c_{\text{base}} + 1}{c_{\text{base}}}\right) + c_{\text{init}}
\label{eq:cpuct}
\end{equation}
ahol tipikusan $c_{\text{base}} = \num{19652}$, és $c_{\text{init}}$ tipikusan 1 és 3 közötti érték (míg jelen rendszerben konkrétan $c_{\text{init}}=\num{1.55}$). Ez biztosítja, hogy ritkábban látogatott állapotokban nagyobb exploráció történjen, tipikusan $c_{\text{puct}} \in [1, 5]$ tartományban (lásd \eqref{eq:cpuct}).

A PUCT akció-kiválasztási formula:
\[
U(s,a) = Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)}
\]
ahol $N(s)$ a csomópont (állapot) látogatási száma, $N(s,a)$ az adott él látogatottsága, $Q(s,a)$ az átlagos érték, $P(s,a)$ pedig a hálózat által adott prior valószínűség. A rendszerben a konfigurációs $c_{\text{puct,scale}}$ érték (pl.~\num{1.35} vagy \num{2.5}) a \eqref{eq:cpuct} szerinti dinamikus faktort skálázza, ahogyan azt az \Tab{tab:scenarios} mutatja.
Dirichlet zaj biztosítja az explorációt a gyökérben. Az AlphaZero \cite{silver2017chess} ezt általánosította sakkra és shogira, emberfeletti szintet érve el néhány óra tanítással (sakkra ~9~óra).

\subsection{AlphaZero tanítási eljárás részletei}

Az AlphaZero tanítási folyamata négy iterálódó fázisból áll~\cite{silver2017chess}. Az \textbf{önjátszás} (self-play) fázisban a jelenlegi modell játszik önmaga ellen változatos kezdőpozíciókból. Minden lépésnél az MCTS 800 szimulációt végez, amely a neurális háló értékeléseit használja levelekben~\cite{silver2017chess}. A lépésválasztás hőmérséklettel módosított látogatási eloszlásból történik mintavételezéssel ($\tau > 1$ korai játékban, $\tau \to 0$ késői játékban), biztosítva a diverzitást. A játék végén a kimenetel ($z \in \{-1, 0, +1\}$) minden pozíció értékcéljává válik.

Az \textbf{adattárolás} fázisban a pozíció-lépésirány-kimenetel hármasokat visszajátszási pufferben gyűjtik. Az AlphaGo Zero~\cite{silver2017go} kiegészítő anyaga explicit módon a legutóbbi $\sim$\num{500000} játékot tartalmazó visszajátszási puffert ír elő. Az AlphaZero-tanulmányok~\cite{silver2017chess,silver2018science} ugyanennek a tanítási protokollnak az adaptációi, de a konkrét pufferméretet nem minden esetben részletezik. A közösségi reimplementációk ezért jellemzően 500k és 1M közötti puffert alkalmaznak. A minták $(\boldsymbol{s}_t, \boldsymbol{\pi}_t, z_t)$ formában tárolódnak, ahol $\boldsymbol{s}_t$ a bemeneti reprezentáció (8 történeti lépés), $\boldsymbol{\pi}_t$ az MCTS látogatási eloszlás (policy target), és $z_t$ a végső kimenetel a $t$-edik pozíció nézőpontjából.

A \textbf{tanítási fázis} mini-kötegeken optimalizálja a hálót. A veszteségfüggvény \cite{silver2017chess} három tagból áll:
\begin{equation*}
\mathcal{L} = \underbrace{-\boldsymbol{\pi}^T \log \boldsymbol{p}}_{\text{policy-veszteség (döntési veszteség)}} + \underbrace{\lambda_v (z - v)^2}_{\text{értékveszteség (értékbecslési veszteség)}} + \underbrace{\lambda_c \lVert\boldsymbol{\theta}\rVert^2}_{\text{regularizációs tag (\textit{regularization})}}
\end{equation*}
ahol $\boldsymbol{p}$ és $v$ a háló kimenetei, $\lambda_v$ tipikusan 1, $\lambda_c$ a súlycsökkenés koefficiense ($10^{-4}$). A gyakorlatban $\lambda_v = 1$, ezért a dolgozat a $\mathcal{L}_{\text{val}}$ definíciójában elhagyja. Az AlphaZero az eredeti cikkben \cite{silver2017chess} momentumos SGD-t és lépcsőzetes (\textit{stepwise}) ütemezést alkalmaz a tanulási rátára.

A modern AlphaZero-típusú reimplementációk gyakran bemelegítési szakasszal (\textit{warmup}) és koszinuszos ütemezésű csökkenéssel kombinált stratégiát (SGDR-szerű \cite{loshchilov2017sgdr} ütemezés újraindítás nélkül) használnak. Jelen dolgozat is ezt a megközelítést követi.

Az \textbf{aréna-értékelés} (\textit{arena evaluation}) fogalma a szakirodalomban két, egymástól elkülönülő szerepben jelenik meg. Az AlphaGo Zero~\cite{silver2017go} tanítási protokolljában az úgynevezett aréna-kapuzás (\textit{arena gating}) döntötte el, hogy egy jelölt háló átveheti-e az önjátszásban használt „best player” szerepét: minden új háló 400 játszmát játszott az aktuális legjobb ellen, és csak akkor lépett elő, ha a győzelmi aránya meghaladta az 55\%-ot. Ez a konzervatív küszöb csökkentette a zajból eredő fluktuációt, ugyanakkor a tanulást nem tette monoton növekvővé: sok iteráció is eltelhetett új „best player” nélkül. Az AlphaZero-tanulmányok~\cite{silver2017chess,silver2018science} ezzel szemben kifejezetten elhagyják ezt a kapuzási lépést: az önjátszásos adatok generálására mindig az éppen legutóbb tanított hálót használják, és az aréna jellegű tornákat csak utólagos erősségmérésre (pl.\ Elo-becslés Stockfish vagy AlphaGo-változatok ellen) alkalmazzák, nem pedig a további önjátszásban használt modell kiválasztására.

\subsection{Ismert tipikus hibamódok és kihívások}

Az AlphaZero-típusú rendszerek számos tipikus hibamóddal rendelkeznek, amelyeket a szakirodalom~\cite{silver2017chess,lczero,wu2019katago} és a közösségi reimplementációk tapasztalatai részletesen dokumentálnak.

\textbf{Értékbecslő ág kollapszusa}: A neurális háló értékbecslő ága konstans értéket tanulhat meg (pl.~minden pozícióra $v \approx 0$), különösen túl kis adatmennyiség vagy túl nagy hálókapacitás esetén. Ez azt eredményezi, hogy az MCTS nem kap hasznos értékbecsléseket, és véletlenszerű lépéseket választ. A lehetséges megoldások: kisebb háló, több adat, vagy az értékbecslő ág tanulási rátájának csökkentése.

\textbf{Lépésirány-túlillesztés}: A policy ág memorizálhatja a tanítóadatok specifikus pozícióit anélkül, hogy általános sakkstratégiát tanulna. Ez magas tanítási pontosságot és alacsony validációs teljesítményt eredményez. Gyakori kis nyitókönyv esetén, amikor a modell ugyanazokat a pozíciókat látja újra és újra. A megoldás: nagyobb nyitókönyv, adataugmentáció (tükrözés, forgatás), dropout.

\textbf{Exploráció-kizsákmányolás egyensúly}: Túl konzervatív exploráció (alacsony Dirichlet zaj, magas $c_{\text{puct}}$) korai konvergenciát okoz rossz lokális optimumba. Túl agresszív exploráció véletlenszerű játékot eredményez, lassítva a tanulást. Az optimális paraméterek játékonként változnak: sakkban $\alpha = \num{0.3}$, shogiban $\alpha = \num{0.15}$, Go-ban $\alpha = \num{0.03}$, amelyeket az AlphaZero cikk \cite{silver2017chess} és az AlphaGo Zero cikk \cite{silver2017go} részletesen dokumentál (a kiegészítő anyagokban), és amelyeket a közösségi AlphaZero-típusú implementációk gyakorlatban is megerősítettek.

\textbf{Aréna-értékelés zajérzékenysége}: Kis meccsszám ($< 100$ játék) nagy varianciát eredményez a becslésben. Két azonos erejű modell mérkőzése esetén a kimenetel közel azonos, nagyjából 50\%-os arány mindkét fél számára, így 100 játék esetén a 95\%-os konfidenciaintervallum $\pm 10\%$. Az AlphaGo Zero által alkalmazott konzervatív küszöb (55\%+ győzelmi arány) és nagy meccsszám (400+ arénaparti) csökkenti a hamis pozitív előléptetéseket; az AlphaZero esetében ehelyett folyamatos, kapuzás nélküli önjátszás zajlik, és az arénajellegű tornák csak külső erősségmérésre szolgálnak.

\textbf{Számítási költség skálázódása}: Az AlphaZero eredeti sakkfuttatása mintegy 9~órás volt, 5000 első generációs TPU-t használva az önjátszásos adatok generálására és 16 második generációs TPU-t a háló tanítására~\cite{silver2018science}. Közösségi becslések szerint ez több tízmilliónyi önjátszásos játszmát eredményezhetett, ami egyetlen fogyasztói GPU-n nagyságrendileg éveket venne igénybe. Kompromisszumok szükségesek: kevesebb iteráció, kisebb háló, kevesebb MCTS szimuláció, kisebb puffer.

\subsection{Leela Chess Zero tanulságok}

A Leela Chess Zero \cite{lczero} nyílt forráskódú közösségi projekt, amely az AlphaZero-módszertant valósítja meg elosztott önkéntes GPU-kon. Több ezer önkéntes GPU-idejét aggregálva a projekt 2018 óta több milliárd önjátszásos játszmát generált.

\textbf{Hálóarchitektúra evolúció}: Az LC0 kezdetben kisebb reziduális hálókat használt (pl.~6 reziduális blokk 64 és 128 csatornával), majd fokozatosan 15, 20 és később $\sim$40 blokkos hálókra nőtt több tízmillió paraméterrel. A 2020-as évek közepére a legmodernebb LC0/LZ hálók $\sim$40 vagy több blokkot és nagyobb csatornaszámokat használnak, gyakran tízmilliós paraméterszámmal, Squeeze-and-Excitation (SE, csatornaválasztó modul)~\cite{hu2018senet} rétegekkel kiegészítve. Ez arra utal, hogy a sakkhoz szükséges reprezentációs kapacitás jelentős, és a kisebb hálók (5 és 10 blokk között) csak korlátozott játékerőt érnek el.

\textbf{Tanítási trükkök}: A KataGo \cite{wu2019katago} és az LC0 projekt számos fejlesztést vezetett be az AlphaZero eredeti receptjéhez képest. A szakirodalomban \emph{Q-ratio} néven emlegetett, a KataGo-ban \cite{wu2019katago} kifejlesztett módszer súlyozott átlagot képez a háló értékbecslése ($v$) és az MCTS Q-értéke között, csökkentve az értékfej kollapszust. Az LC0 projekt emellett \emph{FPU reductiont} (First-Play Urgency csökkentés) és \emph{smart pruningot} (intelligens levágás) vezetett be a gyakorlatban: az FPU reduction agresszívebb explorációt biztosít nem látogatott csomópontokhoz, a smart pruning korai levágást alkalmaz alacsony prior-ú lépésekre, gyorsítva a keresést minőségvesztés nélkül.

\textbf{Adatmennyiség vs. hálókapacitás}: Az LC0 tapasztalata, hogy a hálóméret növelése csak akkor hasznos, ha az adatmennyiség arányosan nő. Korai fázisban (< 10M játék) a kis hálók (6 és 10 blokk között) gyorsabban tanulnak. Nagyobb adatbázissal (100M+ játék) a nagy hálók (20 és 40 blokk között) előnybe kerülnek, de konvergenciájuk lassabb. Ez alátámasztja a tanterv alapú tanulás (\textit{curriculum learning})~\cite{bengio2009curriculum} megközelítést: kezdjük kis hálóval, majd növeljük a kapacitást konvergencia után.

\textbf{Elosztott infrastruktúra}: Az LC0 szerver-kliens architektúrát használ: központi szerver osztja ki a tanítási súlyokat és gyűjti az önjátszás adatokat, kliensek (önkéntes GPU-k) generálják a játékokat és küldik vissza. Ez lehetővé teszi a skálázódást, de késleltetést és koordinációs költséget vezet be. Egyetlen GPU-s megközelítés (jelen dolgozat) elkerüli ezt a bonyolultságot, de korlátozott adatgenerálási sebességgel rendelkezik.

\section{Közösségi implementációk és hardveroptimalizáció}

A Leela Chess Zero (LC0, \cite{lczero}) kezdetben az AlphaZero-nál kompaktabb, $\sim$\num{1858} kimenetes lépéskódolást használt, szemben az AlphaZero \num{4672}-es ($73\times64$) kódolásával. Az újabb LC0 hálók azonban már jellemzően AlphaZero-stílusú, $73\times8\times8$ policy fejet alkalmaznak. A KataGo \cite{wu2019katago} és MuZero \cite{schrittwieser2020muzero} kifinomult tanítási technikákat vezetett be. Kulcsfontosságú mérnöki technikák: kötegelt feldolgozás, \texttt{channels\_last} (csatorna-utolsó) memória, vegyes precizió \cite{micikevicius2018amp}, aszinkron feldolgozási láncok, gyorsítótár. Hibrid C++/Python architektúrák \cite{pybind11} egyensúlyban tartják a sebességet és fejlesztési hatékonyságot.

\section{Kutatási rés}

Az AlphaZero-irányzat ipari léptékű hardverigénye \cite{silver2018science} jelentős korlátot jelent a módszertan széles körű elterjedése szempontjából. A szerző tudomása szerint a szakirodalomban jelenleg nem található olyan, széles körben hivatkozott, egyetlen fogyasztói GPU-ra optimalizált referenciaimplementáció, amely rövid futási idő mellett demonstrálná a megközelítés működőképességét. Ez nehezíti az AlphaZero-módszertan oktatási alkalmazását és a reprodukálható kutatást: korlátozott erőforrással rendelkező egyetemi kutatócsoportok és egyéni fejlesztők gyakorlatilag nem tudják megismételni vagy továbbfejleszteni az eredeti rendszert.

A dolgozat ezért elsődlegesen a módszertan megvalósíthatóságára és mérhető, korlátos erőforrások mellett elérhető teljesítményére fókuszál; a pontos célkitűzéseket a következő fejezet részletezi.

\chapter{Célkitűzések és módszertan}

A dolgozat központi kérdése az, hogy az AlphaZero-típusú sakkprogram megvalósítható-e fogyasztói hardveren, korlátozott időkeret mellett, és ehhez milyen mérnöki döntések szükségesek. Emellett azt is vizsgálom, hogy ezekkel a korlátokkal milyen teljesítmény érhető el.

Ebben a fejezetben foglalom össze a kutatási célkitűzéseket, a hozzájuk tartozó kutatási kérdéseket és a kísérleti elrendezés főbb elemeit; a részletes rendszerleírást a következő fejezet tartalmazza.

A munka öt fő célkitűzése:
\begin{enumerate}
  \item \textbf{Hibrid C++/Python architektúra megvalósítása} tiszta, jól dokumentált API-val.
  \item \textbf{Komponens-szintű gyorsulások mérése} (sakkmag, MCTS, GPU-következtetés).
  \item \textbf{Teljes tanítási ciklus demonstrálása} egynapos költségvetéssel.
  \item \textbf{Arénamérkőzések metrikáinak rögzítése} a tanulási stabilitás követéséhez.
  \item \textbf{A kísérletek reprodukálhatóságának biztosítása} benchmarkokkal és konfigurációkkal.
\end{enumerate}

\section{Kutatási kérdések (RQ1-től RQ5-ig)}
\label{sec:kutatasi-kerdesek}

A célkitűzésekből öt kutatási kérdés (RQ1-től RQ5-ig) adódik:
\begin{description}
  \item[RQ1, Teljesítmény:] Milyen komponens-szintű gyorsulások érhetők el a hibrid architektúrával?
  \item[RQ2, Skálázhatóság:] Hogyan skálázódik a rendszer a párhuzamosítással, és melyek a szűk keresztmetszetek?
  \item[RQ3, Tanulási dinamika:] Mutat-e a rendszer belső tanulási jeleket (a policy- és value-veszteség tartós csökkenése, valamint az arénamérkőzések győzelmi arányainak javulása) rövid (egynapos) tanítási horizonton?
  \item[RQ4, Adatdiverzitás:] Milyen mértékben biztosítja a nyitókönyvvel~\cite{lichess} indított önjátszás és az explorációs beállítások együttese a pozíciók kellő sokszínűségét?
  \item[RQ5, Reprodukálhatóság:] Mennyire támogatja a rendszer felépítése és az automatizált eszközlánc a futások reprodukálhatóságát (azonos beállítások mellett)?
\end{description}

A dolgozat célja \textbf{nem} egy versenyképes sakkprogram létrehozása, hanem az AlphaZero-módszertan \textbf{megvalósíthatóságának} demonstrálása korlátozott erőforrásokkal, valamint a rendszer \textbf{mérnöki teljesítményének} jellemzése.

A megvalósítás során a kritikus útvonalhoz tartozó komponenseket (bitboard-alapú sakkmag, PUCT-MCTS) C++23-ban, a tanulási folyamatokat pedig Pythonban, PyTorch~\cite{pytorch}-ra épülő gépi tanulási feldolgozási lánccal valósítottam meg. A tervezés köteg-orientált, kötegelt GPU-feldolgozással. A rendszer többszintű LRU-gyorsítótárat (érték-, kimenet- és kódolási gyorsítótár) és ritka lépésirány-tárolást alkalmaz (körülbelül \num{75}-szörös memóriamegtakarítással).

A sikerkritériumok három kategóriába sorolhatók:
\begin{itemize}
  \item \textbf{Teljesítmény:} a pozíció/mp-ben és áteresztőképességben mért mutatók javítása a Python-alapú kiinduló megoldáshoz képest
      (python-chess sakkmag, tisztán Python MCTS, nem optimalizált neurális következtetés), a komponensekre kitűzött
      $\geq\num{5}$-szörös (sakkmag), $\geq\num{3}$-szoros (MCTS) és $\geq\num{10}$-szeres (GPU-következtetés)
      gyorsulási célértékek teljesítésével.
  \item \textbf{Tanulás:} belső metrikák csökkenése (a policy-veszteség legalább 40\%-os, az érték-veszteség legalább 50\%-os relatív javulása).
  \item \textbf{Mérnöki:} automatikus metrika-naplózás (CSV/JSONL), YAML-konfigurációk és unit tesztek megléte.
\end{itemize}

\textbf{Numerikus küszöbértékek indoklása:} Ezeket a célértékeket ($\geq$\num{5}-szörös, $\geq$\num{3}-szoros, $\geq$\num{10}-szeres) az RTX 3070 Laptop GPU hardveres korlátai alapján, előzetes mérésekre alapozva határoztam meg. Az ötszörös sakkmag-gyorsulás tükrözi a C++-natív kód és a Python-implementációk közötti tipikus különbséget bitboard-műveletekre. A háromszoros MCTS-gyorsulási cél azt jelzi, hogy a fa-bejárási optimalizációk (gyorsítótár, látogatásszám-korlátozás) jelentős hozamot adnak. A tízszeres GPU-következtetési célérték nagy kötegméreteknél (a mérésekben $b=\num{512}$ esetén) elérhető, kihasználva a GPU párhuzamosságát. A veszteségcsökkenési célok a policy-fej esetén >40\%-os, az értékfej esetén >50\%-os relatív csökkenést céloznak, ami empirikusan elegendőnek bizonyult a tanulási folyamat kimutatásához.

A fenti célkitűzéseket a későbbi fejezetekben bemutatott kísérleti eredmények alapján értékelem, különösen az RQ1 (teljesítmény) és RQ3 (tanulási dinamika) kapcsán. A játékerő hosszú távú növekedésének és Elo-alapú külső validációnak a vizsgálata a dolgozat hatókörén kívül esik; a fókusz a technikai megvalósíthatóságon és a komponens-teljesítményen van.

A kísérleti elrendezés három pillérre épül:
\begin{enumerate}
    \item Számos izolált teljesítménymérés készült, amelyek lefedték a sakkmag, az MCTS-keresés, a neurális következtetés, az augmentáció és a visszajátszási puffer sebességét.
    \item Ezt követően teljes rendszerfuttatások történtek egységes, 6 blokkos, 96 csatornás hálóval, eltérő szimulációs mélység és kötegméret beállításokkal.
    \item Végül ablációs vizsgálatok készültek nyitókönyvvel és anélkül, erősen lecsökkentett gyorsítótár-kapacitással (gyakorlatilag gyorsítótár nélküli konfigurációkban), exponenciális mozgóátlag (\textit{Exponential Moving Average}, EMA) és közvetlen súlyok összehasonlításával, valamint a játszma-kiértékelés progresszív és fix változataival.
\end{enumerate}

A következő fejezet a fenti célok és módszertan megvalósítását mutatja be: a hibrid architektúra részleteit, a teljesítménykritikus komponensek tervezési döntéseit, valamint a széles körben alkalmazott optimalizációs technikákat. A \ref{sec:kutatasi-kerdesek}. szakaszban megfogalmazott, RQ1-től RQ5-ig terjedő kérdéseket az \ref{ch:experiments}. fejezet értékeli ki.

\chapter{Rendszer tervezése és implementáció}

\section{Architektúra}

A rendszer hibrid felépítésű: a számításigényes komponensek (bitboard sakkmag, virtuális veszteségre (\textit{virtual loss}) épülő konkurencia-kezelésű MCTS-motor) C++23-ban íródtak, míg a neurális háló tanítása és következtetése Pythonra épülő PyTorch környezetben történik. A C++ és a Python közötti interfészt a \texttt{pybind11} könyvtár biztosítja, amely lapos tömbök használatával gyakorlatilag másolásmentes adatcserét tesz lehetővé. A numerikus adatok hatékony kezelését a \texttt{NumPy}~\cite{harris2020numpy} csomag biztosítja, míg a kísérleti eredmények vizualizációját a \texttt{matplotlib}~\cite{hunter2007matplotlib} és \texttt{seaborn}~\cite{waskom2021seaborn} könyvtárak támogatják.

A rendszer alapvető tervezési elve a \textbf{kanonikus reprezentáció}, azaz a „Mindig világos” nézet: a hálózat kizárólag a világos szemszögéből látja a táblát. Sötét lépése esetén a táblát és a lépéseket a bemeneti kódolás során a rendszer tükrözi (vertikális tükrözés + színcsere), majd a hálózat kimenetét (policy) visszatükrözi. Ez a megoldás felére csökkenti az állapottér komplexitását és feleslegessé teszi a heurisztikus adat-augmentációt.

A tanulási ciklus folyamatos, AlphaZero-stílusú tanulást valósít meg: a hálózat a futás teljes időtartama alatt frissül, és az önjátszás mindig az éppen legfrissebb (EMA-val súlyozott) modellt használja. A klasszikus AlphaGo Zero-féle aréna-kapuzással szemben itt nincs olyan küszöbérték, amely az arénamérkőzések eredménye alapján engedélyezné vagy elutasítaná a jelölt modellt. Az arénamérkőzések a gyakorlatban kizárólag értékelő metrikaként szolgálnak: rendszeres időközönként az aktuális modell egy korábbi pillanatfelvétel ellen játszik, az eredményekből pedig győzelmi arányt, döntetlenarányt és futásidőt számolok:
\begin{enumerate}
  \item \textbf{Önjátszás:} Aszinkron munkaszálak generálják a játszmákat a legfrissebb hálózattal.
  \item \textbf{Adatgyűjtés:} A generált játszmák egy körkörös visszajátszási pufferbe kerülnek (\num{50000}-es kapacitás, nagy áteresztőképességű konfiguráció esetén \num{80000}).
  \item \textbf{Tanítás:} A GPU folyamatosan mintavételez a pufferből és frissíti a hálózat súlyait SGD optimalizálóval.
  \item \textbf{Szinkronizáció:} Az önjátszó szálak rendszeres időközönként átveszik az új súlyokat.
\end{enumerate}

\section{Módszerek}

\subsection{Sakkmag és bitboard reprezentáció}
A rendszer alapját egy nagy teljesítményű C++23 sakkmag képezi. A sakktábla reprezentációjának egyik legelterjedtebb és hatékony módja a \textbf{bitboard} technika. Egy 64 bites egész szám (pl.~\texttt{uint64\_t} C++-ban) pontosan megfeleltethető a sakktábla 64 mezejének. Minden báb típushoz (gyalog, huszár, futó, bástya, vezér, király) és színhez (világos, sötét) külön bittáblát tart a rendszer nyilván, ahol az 1-es bit jelzi a báb jelenlétét az adott mezőn.

A bitboardok előnye, hogy a lépésgenerálás és a táblaműveletek (pl.~támadott mezők számítása) bitműveletekkel (AND, OR, XOR, bitshift) párhuzamosan végezhetők el az egész táblán. Például egy huszár összes lehetséges lépése egy adott mezőről előre kiszámítható és egy keresőtáblában (\textit{lookup table}) tárolható. A lépésgenerálás során a
\[ \texttt{targets} = \texttt{knight\_attacks}[\texttt{sq}] \ \& \ \sim\texttt{own\_pieces} \]
művelet néhány CPU ciklus alatt megadja az összes legális célmezőt, automatikusan kizárva a saját bábukat tartalmazó mezőket.

A csúszó bábuk (bástya, futó, vezér) esetében a rendszer a \textbf{PEXT-alapú bitboard-technikát} (Parallel Bit Extract) alkalmazza BMI2 utasításkészlettel (régebbi CPU-kon ennek szoftveres PEXT/bit-kompressziós megvalósításával). Ez egy hardveresen gyorsított, hash-alapú módszerrel kezeli a blokkoló bábukat, és gyakorlatilag konstans idejű ($O(1)$) lekérdezést biztosít a sugárirányú támadásokhoz.

A sakkmag felelős továbbá a szabályosság ellenőrzéséért (pl.~király nem léphet sakkba), a háromszori lépésismétlés és az 50 lépéses szabály detektálásáért, valamint a Zobrist-hash \cite{zobrist1970} számításáért, amelyet a neurális kiértékelések gyorsítótárazásához (transzpozíciós gyorsítótár) használ a rendszer.

\subsection{MCTS motor (C++)}
A keresőmotor a Monte Carlo Tree Search (MCTS) algoritmust valósítja meg PUCT kiválasztási stratégiával. A C++ implementáció kritikus optimalizációkat tartalmaz a Python-alapú megoldásokhoz képest:

\begin{itemize}
    \item \textbf{Konkurencia-kezelés (virtuális veszteség):} A \texttt{VIRTUAL\_LOSS = 1.0} beállítás lehetővé teszi a függőben lévő kiértékelések hatékony kezelését, míg a rendszerszintű párhuzamosítást aszinkron munkaszálak biztosítják.
    \item \textbf{Kötegelt levélkiértékelés:} A levélcsomópontok kötegelve kerülnek kiértékelésre; a C++ oldal int32 tömbökbe gyűjti a pozíciókat, majd egyetlen tenzorként adja át a Python rétegnek.
    \item \textbf{Robusztus memóriakezelés:} A NodePool indexalapú, vektoralapú allokációja biztosítja, hogy az átméretezés a pointerek érvényességének megtartása mellett is skálázható legyen.
    \item \textbf{Nullösszegű visszaterjesztés:} A Backpropagate rutin minden szinten előjelet vált ($\text{value} = -\text{value}$), biztosítva az ellenfél szemszögének helyes kezelését.
\end{itemize}

\subsection{Bemeneti reprezentáció és hálózat}
A sakkállásokat az AlphaZero-ban is alkalmazott, síkokra bontott tenzorral kódolja a rendszer. Az aktuális és az azt megelőző 7 lépésállás mindegyikét 14 síkon kódolja a rendszer (összesen $8 \cdot 14 = 112$ sík), valamint további 7 meta-síkot ad hozzá, így a teljes bemenet $119 \times 8 \times 8$ méretű:
\begin{itemize}
    \item \textbf{Előzmény (History):} Az aktuális és az azt megelőző 7 lépésállás (History=8).
    \item \textbf{Pozíció kódolás (14 sík):}
    \begin{itemize}
        \item 12 sík: Saját/Ellenfél gyalog, huszár, futó, bástya, vezér, király (P, N, B, R, Q, K).
        \item 1 sík: En Passant célmező (one-hot jelöléssel).
        \item 1 sík: Ismétlődési számláló $\ge 2$ (Döntetlen veszély jelzése).
    \end{itemize}
    \item \textbf{Meta-síkok (7 db):} szín, lépésszám, sáncjogok és fél-lépés számláló, ezek több bináris síkon kódolva (pl. külön sík a világos/sötét jogoknak).
\end{itemize}

A neurális hálózat egy \textbf{ResNet v1 architektúrát} követi~\cite{he2016resnet}, amely a mély hálózatok tanítását teszi lehetővé az eltűnő gradiens probléma (\textit{vanishing gradient}) enyhítésével. A hálózat alapépítőköve a reziduális blokk (Residual Block).

Egy reziduális blokk a következő rétegekből áll:
\begin{itemize}
    \item Konvolúciós réteg ($3 \times 3$ kernel, padding=1)
    \item Batch Normalization
    \item ReLU aktiváció (Rectified Linear Unit)
    \item Konvolúciós réteg ($3 \times 3$ kernel, padding=1)
    \item Batch Normalization
    \item \textbf{Áthidaló kapcsolat (Skip Connection):} A blokk bemenetét hozzáadjuk a második konvolúció kimenetéhez.
    \item ReLU aktiváció
\end{itemize}
Az áthidaló kapcsolat (\textit{skip connection}, \textit{identity shortcut}) lehetővé teszi, hogy a gradiens akadálytalanul áramoljon vissza a hálózat elejére a backpropagation során, így sokkal mélyebb hálózatok is hatékonyan taníthatók.

A jelenlegi hálóarchitektúra főbb jellemzői a következők:
\begin{itemize}
    \item \textbf{Törzs:} 6 reziduális blokk, 96 csatorna, $3 \times 3$ konvolúciók. Ez egyensúlyt teremt a kiértékelési sebesség és a játékerő között a korlátozott hardver környezetben.
    \item \textbf{Policy ág:} $1 \times 1$ konvolúció (2 csatorna) $\to$ Batch Norm $\to$ ReLU $\to$ Flatten $\to$ Fully Connected $\to$ 73×64 kimenet (4672 logit). A policy ág 73×64 logitet ad, amelyet a tanítási/inferencia réteg Softmax-szal valószínűségi eloszlássá alakít.
    \item \textbf{Értékbecslő ág (\textit{Value Head}):} $1 \times 1$ konvolúció (12 csatorna) $\to$ Batch Norm $\to$ ReLU $\to$ Flatten $\to$ 256 rejtett neuron (ReLU) $\to$ teljesen összekötött réteg (\textit{fully connected}) $\to$ 1 kimenet $\to$ Tanh. A kimenet $[-1, 1]$ intervallumban becsli a pozíció értékét (1: győzelem, 0: döntetlen, -1: vereség).
    \item \textbf{Inicializálás:} Kaiming Normal (\textit{He}) inicializáció kerül alkalmazásra, amely a ReLU aktivációkhoz optimalizált szórású véletlen súlyokkal indítja a hálózatot.
\end{itemize}

\subsection{Tanítási folyamat}
A rendszer a „folyamatos tanulás” paradigmát követi. A self-play és a tanítás egy időben zajlik, a hálózat súlyai minden iterációban frissülnek, és az önjátszást mindig a legutóbbi modell végzi.

A veszteségfüggvény:
\begin{equation*}
\mathcal{L} = \mathcal{L}_{\text{pol}} + \mathcal{L}_{\text{val}} - \lambda_{\text{entropy}} \cdot \mathcal{H}(p) + \lambda_c \cdot \lVert\theta\rVert^2
\end{equation*}
Ahol $\mathcal{L}_{\text{pol}}$ a keresztentrópia, $\mathcal{L}_{\text{val}}$ a négyzetes hiba (MSE), az entrópia-regularizációs tag súlya $\lambda_{\text{entropy}} \approx 2\cdot 10^{-4}$
pedig a negatív előjel miatt a hálózat kimeneti politikájának entrópiájának növelésére ösztönöz, gátolva a túl korai konvergenciát. A veszteségfüggvény az AlphaZero-ban használt formulát követi, kiegészítve ezzel az entrópia-taggal. A korábbi AlphaZero-jellegű rendszerekben használt táblaszimmetriákra épülő tükrözéses adataugmentáció helyett a kanonikus nézet biztosítja a szimmetria-invarianciát.

A következő fejezet a fenti architektúra alapján lefuttatott kísérleteket és az azokból származó eredményeket mutatja be.

\chapter{Kísérletek és eredmények}
\label{ch:experiments}

A kiértékelés során öt kísérleti forgatókönyv került vizsgálatra, hogy a különböző hiperparaméter-beállítások hatása elkülöníthető legyen a tanulási teljesítményre. Minden mérés azonos hardverkonfiguráción (RTX 3070 Laptop GPU, Ryzen 5800H), kontrollált környezetben futott; a teljes tanítási idők 9 és 18{,}4~óra között változtak.

Öt célzott konfigurációt definiáltam a \texttt{configs/} könyvtárban található beállítások alapján.

\begin{table}[htbp]
\centering
\caption{Az 5 kísérleti forgatókönyv paraméterei.}
\label{tab:scenarios}
\small
\begin{tabular}{l|l|ccc}
\toprule
\textbf{Név / Cél} & \textbf{Szimulációk száma} & \textbf{Köteg} & \textbf{PUCT} & \textbf{Zaj ($\alpha$)} \\
\midrule
\textbf{Referencia} (kontroll) & 128 & 256 & 1.35 & 0.3 \\
\textbf{Mély Keresés} (pontosság) & \textbf{256} & 256 & 1.35 & 0.3 \\
\textbf{Nagy Áteresztőképesség} (sekély keresés) & \textbf{64} & 256 & 1.35 & 0.3 \\
\textbf{Nagy Entrópia} (exploráció) & 128 & 256 & \textbf{2.5} & \textbf{0.5} \\
\textbf{Hatékonyság} (gyakori frissítés) & 128 & \textbf{128} & 1.35 & 0.3 \\
\bottomrule
\end{tabular}
\end{table}

\section{Eredmények}

\subsection{RQ1: Rendszer teljesítmény}

A C++23-ra történő áttérés és a \texttt{virtual loss}-alapú konkurenciakezelés bevezetése számottevő sebességnövekedést eredményezett (lásd \Tab{tab:chess_cpp_bench}).

\begin{table}[htbp]
\centering
\caption{Sakkmag teljesítményének összehasonlítása.}
\label{tab:chess_cpp_bench}
\begin{tabular}{lcc}
\toprule
\textbf{Implementáció} & \textbf{Pozíció/mp} & \textbf{Relatív gyorsulás} \\
\midrule
Python (python-chess)~\cite{pythonchess} & \num{10699} & 1.0$\times$ \\
C++ (Hybrid Core) & \textbf{\num{56887}} & \textbf{\num{5.3}-szoros} \\
\bottomrule
\end{tabular}
\end{table}

A neurális következtetés áteresztőképességét az \Tab{tab:inference_bench} szemlélteti.

\begin{table}[htbp]
\centering
\caption{Neurális következtetés teljesítménye (RTX 3070 Laptop GPU).}
\label{tab:inference_bench}
\begin{tabular}{llcr}
\toprule
\textbf{Eszköz} & \textbf{Precízió} & \textbf{Kötegméret} & \textbf{Pozíció/mp} \\
\midrule
CPU & Float32 & 1 & 570 \\
CPU & Float32 & 64 & \num{2850} \\
GPU (CUDA) & Float16 & 1 & 201 \\
GPU (CUDA) & Float16 & 64 & \num{12715} \\
GPU (CUDA) & Float16 & 512 & \textbf{\num{81519}} \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Sakkmotor (MoveGen):} A C++ implementáció \num{56887} pozíció/mp sebességet ért el, szemben a Python (python-chess) \num{10699} pozíció/mp teljesítményével. Ez \textbf{\num{5.3}-szoros gyorsulást} jelent.
    \item \textbf{Neurális következtetés:} Az FP16 precízió és a \texttt{channels\_last} memóriaelrendezés révén a rendszer 512-es kötegméret mellett \textbf{\num{81519} pozíció/mp} áteresztőképességet ért el.
    \item \textbf{MCTS-skálázódás:} A keresési sebesség (NPS) a kötegméret növelésével mérsékelten nő 64-es méretig, ahol eléri a \textbf{\num{25000} NPS} körüli csúcsot, majd csökken; a skálázódás nem lineáris.
\end{itemize}

\textbf{Erőforrás-hatékonyság:} A rendszer energiahatékonysága a „Hatékonyság” forgatókönyv alapján került vizsgálatra. A 300~iterációs teljes tanítási ciklus futási ideje \textbf{\num{9.1}~óra} volt az RTX 3070 Laptop GPU-n. A becsült energiafogyasztás (\SI{220}{\watt} TDP mellett) nagyságrendileg \textbf{\SI{2.0}{\kilo\watt\hour}}, ami azt mutatja, hogy a módszertan otthoni környezetben is alkalmazható, mérsékelt energiaigénnyel; a hosszabb futások arányosan több energiát igényeltek.

A mérések alapján az \textbf{RQ1}-re válaszolva kijelenthető, hogy a hibrid C++/Python architektúra a sakkmagban \num{5.3}-szoros gyorsulást, a neurális következtetésben pedig nagykötegű futtatás mellett nagyságrendileg tízszeresnél nagyobb sebességnövekedést eredményezett a Python-alapú kiinduló megoldáshoz képest. Ez a teljesítmény elegendő az egynapos önjátszásos tanítási ciklus megvalósításához.

\subsection{RQ2: Skálázhatóság}

Az önjátszás párhuzamosítása során jól megfigyelhető az Amdahl-törvény hatása~\cite{amdahl1967} (\Fig{fig:nps_scaling}). Az ábra bal oldali grafikonja az önjátszás teljesítményének skálázódását mutatja a szálak számának függvényében. A játszma/mp érték a szálak számával nő (1 szálon $\approx$0.5, 6 szálon $\approx$1.8 játszma/mp), azonban a mért gyorsulás jelentősen elmarad az elméleti lineáris maximumtól (szaggatott vonal): 6 szálnál a gyorsulás nagyjából 3.5$\times$, a várható 6$\times$ helyett. Ez a Python GIL, a szálak közti szinkronizáció és az adatmozgatás többletköltségei miatt fellépő párhuzamosítási veszteségekre utal.

A jobb oldali grafikon a keresési sebességet (NPS) ábrázolja a kötegméret függvényében. A görbe egyértelmű maximumot mutat 64-es kötegméretnél ($\approx$25\,000 NPS), amely az adott hardveren az optimális munkapontot jelenti. Kisebb kötegeknél (1 és 4 közötti kötegméret esetén) a keretrendszer fix költségei dominálnak, ezért csak $\sim$20\,000 NPS érhető el. 8 és 32 között meredek emelkedés figyelhető meg, ahogy a kötegelt feldolgozás jobban kihasználja az erőforrásokat, míg 128 és 256 esetén a várakozási idők, a memóriasávszélesség és a cache-hatások miatt ismét csökken a hatékonyság. A 4$\to$8 közötti lokális törés, illetve az átmeneti visszaesés $\sim$20\,000 NPS körül arra utal, hogy a kötegelt végrehajtás inicializációs költségei a kis-közepes tartományban még jelentősen terhelik a rendszert.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{nps_scaling}
  \caption{A keresési sebesség (NPS) és az önjátszás skálázódása.}
  \label{fig:nps_scaling}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Önjátszás skálázódása munkaszálak függvényében.}
\label{tab:scaling}
\begin{tabular}{cc}
\toprule
Munkaszálak & Játszma/mp \\
\midrule
1 & 0{,}53 \\
2 & 0{,}98 \\
4 & 1{,}53 \\
6 & \textbf{1{,}89} \\
\bottomrule
\end{tabular}
\end{table}

A \Tab{tab:scaling} alapján a 6 szálas konfiguráció \textbf{\num{3.5}-szörös gyorsulást} nyújt az egyszálúhoz képest, ami alátámasztja a párhuzamos architektúra hatékonyságát, még ha a skálázódás nem is tökéletesen lineáris.

A skálázódást vizsgáló \textbf{RQ2} eredményei azt mutatják, hogy az önjátszás teljesítménye 6 munkaszálon kb.\ \num{3.5}-szörösére nőtt az egyszálú futtatáshoz képest, a keresési sebesség (NPS) pedig 64-es kötegméretig skálázódik jól, ahol eléri a $\sim$\num{25000} NPS körüli csúcsot; a további skálázást elsősorban a Python GIL és a host és a device közötti adatmozgatás korlátozza.

\subsection{RQ3: Tanulási dinamika és bajnokság}

A tanulási dinamikát az \Fig{fig:training_dynamics} foglalja össze: a bal oldali ábrák a stratégiai (policy) veszteséget, a jobb oldaliak az értékbecslési (value) MSE-t mutatják, felül az iterációk, alul az eltelt futási idő függvényében. A stratégiai veszteség szempontjából a \textbf{Nagy Áteresztőképesség} (zöld) konfiguráció adja a legjobb eredményt: a keresztentrópia kb.\ 2{,}7 körüli értéken stabilizálódik. Ezt követi a \textbf{Mély Keresés} (sárga) és a \textbf{Nagy Entrópia} (narancs) 3{,}3 és 3{,}5 közötti végértékkel. A \textbf{Hatékonyság} (lila) és különösen a \textbf{Referencia} (kék) konfigurációk ugyan az első $\sim$50 iterációban gyorsan csökkennek, utána azonban 4{,}2 és 4{,}6 közötti platóra állnak be, ami arra utal, hogy a gyakori frissítések miatt a politika viszonylag korán „megfagy”.

Az értékbecslési veszteség eltérő mintázatot mutat. A \textbf{Nagy Áteresztőképesség} MSE-je nagyon alacsony, $\sim$0{,}05 és 0{,}07 közötti tartományban marad. A \textbf{Hatékonyság} konfiguráció lassabban, de hasonlóan alacsony szintre (kb.\ 0{,}08 és 0{,}1 közötti tartományba) konvergál. A \textbf{Referencia} és a \textbf{Mély Keresés} görbéje valamivel magasabban, nagyjából 0{,}1 és 0{,}15 közötti tartományban stabilizálódik, míg a \textbf{Nagy Entrópia} esetén a veszteség a 100. iteráció után ismét növekedni kezd, és a tanítás végére $\sim$0{,}25 és 0{,}3 közötti értékre áll be. Ez arra utal, hogy a tartósan magas exploráció ugyan javítja a politika sokféleségét, de a value-fej tanulását a zajos, nem stacionárius célok megnehezítik.

Az időalapú összehasonlítás (alsó sor) a fenti trendeket futásidővel súlyozza. A \textbf{Hatékonyság} konfiguráció fejezi be leggyorsabban a tanulást (kb.\ 9~óra), de közben magas stratégiai veszteségen marad, és a későbbi Elo-mérések alapján ez a konfiguráció teljesít a leggyengébben. A többi konfiguráció 14 és 18 óra közötti futási időt igényel: a \textbf{Nagy Áteresztőképesség} és a \textbf{Mély Keresés} a leghosszabb, de ezek adják a legalacsonyabb végső stratégiai veszteséget. A \textbf{Nagy Áteresztőképesség} így a futásidő és a végső teljesítmény között kedvező kompromisszumot jelent, míg a \textbf{Referencia} konfiguráció hosszabb idő alatt is lényegesen rosszabb politikán marad.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{training_dynamics}
  \caption{Tanulási dinamika: policy- és value-veszteségek az iterációk függvényében.}
  \label{fig:training_dynamics}
\end{figure}

Az arénamérkőzések eredményeit az \Fig{fig:evaluation_win_rate} szemlélteti. Az átlagos győzelmi arány az első 20 és 30 közötti iterációban gyorsan konvergál az 50\%-os, kiegyenlített szint közelébe, és ezt követően mind az öt konfiguráció nagyon hasonló tartományban marad. A \textbf{Referencia} (kék) kb.\ 60\% körüli kezdő értékről fokozatosan esik vissza $\sim$50\% alá. A \textbf{Mély Keresés} (sárga) és a \textbf{Nagy Áteresztőképesség} (zöld) végig enyhén 50\% fölött, míg a \textbf{Nagy Entrópia} (narancs) és a \textbf{Hatékonyság} (lila) közvetlenül az 50\%-os vonal körül stabilizálódik. A tanítás végére a görbék mindössze néhány százalékpontos különbséggel, nagyjából 48 és 52\% között helyezkednek el, ezért a konfigurációk közötti eltérések a mérési bizonytalanságon belül maradnak.

Az árnyékolt sávok az iterációnkénti 24 arénapartiból számolt 95\%-os konfidenciaintervallumokat jelölik. A sávok nagy szélessége (tipikusan kb.\ $\pm$20 és 25 százalékpont közötti érték) azt mutatja, hogy az egyes iterációk közti ingadozások jelentős része statisztikai zaj, nem pedig tartós teljesítménykülönbség. Ez alátámasztja a folyamatos, AlphaZero-stílusú tanulás választását az AlphaGo Zero-féle szigorú aréna-kapuzással szemben. Ilyen zajos becslések mellett az előreléptetési döntések megbízhatatlanok lettek volna, ezért a jelen kísérletekben az arénamérkőzéseket kizárólag mérőszámként használtam, és nem engedtem, hogy a self-play modell kiválasztásáról döntsenek.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{evaluation_win_rate}
  \caption{Győzelmi arányok alakulása a tanítás során (95\% konfidenciaintervallummal).}
  \label{fig:evaluation_win_rate}
\end{figure}

A modellek relatív játékerejét körmérkőzéses (round-robin) bajnokságban mértem, két kézzel definiált referenciaügynökkel kiegészítve:
\begin{itemize}
    \item \textbf{Véletlen (\textit{Random}):} tisztán véletlenszerű lépések.
    \item \textbf{Mohó (\textit{Greedy}):} egyszerű, anyagelőnyt maximalizáló heurisztika.
\end{itemize}

A bajnokság végeredményét az \Fig{fig:tournament_elo} mutatja Elo-pontszámok formájában. A \textbf{Nagy Entrópia} konfiguráció (1249 Elo) bizonyult a legerősebb tanult modellnek, és egyedüliként múlja felül a \textbf{Mohó} heurisztikát (1228 Elo). Ez megerősíti azt a következtetést, hogy a rendelkezésre álló rövid tanítási időben a nagyobb diverzitás (magas politika-entrópia) fontosabb tényező, mint a puszta keresési mélység vagy sebesség.

A \textbf{Hatékonyság} modell 1164 Elo-val a mezőny végén végzett, és még a \textbf{Véletlen} ágensnél is gyengébbnek bizonyult (1169 Elo; a becsült Elo-pontszám alapján, a mérési zajt figyelembe véve). Ez arra utal, hogy a túl agresszív frissítési stratégia a hálózat instabil, irányt tévesztett tanulásához vezetett (katasztrofális felejtés vagy rossz lokális optimum). A többi tanult konfiguráció (\textbf{Referencia} 1192, \textbf{Mély Keresés} 1195, \textbf{Nagy Áteresztőképesség} 1198 Elo) szűk sávban, közvetlenül a \textbf{Mohó} ügynök alatt csoportosul; ez jelzi, hogy bár mindegyik modell elsajátított egy használható sakkstratégiát, a rendelkezésre álló időben egyik sem tudott a kézzel tervezett heurisztikánál érdemben erősebb játékot kialakítani.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{tournament_elo}
  \caption{A bajnokság végeredménye (Elo-pontszámok).}
  \label{fig:tournament_elo}
\end{figure}

\textbf{Elemzés:}
A körmérkőzéses bajnokság eredményei szerint a Nagy Entrópia konfiguráció (1249 Elo) teljesített a legjobban, egyedüliként megelőzve a Mohó (\textit{Greedy}) ágenst (1228 Elo). A többi tanult konfiguráció szűk sávban, 1192 és 1198 Elo között helyezkedik el, közvetlenül a Mohó alatt, míg a Véletlen és Hatékonyság ügynökök 1164 és 1169 Elo tartományban zártak. Rövid, 250 és 300 iteráció közötti tanítási ciklusban a legfőbb veszély nem a lassú konvergencia, hanem a \textbf{lokális optimumba ragadás} (pl.\ passzív döntetlenek). A magas explorációs beállítások (Dirichlet-zaj $\alpha=0.5$, $c_{\text{puct}}=2.5$) biztosították a kockázatosabb, de informatívabb ágak felderítését.

Fontos ugyanakkor kiemelni, hogy a bajnokság korlátozott játszmaszáma miatt az Elo-becslések bizonytalansága várhatóan több tíz pont nagyságrendű. A kisebb különbségek (például 1164 versus 1169 Elo) ezért inkább jelzésértékűek, mintsem statisztikailag egyértelműen szignifikáns eltérések; a konfigurációk relatív sorrendje megbízhatóbb információt ad, mint az abszolút értékek.

Ezzel szemben a Hatékonyság konfiguráció, amely a „hatékonyságot” célozta (gyakori frissítések, kis köteg), instabillá vált és a legrosszabb eredményt nyújtotta. Teljesítménye még a Random ágenst is alulmúlta. Ebből arra következtetek, hogy korlátozott pufferkapacitás és agresszív frissítési stratégia mellett a hálózat nem képes stabilan megtartani a korábban tanult mintázatokat (katasztrofális felejtés), ezért az ilyen beállítások gyakorlatban kerülendők.

A tanulási dinamikára vonatkozó \textbf{RQ3} kapcsán az látszik, hogy a policy- és value-veszteséggörbék mind az öt konfiguráció esetén jól látható csökkenést mutatnak, különösen a \textbf{Nagy Áteresztőképesség} és \textbf{Mély Keresés} beállításoknál, ami azt jelzi, hogy rövid, 9 és 18 óra közötti tanítási horizonton is kimutatható, stabil tanulás zajlik.

\subsection{RQ4: Adatdiverzitás}

Az \Fig{fig:game_length_dist} a lejátszott játszmák hosszának eloszlását mutatja fél-lépésekben. A \textbf{Mély Keresés} (sárga) konfiguráció sűrűségfüggvénye adja a legmagasabb csúcsot: a maximum kb.\ 0{,}036 körül alakul, nagyjából 45 és 50 fél-lépés között. A \textbf{Hatékonyság} (lila) eloszlása a legkeskenyebb és legkoncentráltabb: csúcsa kb.\ 35 és 40 fél-lépés között jelenik meg, ami arra utal, hogy a gyakori gradiens-frissítések (\texttt{samples\_per\_new\_game} = 8) és a kisebb kötegméret gyors, de viszonylag egyoldalú konvergenciához vezettek, így a játszmák többsége hasonló hosszúságú, kevés változatos végjátékkal.

A legmarkánsabban eltérő viselkedést a \textbf{Nagy Entrópia} (narancs) konfiguráció mutatja, amelynek eloszlása egyértelműen \emph{bimodális}: egy első csúcs látható kb.\ 50 fél-lépésnél, míg egy második, laposabb csúcs kb.\ 110 fél-lépés körül jelenik meg. Az első csúcs a középjátékban lezáruló partikat, a második a hosszú, végjátékig elnyúló játszmákat reprezentálja. A magas exploráció ($c_{\text{puct}} = 2.5$, Dirichlet $\alpha = 0.5$) egyrészt lehetővé teszi a döntő taktikai ütközéseket, másrészt gyakran megakadályozza a korai lépésismétléses döntetleneket, így széles skálán, sokféle pozíciót lefedve tud tanulni a modell. Ez a diverzitás összhangban van a konfiguráció kiemelkedő Elo-eredményével.

A \textbf{Mély Keresés} (sárga) és a \textbf{Nagy Áteresztőképesség} (zöld) eloszlásai hasonló, egymóduszú görbéket adnak: a tömegük zöme 40 és 60 fél-lépés között helyezkedik el, a \textbf{Nagy Áteresztőképesség} csúcsa azonban kissé jobbra tolódik, ami arra utal, hogy a sekélyebb keresés (64 szimuláció) ritkábban dönt el partikat már a középjátékban. A \textbf{Referencia} (kék) konfiguráció eloszlása a két szélsőség között helyezkedik el, mind csúcsmagasságban, mind pozícióban, ami jól illeszkedik a baseline jellegéhez.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{game_length_dist}
  \caption{A lejátszott játszmák hosszának eloszlása (fél-lépésekben).}
  \label{fig:game_length_dist}
\end{figure}

Az adatdiverzitást firtató \textbf{RQ4}-re a válasz, hogy a magas explorációs paraméterekkel futó \textbf{Nagy Entrópia} konfiguráció sokkal változatosabb, bimodális játszmahossz-eloszlást produkál, ami szélesebb pozíciókészletet eredményez. A Nagy Entrópia konfiguráció bimodális játszmahossz-eloszlása jól rezonál az RQ3 alatt mért Elo-előnnyel: a sokszínűbb tanítópozíciók rövid tanítási horizonton fontosabbnak bizonyultak, mint a puszta keresési mélység növelése.

\subsection{RQ5: Reprodukálhatóság}
A kísérletek reprodukálhatóságát a teljesen automatizált szkriptek (\texttt{benchmarks.py}, \texttt{generate\_artifacts.py}) és a YAML-konfigurációs fájlok biztosítják, amelyek minden futtatási paramétert rögzítenek. Minden kísérlet egy YAML-konfigurációs fájlhoz kötött, a benchmark- és artefaktumgeneráló szkriptek pedig automatikusan hozzák létre a táblázatokat és ábrákat. A véletlen inicializálás miatt újrafuttatáskor kis Elo-szórás várható.

Végül, a reprodukálhatóságot érintő \textbf{RQ5} tekintetében elmondható, hogy a teljesen automatizált benchmark- és artefaktumgeneráló szkriptek, valamint a YAML-konfigurációk révén a tanítási és mérési futtatások más gépeken is megismételhetők, az eltérést elsősorban a sztochasztikus komponensek okozzák.

\section{Összefoglalás}
A mérések szerint a rendszer technikai teljesítménye (\num{56887} pozíció/mp, \num{25000} NPS) a kitűzött kutatási célok szempontjából elegendőnek bizonyult. A kísérleti eredmények alapján korlátozott erőforrások mellett az \textbf{exploráció maximalizálása} a kulcs a játékerő növeléséhez, nem pedig a puszta számítási mélység vagy a frissítési sebesség.

\chapter{Megbeszélés és korlátok}

Ebben a fejezetben az RQ1-től RQ5-ig terjedő kérdésekre kapott eredményeket értelmezem, majd kitérek a módszertan korlátaira.

\section{Eredmények értelmezése}

\subsection{Az exploráció dominanciája}

Az exploráció dominanciája elsősorban az RQ3 (tanulási dinamika) és RQ4 (adatdiverzitás) szempontjából releváns: korlátozott erőforrás mellett a változatos adatgenerálás fontosabbnak bizonyult, mint a puszta keresési mélység. A magas explorációs beállítások ($\alpha=0.5$, $c_{\text{puct}}=2.5$) megakadályozták, hogy a rendszer túl korán passzív stratégiákhoz tapadjon. Míg a Hatékonyság forgatókönyv gyorsan konvergált egy szűk, de gyenge stratégiára (és ezzel elveszítette rugalmasságát), a Nagy Entrópia ágens szélesebb körű keresést végzett. Ez stabilabb játékot eredményezett, miközben a Hatékonyság konfiguráció a Random referencia szintjén vagy az alatt végzett, érdemi előny nélkül.

\subsection{A rendszer skálázhatósága}

A rendszer skálázhatósága (RQ1, RQ2) tekintetében a kötegelt levélértékelés és a \texttt{Virtual Loss}-ra épülő konkurenciakezelés együtt \textbf{\num{3.5}-szörös} önjátszás-gyorsulást adott 6 szálon, miközben megőrizte a fa diverzitását. A NodePool indexalapú, vektoralapú allokációja biztosítja, hogy az átméretezés a pointerek érvényessége mellett is skálázható legyen. A fő szűk keresztmetszetek a Python GIL és a host és a device közötti adatmozgatás jelentik, ami jelzi, hogy több GPU vagy teljesen natív önjátszás további gyorsulást hozhat.

\section{Korlátok}

\subsection{Abszolút játékerő}

Bár a rendszer a saját, zárt skálán mért 1249 Elo-pontszámmal legyőzte a mohó (Greedy) ágenst, ez a szint nagyjából egy erősebb amatőr játékosnak felel meg, és nem hasonlítható közvetlenül sem a FIDE-, sem az online szerverek Elo-értékeihez. Az 1249-es Elo-érték így kizárólag a dolgozatban definiált, zárt bajnokságon belül értelmezhető. A modell képes alapvető pozíciós elvek követésére (például a gyalogstruktúra védelmére), de hiányzik belőle a mély taktikai számolás képessége, amelyhez nagyságrendileg több (milliós nagyságrendű) önjátszásos játszmára lenne szükség. A szakirodalomban tipikusan több tízezer, sőt akár milliós nagyságrendű játszmára épülő, standardizált ellenfelek elleni méréseket használnak FIDE-közeli skálákhoz; jelen dolgozat keretében ilyen típusú, külső, Stockfish-alapú Elo-kalibrációt tudatosan nem végeztem.

Ez összhangban áll a Célkitűzések fejezetben megfogalmazott korlátozott hatókörrel: a cél a módszertan megvalósíthatóságának demonstrálása volt, nem pedig versenyképes motor fejlesztése.

\subsection{Számítási horizont}

A 300~iteráció és a 9 és 18 óra közötti futási idő a „megvalósíthatósági bizonyítás” kategóriájába esik. Az AlphaZero eredeti tanulmánya több ezer TPU-t használt, nagyságrendekkel nagyobb önjátszásos adatmennyiség mellett. Bár kísérleti validációval sikerült igazolni a módszertant, a versenyképes szint elérése ezen a hardveren hónapokat venne igénybe.

\chapter{Összefoglalás}

A dolgozat egy fogyasztói hardverre optimalizált, AlphaZero-típusú sakkprogramot mutatott be. A rendszer C++23-alapú keresőt és PyTorch-alapú tanítási modult egyesít, kifejezetten egy RTX 3070 Laptop GPU környezetére tervezve.

A fejlesztés három fő pillére az architekturális egyszerűsítés („Mindig világos” kanonikus nézet), a virtuális veszteségre épülő konkurenciakezelés és a kötegelt levélértékelés volt. Ezek együtt biztosítják a szükséges sebességet a korlátos erőforrású környezetben. Az öt konfigurációt lefedő kísérleti validáció megmutatta, hogy a magas entrópiájú keresés hozza a legjobb eredményt a rendelkezésre álló adat- és időkeret mellett.

Az RQ1-től RQ5-ig terjedő kutatási kérdésekre adott válaszok az alábbiakban foglalhatók össze:
\begin{enumerate}
\item \textbf{RQ1, Teljesítmény:} a C++23-alapú sakkmag \num{56887} pozíció/mp sebességet ért el, míg a GPU-alapú neurális következtetés \num{81519} pozíció/mp áteresztőképességet biztosított nagy kötegméreteknél. Ez legalább \num{5.3}-szoros gyorsulást jelent a sakkmag esetén a Python-alapú megoldáshoz képest, és nagyságrendileg tízszeresnél nagyobb gyorsulást a neurális következtetésben, ami elegendő az önjátszásos tanításhoz egynapos időkeretben.
\item \textbf{RQ2, Skálázhatóság:} az önjátszás teljesítménye 6 munkaszálon kb.\ \num{3.5}-szörösére nőtt az egyszálú futtatáshoz képest, a keresési sebesség (NPS) pedig 64-es kötegméretig skálázódik jól, ahol eléri a $\sim$\num{25000} NPS körüli csúcsot; a további skálázást a Python GIL és a host és a device közötti adatmozgatás korlátozza.
\item \textbf{RQ3, Tanulási dinamika:} a policy- és value-veszteség mind az öt vizsgált konfiguráció esetén jelentősen csökkent, különösen a \textbf{Nagy Áteresztőképesség} és \textbf{Mély Keresés} beállításoknál, ami azt mutatja, hogy rövid tanítási horizonton is érdemi, mérhető tanulás zajlik.
\item \textbf{RQ4, Adatdiverzitás:} a magas explorációs beállításokkal futó \textbf{Nagy Entrópia} konfiguráció bimodális játszmahossz-eloszlást és sokkal változatosabb tanítóadatot eredményezett, és a mérések alapján kis előnnyel felülmúlta a mohó heurisztikát a saját Elo-skálán, ami alátámasztja az adatdiverzitás szerepét korlátozott erőforrások mellett.
\item \textbf{RQ5, Reprodukálhatóság:} a teljesen automatizált benchmark- és artefaktumgeneráló szkriptek, valamint a YAML-konfigurációk rögzítik a hardver- és hiperparaméter-beállításokat, így a kísérleti futtatások más környezetben is megismételhetők, legfeljebb kisebb, a sztochasztikusságból adódó Elo-szórással.
\end{enumerate}

A dolgozat elsődleges hozzájárulása egy keretrendszer, amely referenciapontot nyújt az AlphaZero-módszertan korlátozott erőforrású megvalósításához. A jövőbeli fejlesztési irányok közé tartozik a jelenlegi 6 blokkos háló fokozatos skálázása 10 és 20 blokk közötti méretig, és annak vizsgálata, hogy az itt bemutatott paraméterezés mellett hogyan változik a tanulási dinamika. További lehetőség a tanult környezetmodellek (pl.\ MuZero-típusú megközelítések) bevezetése, ahol a környezetmodell is tanulható lenne. Emellett fontos jövőbeli lépés lehet a rendszer adaptálása elosztott (kliens-szerver) környezetre. Ezek a fejlesztések közelebb vihetnek egy olyan AlphaZero-típusú sakkmotorhoz, amely már nemcsak kísérleti, hanem gyakorlati szempontból is versenyképes játékerőt képvisel.

\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Irodalomjegyzék}
\begingroup
\singlespacing
\footnotesize
\begin{thebibliography}{99}\setlength{\itemsep}{\baselineskip}

\bibitem{amdahl1967}
Amdahl, G. M.
Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities.
In: \emph{Proceedings of the April 18--20, 1967, Spring Joint Computer Conference}, 483--485 (1967).\\
\doi{10.1145/1465482.1465560}

\bibitem{auer2002ucb}
Auer, P., Cesa-Bianchi, N., Fischer, P.
Finite-time Analysis of the Multiarmed Bandit Problem.
\emph{Machine Learning} 47(2--3):235--256 (2002).\\
\doi{10.1023/A:1013689704352}

\bibitem{bengio2009curriculum}
Bengio, Y., Louradour, J., Collobert, R., Weston, J.
Curriculum Learning.
In: \emph{Proceedings of the 26th International Conference on Machine Learning (ICML 2009)}, 41--48 (2009).\\
\doi{10.1145/1553374.1553380}

\bibitem{browne2012}
Browne, C. B., Powley, E., Whitehouse, D., \emph{et al.}
A Survey of Monte Carlo Tree Search Methods.
\emph{IEEE Transactions on Computational Intelligence and AI in Games} 4(1):1--43 (2012).\\
\doi{10.1109/TCIAIG.2012.2186810}

\bibitem{campbell2002deepblue}
Campbell, M., Hoane, A. J., Hsu, F.-H.
Deep Blue.
\emph{Artificial Intelligence} 134(1--2):57--83 (2002).\\
\doi{10.1016/S0004-3702(01)00129-1}

\bibitem{cpwLMR}
Chess Programming Wiki.
Late Move Reductions.\\
\url{https://www.chessprogramming.org/Late_Move_Reductions}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{cpwNullMove}
Chess Programming Wiki.
Null Move Pruning.\\
\url{https://www.chessprogramming.org/Null_Move_Pruning}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{coulom2006cg}
Coulom, R.
Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search.
In: \emph{Proceedings of the 5th International Conference on Computers and Games (CG 2006)}, LNCS 4630, 72--83 (2007).\\
\doi{10.1007/978-3-540-75538-8_7}

\bibitem{crazyara}
Czech, J., Willig, M., Beyer, A., Kersting, K., Fürnkranz, J.
Learning to Play the Chess Variant Crazyhouse Above World Champion Level with Deep Neural Networks and Human Data.
\emph{Frontiers in Artificial Intelligence} 3:24 (2020).\\
\doi{10.3389/frai.2020.00024}

\bibitem{gelly2006mogo}
Gelly, S., Wang, Y.
Exploration Exploitation in Go: UCT for Monte-Carlo Go.
In: \emph{NIPS 2006 Workshop on Online Trading of Exploration and Exploitation}, Whistler, Canada (2006).

\bibitem{gelly2007rave}
Gelly, S., Silver, D.
Combining Online and Offline Knowledge in UCT.
In: \emph{Proceedings of the 24th International Conference on Machine Learning (ICML 2007)}, 273--280 (2007).\\
\doi{10.1145/1273496.1273531}

\bibitem{he2016resnet}
He, K., Zhang, X., Ren, S., Sun, J.
Deep Residual Learning for Image Recognition.
In: \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)}, 770--778 (2016).\\
\doi{10.1109/CVPR.2016.90}

\bibitem{hu2018senet}
Hu, J., Shen, L., Sun, G.
Squeeze-and-Excitation Networks.
In: \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)}, 7132--7141 (2018).\\
\doi{10.1109/CVPR.2018.00745}

\bibitem{pybind11}
Jakob, W., Rhinelander, J., Moldovan, D.
pybind11: Seamless operability between C++11 and Python.\\
\url{https://github.com/pybind/pybind11}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{knuth1975alphabeta}
Knuth, D. E., Moore, R. W.
An Analysis of Alpha-Beta Pruning.
\emph{Artificial Intelligence} 6(4):293--326 (1975).\\
\doi{10.1016/0004-3702(75)90019-3}

\bibitem{kocsis2006uct}
Kocsis, L., Szepesvári, C.
Bandit Based Monte-Carlo Planning.
In: \emph{Machine Learning: ECML 2006}, LNCS 4212, 282--293 (2006).\\
\doi{10.1007/11871842_29}

\bibitem{lczero}
Leela Chess Zero (Lc0). Open-source neural network chess engine.\\
Project: \url{https://lczero.org/}.\\
GitHub: \url{https://github.com/LeelaChessZero}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{lichess}
Lichess.
\emph{chess-openings dataset}.\\
\url{https://github.com/lichess-org/chess-openings}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{loshchilov2017sgdr}
Loshchilov, I., Hutter, F.
SGDR: Stochastic Gradient Descent with Warm Restarts.
In: \emph{Proceedings of the 5th International Conference on Learning Representations (ICLR 2017)}.\\
\url{https://openreview.net/forum?id=Skq89Scxx}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{micikevicius2018amp}
Micikevicius, P., Narang, S., Alben, J., \emph{et al.}
Mixed Precision Training.
In: \emph{Proceedings of the 6th International Conference on Learning Representations (ICLR 2018)}.\\
\url{https://openreview.net/forum?id=r1gs9JgRZ}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{nasu2018nnue}
Nasu, Y.
Efficiently Updatable Neural-Network-based Evaluation Function for computer Shogi.
Technical report, 28th World Computer Shogi Championship, 2018.\\
\url{https://github.com/asdfjkl/nnue}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{pytorch}
Paszke, A., Gross, S., Massa, F., \emph{et al.}
PyTorch: An Imperative Style, High-Performance Deep Learning Library.
In: \emph{Advances in Neural Information Processing Systems 32} (NeurIPS 2019), 8024--8035.\\
\url{https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{pythonchess}
Fiekas, N.
python-chess: a chess library for Python.\\
\url{https://python-chess.readthedocs.io}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{schrittwieser2020muzero}
Schrittwieser, J., Antonoglou, I., Hubert, T., \emph{et al.}
Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.
\emph{Nature} 588(7839):604--609 (2020).\\
\doi{10.1038/s41586-020-03051-4}

\bibitem{shannon1950chess}
Shannon, C. E.
Programming a Computer for Playing Chess.
\emph{The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science} 41(314):256--275 (1950).\\
\doi{10.1080/14786445008521796}

\bibitem{silver2016alphago}
Silver, D., Huang, A., Maddison, C. J., \emph{et al.}
Mastering the Game of Go with Deep Neural Networks and Tree Search.
\emph{Nature} 529(7587):484--489 (2016).\\
\doi{10.1038/nature16961}

\bibitem{silver2017chess}
Silver, D., Schrittwieser, J., Simonyan, K., \emph{et al.}
Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.
\emph{arXiv preprint} arXiv:1712.01815 (2017).\\
\url{https://arxiv.org/abs/1712.01815}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{silver2017go}
Silver, D., Schrittwieser, J., Simonyan, K., \emph{et al.}
Mastering the Game of Go without Human Knowledge.
\emph{Nature} 550(7676):354--359 (2017).\\
\doi{10.1038/nature24270}

\bibitem{silver2018science}
Silver, D., Hubert, T., Schrittwieser, J., \emph{et al.}
A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go Through Self-Play.
\emph{Science} 362(6419):1140--1144 (2018).\\
\doi{10.1126/science.aar6404}

\bibitem{stockfish}
Stockfish developers.
Stockfish chess engine.\\
\url{https://stockfishchess.org}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{wu2019katago}
Wu, D. J.
Accelerating Self-Play Learning in Go.
\emph{arXiv preprint} arXiv:1902.10565 (2019).\\
\url{https://arxiv.org/abs/1902.10565}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{zobrist1970}
Zobrist, A. L.
A New Hashing Method with Application for Game Playing.
\emph{Technical Report 88}, Computer Sciences Department, University of Wisconsin, Madison, WI (1970).\\
\url{https://research.cs.wisc.edu/techreports/1970/TR88.pdf}.\\
Utolsó megtekintés: 2025. 12. 07.

\bibitem{harris2020numpy}
Harris, C. R., \emph{et al.}
Array Programming with NumPy.
\emph{Nature} 585(7825):357--362 (2020).\\
\doi{10.1038/s41586-020-2649-2}

\bibitem{hunter2007matplotlib}
Hunter, J. D.
Matplotlib: A 2D Graphics Environment.
\emph{Computing in Science \& Engineering} 9(3):90--95 (2007).\\
\doi{10.1109/MCSE.2007.55}

\bibitem{waskom2021seaborn}
Waskom, M. L.
Seaborn: Statistical Data Visualization.
\emph{Journal of Open Source Software} 6(60):3021 (2021).\\
\doi{10.21105/joss.03021}

\end{thebibliography}
\endgroup

\cleardoublepage
\chapter*{Nyilatkozat}
\thispagestyle{plain}
\phantomsection
\addcontentsline{toc}{chapter}{Nyilatkozat}

\noindent Alulírott \AuthorName{} \StudentProgram{} szakos hallgató, kijelentem, hogy a dolgozatomat a Szegedi Tudományegyetem, Informatikai Intézet \DepartmentNameText{}én készítettem, \StudentProgram{} diploma megszerzése érdekében.

\medskip

\noindent Kijelentem, hogy a dolgozatot más szakon korábban nem védtem meg, saját munkám eredménye, és csak a hivatkozott forrásokat (szakirodalom, eszközök, stb.) használtam fel. Tudomásul veszem, hogy szakdolgozatomat a Szegedi Tudományegyetem Diplomamunka Repozitóriumában tárolja.

\vspace{18mm}
\begin{flushright}
Szeged, \SubmissionDate
\end{flushright}

\vspace{14mm}
\begin{flushright}
\makebox[6cm]{\hrulefill}\\
aláírás
\end{flushright}

\cleardoublepage
\chapter*{Köszönetnyilvánítás}
\thispagestyle{plain}
\phantomsection
\addcontentsline{toc}{chapter}{Köszönetnyilvánítás}

Köszönettel tartozom témavezetőmnek a szakmai iránymutatásért és az értékes visszajelzésekért. Hálás vagyok családomnak és barátaimnak a támogatásért és bátorításért. Köszönet illeti a fejlesztői közösséget is a nyílt forráskódú eszközök és könyvtárak elérhetővé tételéért, amelyek e munka szakmai alapját adták.

\end{document}